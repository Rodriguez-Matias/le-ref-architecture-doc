{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"license/","text":"License \u00b6 MIT License Copyright \u00a9 2017 - 2020 Binbash Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright \u00a9 2017 - 2020 Binbash Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"welcome/","text":"About Leverage \u00b6 What's Leverage? Our focus is on creating reusable, high quality Cloud Infrastructure code, such as our Reference AWS Cloud Solutions Architecture backed by our DevOps Automation Code Library . Because all the code and modules are already built, we can get you up and running at least 2x faster than a consulting company ( typically in just few weeks! ) on top of code that is thoroughly documented, tested, and has been proven in production at dozens of other project deployments. Why Leverage? If you implement the Binbash Leverage Reference AWS Cloud Solutions Architecture via our DevOps Automation Code Library you will get your entire Cloud Native Application Infra in few weeks (depending on your project complexity needs). Leverage will solve your entire infrastructure and will grant you complete control of the source code, and of course you'll be able to run it without us. Implement yourself or we implement it for you! Core Features DevOps Automation Code Library : A collection of reusable, tested, production-ready E2E AWS Cloud infrastructure as code solutions, leveraged by modules written in: Terraform, Ansible, Helm charts, Dockerfiles and Makefiles . Reference Architecture : Designed under optimal configs for the most popular modern web and mobile applications needs. Its design is fully based on the AWS Well Architected Framework . Welcome \u00b6 This is the documentation for the Leverage Reference Architecture . It is built around the AWS Well Architected Framework , using a Terraform , Ansible and Helm . An its compose of the following 3 main repos: le-tf-infra-aws le-ansible-infra Getting Started \u00b6 See How it works for a whirlwind tour that will get you started. See User guide for a hands on help.","title":"Welcome"},{"location":"welcome/#about-leverage","text":"What's Leverage? Our focus is on creating reusable, high quality Cloud Infrastructure code, such as our Reference AWS Cloud Solutions Architecture backed by our DevOps Automation Code Library . Because all the code and modules are already built, we can get you up and running at least 2x faster than a consulting company ( typically in just few weeks! ) on top of code that is thoroughly documented, tested, and has been proven in production at dozens of other project deployments. Why Leverage? If you implement the Binbash Leverage Reference AWS Cloud Solutions Architecture via our DevOps Automation Code Library you will get your entire Cloud Native Application Infra in few weeks (depending on your project complexity needs). Leverage will solve your entire infrastructure and will grant you complete control of the source code, and of course you'll be able to run it without us. Implement yourself or we implement it for you! Core Features DevOps Automation Code Library : A collection of reusable, tested, production-ready E2E AWS Cloud infrastructure as code solutions, leveraged by modules written in: Terraform, Ansible, Helm charts, Dockerfiles and Makefiles . Reference Architecture : Designed under optimal configs for the most popular modern web and mobile applications needs. Its design is fully based on the AWS Well Architected Framework .","title":"About Leverage"},{"location":"welcome/#welcome","text":"This is the documentation for the Leverage Reference Architecture . It is built around the AWS Well Architected Framework , using a Terraform , Ansible and Helm . An its compose of the following 3 main repos: le-tf-infra-aws le-ansible-infra","title":"Welcome"},{"location":"welcome/#getting-started","text":"See How it works for a whirlwind tour that will get you started. See User guide for a hands on help.","title":"Getting Started"},{"location":"es/","text":"","title":"Binbash Leverage"},{"location":"es/bienvenido/","text":"Pr\u00f3ximamente \u00b6","title":"Bienvenido"},{"location":"es/bienvenido/#proximamente","text":"","title":"Pr\u00f3ximamente"},{"location":"examples/","text":"Reference Architecture examples \u00b6 This directory contains a catalog of examples on how to run, configure and scale Leverage Reference Architecture. Please review the prerequisites before trying them. Category Name Description Complexity Level Organizations AWS Organization Orchestrate your Ref Architecture Multi-Account AWS Organization Intermediate K8s K8s Kops Setup a production scale K8s clusters via Terraform + Kops Advanced","title":"Introduction"},{"location":"examples/#reference-architecture-examples","text":"This directory contains a catalog of examples on how to run, configure and scale Leverage Reference Architecture. Please review the prerequisites before trying them. Category Name Description Complexity Level Organizations AWS Organization Orchestrate your Ref Architecture Multi-Account AWS Organization Intermediate K8s K8s Kops Setup a production scale K8s clusters via Terraform + Kops Advanced","title":"Reference Architecture examples"},{"location":"examples/PREREQUISITES/","text":"Prerequisites \u00b6 Many of the examples in this directory have common prerequisites .","title":"Prerequisites"},{"location":"examples/PREREQUISITES/#prerequisites","text":"Many of the examples in this directory have common prerequisites .","title":"Prerequisites"},{"location":"examples/k8s-kops/","text":"TODO: Add ref links here \u00b6","title":"K8s Kops"},{"location":"examples/k8s-kops/#todo-add-ref-links-here","text":"","title":"TODO: Add ref links here"},{"location":"examples/organization/","text":"TODO: Add ref links here \u00b6","title":"Organization"},{"location":"examples/organization/#todo-add-ref-links-here","text":"","title":"TODO: Add ref links here"},{"location":"first-steps/","text":"First Steps \u00b6 The objective of this sections is to explain the Binbash Leverage Reference AWS Cloud Solutions Architecture first steps. Overview \u00b6 TODO","title":"Overview"},{"location":"first-steps/#first-steps","text":"The objective of this sections is to explain the Binbash Leverage Reference AWS Cloud Solutions Architecture first steps.","title":"First Steps"},{"location":"first-steps/#overview","text":"TODO","title":"Overview"},{"location":"how-it-works/","text":"How it works \u00b6 The objective of this document is to explain how the Binbash Leverage Reference AWS Cloud Solutions Architecture works, in particular how the Reference Architecure model is built and why we need it. Overview \u00b6 This documentation contains all the guidelines to create Binbash Leverage Reference AWS Cloud Solutions Architecture that will be implemented on the Projects\u2019s AWS infrastructure. Our Purpose Democratize advanced technologies: As complex as it may sound, the basic idea behind this design principle is simple. It is not always possible for a business to maintain a capable in-house IT department while staying up to date. It is entirely feasible to set up your own cloud computing ecosystem from scratch without experience, but that would take a considerable amount of resources; it is definitely not the most efficient way to go. An efficient business-minded way to go is to employ AWS as a service allows organizations to benefit from the advanced technologies integrated into AWS without learning, researching, or creating teams specifically for those technologies. Info This documentation will provide a detailed reference of the tools and techs used, the needs they address and how they fit with the multiple practices we will be implementing. Reference Architecture \u00b6 Reference AWS Cloud Solutions architecture designed under optimal configs for the most popular modern web and mobile applications needs based on AWS \u201cWell Architected Framework\u201d . With it's complete Leverage DevOps Automation Code Library to rapidly implement it will solve your entire infrastructure and will grant you complete control of the source code and of course you'll be able to run it without us. Ref Architecture model \u00b6 Strengths Faster updates (new features and bug fixes). Better code quality and modules maturity (proven and tested). Supported by Binbash, and Open ones even by Binbash + 1000\u2019s of top talented Open Source community contributors. Development cost savings. Client keeps full rights to all commercial, modification, distribution, and private use of the code (No Lock-In) through forks inside their own Projects repos (open-source and commercially reusable via license MIT and Apache 2.0 - https://choosealicense.com/licenses/). Documented. Reusable, Supported & Customizable. Reference Architecture Design \u00b6 DevOps Workflow model \u00b6 Read More \u00b6 Related articles Don't get locked up into avoiding lock-in AWS Managed Services","title":"Overview"},{"location":"how-it-works/#how-it-works","text":"The objective of this document is to explain how the Binbash Leverage Reference AWS Cloud Solutions Architecture works, in particular how the Reference Architecure model is built and why we need it.","title":"How it works"},{"location":"how-it-works/#overview","text":"This documentation contains all the guidelines to create Binbash Leverage Reference AWS Cloud Solutions Architecture that will be implemented on the Projects\u2019s AWS infrastructure. Our Purpose Democratize advanced technologies: As complex as it may sound, the basic idea behind this design principle is simple. It is not always possible for a business to maintain a capable in-house IT department while staying up to date. It is entirely feasible to set up your own cloud computing ecosystem from scratch without experience, but that would take a considerable amount of resources; it is definitely not the most efficient way to go. An efficient business-minded way to go is to employ AWS as a service allows organizations to benefit from the advanced technologies integrated into AWS without learning, researching, or creating teams specifically for those technologies. Info This documentation will provide a detailed reference of the tools and techs used, the needs they address and how they fit with the multiple practices we will be implementing.","title":"Overview"},{"location":"how-it-works/#reference-architecture","text":"Reference AWS Cloud Solutions architecture designed under optimal configs for the most popular modern web and mobile applications needs based on AWS \u201cWell Architected Framework\u201d . With it's complete Leverage DevOps Automation Code Library to rapidly implement it will solve your entire infrastructure and will grant you complete control of the source code and of course you'll be able to run it without us.","title":"Reference Architecture"},{"location":"how-it-works/#ref-architecture-model","text":"Strengths Faster updates (new features and bug fixes). Better code quality and modules maturity (proven and tested). Supported by Binbash, and Open ones even by Binbash + 1000\u2019s of top talented Open Source community contributors. Development cost savings. Client keeps full rights to all commercial, modification, distribution, and private use of the code (No Lock-In) through forks inside their own Projects repos (open-source and commercially reusable via license MIT and Apache 2.0 - https://choosealicense.com/licenses/). Documented. Reusable, Supported & Customizable.","title":"Ref Architecture model"},{"location":"how-it-works/#reference-architecture-design","text":"","title":"Reference Architecture Design"},{"location":"how-it-works/#devops-workflow-model","text":"","title":"DevOps Workflow model"},{"location":"how-it-works/#read-more","text":"Related articles Don't get locked up into avoiding lock-in AWS Managed Services","title":"Read More"},{"location":"how-it-works/considerations/","text":"Important Considerations \u00b6 Assumptions AWS Regions: Multi Region setup \u2192 1ry: us-east-1 (N. Virginia) & 2ry: us-west-2 (Oregon). DevOps necessary repositories will be created. There will be feature branches ( ID-XXX -> master ) and either Binbash or the Client Engineers will be reviewers of each other and approvers (at least 1 approver). After deployment via IaC (Terraform, Ansible & Helm) all subsequent changes will be performed via versioned controlled code, by modifying the corresponding repository and running the proper IaC Automation execution. Will start the process via Local Workstations. Afterwards full exec automation will be considered via: Jenkins, CircleCI or Terraform Cloud Jobs (GitOps). Consideration: Note that any change manually performed will generate inconsistencies on the deployed resources (which left them out of governance and support scope). All AWS resources will be deployed via Terraform and rarely occasional CloudFormation, Python SDK & AWS CLI when the resource is not defined by Terraform (almost none scenario). All code and scripts will be included in the repository. Provisioning via Ansible for resources that need to be provisioned on an OS. Orchestration via Helm + Helmsfile for resources that need to be provisioned in Kubernetes (with Docker as preferred container engine). Infra as code deployments should run from the new ID-XXX or master branch. ID-XXX branch must be merged immediately (ASAP) via PR to the master branch. Consideration: validating that the changes within the code will only affect the desired target resources is the responsibility of the executor (to ensure everything is OK please consider exec after review/approved PR). All resources will be deployed in several new AWS accounts created inside the Client AWS Organization. Except for the AWS Legacy Account invitation to the AWS Org and OrganizationAccountAccessRole creation in it, there will be no intervention whatsoever in current Client Legacy Production account, unless required by Client authority and given a specific requirement. Info We will explore the details of all the relevant Client application stacks, CI/CD processes, monitoring, security, target service level objective (SLO) and others in a separate document.","title":"Considerations"},{"location":"how-it-works/considerations/#important-considerations","text":"Assumptions AWS Regions: Multi Region setup \u2192 1ry: us-east-1 (N. Virginia) & 2ry: us-west-2 (Oregon). DevOps necessary repositories will be created. There will be feature branches ( ID-XXX -> master ) and either Binbash or the Client Engineers will be reviewers of each other and approvers (at least 1 approver). After deployment via IaC (Terraform, Ansible & Helm) all subsequent changes will be performed via versioned controlled code, by modifying the corresponding repository and running the proper IaC Automation execution. Will start the process via Local Workstations. Afterwards full exec automation will be considered via: Jenkins, CircleCI or Terraform Cloud Jobs (GitOps). Consideration: Note that any change manually performed will generate inconsistencies on the deployed resources (which left them out of governance and support scope). All AWS resources will be deployed via Terraform and rarely occasional CloudFormation, Python SDK & AWS CLI when the resource is not defined by Terraform (almost none scenario). All code and scripts will be included in the repository. Provisioning via Ansible for resources that need to be provisioned on an OS. Orchestration via Helm + Helmsfile for resources that need to be provisioned in Kubernetes (with Docker as preferred container engine). Infra as code deployments should run from the new ID-XXX or master branch. ID-XXX branch must be merged immediately (ASAP) via PR to the master branch. Consideration: validating that the changes within the code will only affect the desired target resources is the responsibility of the executor (to ensure everything is OK please consider exec after review/approved PR). All resources will be deployed in several new AWS accounts created inside the Client AWS Organization. Except for the AWS Legacy Account invitation to the AWS Org and OrganizationAccountAccessRole creation in it, there will be no intervention whatsoever in current Client Legacy Production account, unless required by Client authority and given a specific requirement. Info We will explore the details of all the relevant Client application stacks, CI/CD processes, monitoring, security, target service level objective (SLO) and others in a separate document.","title":"Important Considerations"},{"location":"how-it-works/read-more/","text":"Read more \u00b6 Please consider some official AWS docs, blog post and whitepapers we've considered for the current Reference Solutions Architecture design: AWS Reference Articles CloudTrail for AWS Organizations: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html Reserved Instances - Multi Account: https://aws.amazon.com/about-aws/whats-new/2019/07/amazon-ec2-on-demand-capacity-reservations-shared-across-multiple-aws-accounts/ AWS Multiple Account Security Strategy: https://d0.awsstatic.com/aws-answers/AWS_Multi_Account_Security_Strategy.pdf AWS Multiple Account Billing Strategy: https://aws.amazon.com/answers/account-management/aws-multi-account-billing-strategy/ AWS Secure Account Setup: https://aws.amazon.com/answers/security/aws-secure-account-setup/ Authentication and Access Control for AWS Organizations: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions.html AWS Regions: https://www.concurrencylabs.com/blog/choose-your-aws-region-wisely/ VPC Peering: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html Route53 DNS VPC Associations: https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/ AWS Well Architected Framework: https://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/ AWS Tagging strategies: https://aws.amazon.com/answers/account-management/aws-tagging-strategies/ Inviting an AWS Account to Join Your Organization : https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html","title":"Read More"},{"location":"how-it-works/read-more/#read-more","text":"Please consider some official AWS docs, blog post and whitepapers we've considered for the current Reference Solutions Architecture design: AWS Reference Articles CloudTrail for AWS Organizations: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/creating-trail-organization.html Reserved Instances - Multi Account: https://aws.amazon.com/about-aws/whats-new/2019/07/amazon-ec2-on-demand-capacity-reservations-shared-across-multiple-aws-accounts/ AWS Multiple Account Security Strategy: https://d0.awsstatic.com/aws-answers/AWS_Multi_Account_Security_Strategy.pdf AWS Multiple Account Billing Strategy: https://aws.amazon.com/answers/account-management/aws-multi-account-billing-strategy/ AWS Secure Account Setup: https://aws.amazon.com/answers/security/aws-secure-account-setup/ Authentication and Access Control for AWS Organizations: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions.html AWS Regions: https://www.concurrencylabs.com/blog/choose-your-aws-region-wisely/ VPC Peering: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html Route53 DNS VPC Associations: https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/ AWS Well Architected Framework: https://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/ AWS Tagging strategies: https://aws.amazon.com/answers/account-management/aws-tagging-strategies/ Inviting an AWS Account to Join Your Organization : https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html","title":"Read more"},{"location":"how-it-works/why/","text":"Why we choose our tech stack \u00b6 Why AWS\u2753 Amazon Web Services (AWS) is the world\u2019s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. Millions of customers\u2014including the fastest-growing startups, largest enterprises, and leading government agencies\u2014are using AWS to lower costs, become more agile, and innovate faster. Build, Deploy, and Manage Websites, Apps or Processes On AWS' Secure, Reliable Network. AWS is Secure, Reliable, Scalable Services. HIPAA Compliant. Easily Manage Clusters. Global Infrastructure. Highly Scalable. Read More: What is AWS Why WAF (Well Architected Framework)\u2753 AWS Well-Architected helps cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications and workloads. Based on five pillars \u2014 operational excellence, security, reliability, performance efficiency, and cost optimization \u2014 AWS Well-Architected provides a consistent approach for customers and partners to evaluate architectures, and implement designs that can scale over time. Read More: AWS Well-architected Why Infra as Code (IaC) & Terraform\u2753 Confidence: A change breaks the env? Just roll it back. Still not working? Build a whole new env with a few keystrokes. IaC enables this. Repeatability: Allows your infra to be automatically instantiated, making it easy to build multiple identical envs. Troubleshooting: Check source control and see exactly what changed in the env. As long as you are diligent and don\u2019t make manual envs changes, then IaC can be a game changer. DR: Require the ability to set up an alternate env in a different DC or Region. IaC makes this a much more manageable prospect. Auditability: You will need to be able to audit both changes and access to an env, IaC gives you this right out of the box. Visibility: As an env expands over time, is challenging to tell what has been provisioned. In the #cloud this can be a huge #cost issue. IaC allows tracking your resources. Portability: Some IaC techs are #multicloud. Also, translating #Terraform from one cloud provider to another is considerably more simple than recreating your entire envs in a cloud-specific tool. Security: See history of changes to your SG rules along with commit messages can do wonders for being confident about the security configs of your envs. Terraform allows to codify your application infrastructure, reduce human error and increase automation by provisioning infrastructure as code. With TF we can manage infrastructure across clouds and provision infrastructure across 300+ public clouds and services using a single workflow. Moreover it helps to create reproducible infrastructure and provision consistent testing, staging, and production environments with the same configuration. Terraform has everything we expect from a IaC framework: open source, cloud-agnostic provisioning tool that supported immutable infrastructure, a declarative language, and a client-only architecture. Read More Why Infrastructure as Code Why Terraform by Gruntwork Why Organizations\u2753 AWS Organizations helps you centrally manage and govern your environment as you grow and scale your AWS resources. Using AWS Organizations, you can programmatically create new AWS accounts and allocate resources, group accounts to organize your workflows, apply policies to accounts or groups for governance, and simplify billing by using a single payment method for all of your accounts. Read More How it works: AWS Organizations AWS Organizations Why AIM and roles\u2753 AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. Integration and Fine-grained access control with almost every AWS service and its resources. Multi-factor authentication for highly privileged users. Analyze, monitor and audit access. Read More How it works: AWS IAM AWS Identity and Access Management (IAM) Security | Why Web Application Firewall (WAF), Cloud Trail, Config, Guarduty\u2753 Raise your security posture with AWS infrastructure and services. Using AWS, you will gain the control and confidence you need to securely run your business with the most flexible and secure cloud computing environment available today. As an AWS customer, you will benefit from AWS data centers and a network architected to protect your information, identities, applications, and devices. With AWS, you can improve your ability to meet core security and compliance requirements, such as data locality, protection, and confidentiality with our comprehensive services and features. Read More How it works: AWS Security AWS Cloud Security Why VPC\u2753 Amazon Virtual Private Cloud (Amazon VPC) is a service that lets you launch AWS resources in a logically isolated virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 for most resources in your virtual private cloud, helping to ensure secure and easy access to resources and applications. Read More How it works: AWS Networking AWS Virtual Private Cloud Why Kubernetes (K8s) & AWS EKS\u2753 Kubernetes , also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery. Kubernetes builds upon 15 years of experience of running production workloads at Google, combined with best-of-breed ideas and practices from the community. Amazon Elastic Kubernetes Service (Amazon EKS) gives you the flexibility to start, run, and scale Kubernetes applications in the AWS cloud or on-premises. Amazon EKS helps you provide highly-available and secure clusters and automates key tasks such as patching, node provisioning, and updates. Customers such as Intel, Snap, Intuit, GoDaddy, and Autodesk trust EKS to run their most sensitive and mission critical applications. EKS runs upstream Kubernetes and is certified Kubernetes conformant for a predictable experience. You can easily migrate any standard Kubernetes application to EKS without needing to refactor your code. Read More How it works: AWS EKS AWS EKS Kubernetes Why S3\u2753 Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world. Read More How it works: AWS Storage AWS S3 Why RDS\u2753 Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. It frees you to focus on your applications so you can give them the fast performance, high availability, security and compatibility they need. Amazon RDS is available on several database instance types - optimized for memory, performance or I/O - and provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle Database, and SQL Server. You can use the AWS Database Migration Service to easily migrate or replicate your existing databases to Amazon RDS. Read More How it works: AWS Databases AWS RDS Why Hashicorp Vault\u2753 As many organizations migrate to the public cloud, a major concern has been how to best secure data, preventing it from unauthorized access or exfiltration. Deploying a product like HashiCorp Vault gives you better control of your sensitive credentials and helps you meet cloud security standards. HashiCorp Vault is designed to help organizations manage access to secrets and transmit them safely within an organization. Secrets are defined as any form of sensitive credentials that need to be tightly controlled and monitored and can be used to unlock sensitive information. Secrets could be in the form of passwords, API keys, SSH keys, RSA tokens, or OTP. HashiCorp Vault makes it very easy to control and manage access by providing you with a unilateral interface to manage every secret in your infrastructure. Not only that, you can also create detailed audit logs and keep track of who accessed what. Manage Secrets and Protect Sensitive Data. Secure, store and tightly control access to tokens, passwords, certificates, encryption keys for protecting secrets and other sensitive data using a UI, CLI, or HTTP API. Read More How it works: Secrets Hashicorp Vault Project","title":"Why our stack?"},{"location":"how-it-works/why/#why-we-choose-our-tech-stack","text":"Why AWS\u2753 Amazon Web Services (AWS) is the world\u2019s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. Millions of customers\u2014including the fastest-growing startups, largest enterprises, and leading government agencies\u2014are using AWS to lower costs, become more agile, and innovate faster. Build, Deploy, and Manage Websites, Apps or Processes On AWS' Secure, Reliable Network. AWS is Secure, Reliable, Scalable Services. HIPAA Compliant. Easily Manage Clusters. Global Infrastructure. Highly Scalable. Read More: What is AWS Why WAF (Well Architected Framework)\u2753 AWS Well-Architected helps cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications and workloads. Based on five pillars \u2014 operational excellence, security, reliability, performance efficiency, and cost optimization \u2014 AWS Well-Architected provides a consistent approach for customers and partners to evaluate architectures, and implement designs that can scale over time. Read More: AWS Well-architected Why Infra as Code (IaC) & Terraform\u2753 Confidence: A change breaks the env? Just roll it back. Still not working? Build a whole new env with a few keystrokes. IaC enables this. Repeatability: Allows your infra to be automatically instantiated, making it easy to build multiple identical envs. Troubleshooting: Check source control and see exactly what changed in the env. As long as you are diligent and don\u2019t make manual envs changes, then IaC can be a game changer. DR: Require the ability to set up an alternate env in a different DC or Region. IaC makes this a much more manageable prospect. Auditability: You will need to be able to audit both changes and access to an env, IaC gives you this right out of the box. Visibility: As an env expands over time, is challenging to tell what has been provisioned. In the #cloud this can be a huge #cost issue. IaC allows tracking your resources. Portability: Some IaC techs are #multicloud. Also, translating #Terraform from one cloud provider to another is considerably more simple than recreating your entire envs in a cloud-specific tool. Security: See history of changes to your SG rules along with commit messages can do wonders for being confident about the security configs of your envs. Terraform allows to codify your application infrastructure, reduce human error and increase automation by provisioning infrastructure as code. With TF we can manage infrastructure across clouds and provision infrastructure across 300+ public clouds and services using a single workflow. Moreover it helps to create reproducible infrastructure and provision consistent testing, staging, and production environments with the same configuration. Terraform has everything we expect from a IaC framework: open source, cloud-agnostic provisioning tool that supported immutable infrastructure, a declarative language, and a client-only architecture. Read More Why Infrastructure as Code Why Terraform by Gruntwork Why Organizations\u2753 AWS Organizations helps you centrally manage and govern your environment as you grow and scale your AWS resources. Using AWS Organizations, you can programmatically create new AWS accounts and allocate resources, group accounts to organize your workflows, apply policies to accounts or groups for governance, and simplify billing by using a single payment method for all of your accounts. Read More How it works: AWS Organizations AWS Organizations Why AIM and roles\u2753 AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. Integration and Fine-grained access control with almost every AWS service and its resources. Multi-factor authentication for highly privileged users. Analyze, monitor and audit access. Read More How it works: AWS IAM AWS Identity and Access Management (IAM) Security | Why Web Application Firewall (WAF), Cloud Trail, Config, Guarduty\u2753 Raise your security posture with AWS infrastructure and services. Using AWS, you will gain the control and confidence you need to securely run your business with the most flexible and secure cloud computing environment available today. As an AWS customer, you will benefit from AWS data centers and a network architected to protect your information, identities, applications, and devices. With AWS, you can improve your ability to meet core security and compliance requirements, such as data locality, protection, and confidentiality with our comprehensive services and features. Read More How it works: AWS Security AWS Cloud Security Why VPC\u2753 Amazon Virtual Private Cloud (Amazon VPC) is a service that lets you launch AWS resources in a logically isolated virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 for most resources in your virtual private cloud, helping to ensure secure and easy access to resources and applications. Read More How it works: AWS Networking AWS Virtual Private Cloud Why Kubernetes (K8s) & AWS EKS\u2753 Kubernetes , also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery. Kubernetes builds upon 15 years of experience of running production workloads at Google, combined with best-of-breed ideas and practices from the community. Amazon Elastic Kubernetes Service (Amazon EKS) gives you the flexibility to start, run, and scale Kubernetes applications in the AWS cloud or on-premises. Amazon EKS helps you provide highly-available and secure clusters and automates key tasks such as patching, node provisioning, and updates. Customers such as Intel, Snap, Intuit, GoDaddy, and Autodesk trust EKS to run their most sensitive and mission critical applications. EKS runs upstream Kubernetes and is certified Kubernetes conformant for a predictable experience. You can easily migrate any standard Kubernetes application to EKS without needing to refactor your code. Read More How it works: AWS EKS AWS EKS Kubernetes Why S3\u2753 Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world. Read More How it works: AWS Storage AWS S3 Why RDS\u2753 Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. It frees you to focus on your applications so you can give them the fast performance, high availability, security and compatibility they need. Amazon RDS is available on several database instance types - optimized for memory, performance or I/O - and provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle Database, and SQL Server. You can use the AWS Database Migration Service to easily migrate or replicate your existing databases to Amazon RDS. Read More How it works: AWS Databases AWS RDS Why Hashicorp Vault\u2753 As many organizations migrate to the public cloud, a major concern has been how to best secure data, preventing it from unauthorized access or exfiltration. Deploying a product like HashiCorp Vault gives you better control of your sensitive credentials and helps you meet cloud security standards. HashiCorp Vault is designed to help organizations manage access to secrets and transmit them safely within an organization. Secrets are defined as any form of sensitive credentials that need to be tightly controlled and monitored and can be used to unlock sensitive information. Secrets could be in the form of passwords, API keys, SSH keys, RSA tokens, or OTP. HashiCorp Vault makes it very easy to control and manage access by providing you with a unilateral interface to manage every secret in your infrastructure. Not only that, you can also create detailed audit logs and keep track of who accessed what. Manage Secrets and Protect Sensitive Data. Secure, store and tightly control access to tokens, passwords, certificates, encryption keys for protecting secrets and other sensitive data using a UI, CLI, or HTTP API. Read More How it works: Secrets Hashicorp Vault Project","title":"Why we choose our tech stack"},{"location":"how-it-works/cdn/cdn/","text":"CDN \u00b6 AWS Cloud Front Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront is integrated with AWS \u2013 both physical locations that are directly connected to the AWS global infrastructure, as well as other AWS services. CloudFront works seamlessly with services including AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing, API Gateway or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code closer to customers\u2019 users and to customize the user experience. Lastly, if you use AWS origins such as Amazon S3, Amazon EC2 or Elastic Load Balancing, you don\u2019t pay for any data transferred between these services and CloudFront. Load Balancer (ALB | NLB) & S3 Cloudfront Origins \u00b6 Figure: AWS CloudFront with ELB and S3 as origin diagram. (Source: Lee Atkinson, \"How to Help Achieve Mobile App Transport Security (ATS) Compliance by Using Amazon CloudFront and AWS Certificate Manager\" , AWS Security Blog, accessed November 17th 2020). API Gateway Cloudfront Origins \u00b6 Figure: AWS CloudFront with API Gateway as origin diagram. (Source: AWS, \"AWS Solutions Library AWS Solutions Implementations Serverless Image Handler\" , AWS Solutions Library Solutions Implementations, accessed November 17th 2020).","title":"CDN"},{"location":"how-it-works/cdn/cdn/#cdn","text":"AWS Cloud Front Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront is integrated with AWS \u2013 both physical locations that are directly connected to the AWS global infrastructure, as well as other AWS services. CloudFront works seamlessly with services including AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing, API Gateway or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code closer to customers\u2019 users and to customize the user experience. Lastly, if you use AWS origins such as Amazon S3, Amazon EC2 or Elastic Load Balancing, you don\u2019t pay for any data transferred between these services and CloudFront.","title":"CDN"},{"location":"how-it-works/cdn/cdn/#load-balancer-alb-nlb-s3-cloudfront-origins","text":"Figure: AWS CloudFront with ELB and S3 as origin diagram. (Source: Lee Atkinson, \"How to Help Achieve Mobile App Transport Security (ATS) Compliance by Using Amazon CloudFront and AWS Certificate Manager\" , AWS Security Blog, accessed November 17th 2020).","title":"Load Balancer (ALB | NLB) &amp; S3 Cloudfront Origins"},{"location":"how-it-works/cdn/cdn/#api-gateway-cloudfront-origins","text":"Figure: AWS CloudFront with API Gateway as origin diagram. (Source: AWS, \"AWS Solutions Library AWS Solutions Implementations Serverless Image Handler\" , AWS Solutions Library Solutions Implementations, accessed November 17th 2020).","title":"API Gateway Cloudfront Origins"},{"location":"how-it-works/ci-cd/ci-cd/","text":"Continuous Integration / Continuous Delivery (CI/CD) \u00b6 Opt-1: Jenkins + ArgoCD \u00b6 Figure: ACI/CD with Jenkins + ArgoCD architecture diagram. (Source: ArgoCD, \"Overview - What Is Argo CD\" , ArgoCD documentation, accessed November 18th 2020). Opt-2: Jenkins + Spinnaker \u00b6 Figure: CI/CD with Jenkins + Spinnaker diagram. (Source: Irshad Buchh, \"Continuous Delivery using Spinnaker on Amazon EKS\" , AWS Open Source Blog, accessed November 18th 2020).","title":"CI/CD"},{"location":"how-it-works/ci-cd/ci-cd/#continuous-integration-continuous-delivery-cicd","text":"","title":"Continuous Integration / Continuous Delivery (CI/CD)"},{"location":"how-it-works/ci-cd/ci-cd/#opt-1-jenkins-argocd","text":"Figure: ACI/CD with Jenkins + ArgoCD architecture diagram. (Source: ArgoCD, \"Overview - What Is Argo CD\" , ArgoCD documentation, accessed November 18th 2020).","title":"Opt-1: Jenkins + ArgoCD"},{"location":"how-it-works/ci-cd/ci-cd/#opt-2-jenkins-spinnaker","text":"Figure: CI/CD with Jenkins + Spinnaker diagram. (Source: Irshad Buchh, \"Continuous Delivery using Spinnaker on Amazon EKS\" , AWS Open Source Blog, accessed November 18th 2020).","title":"Opt-2: Jenkins + Spinnaker"},{"location":"how-it-works/code-library/code-library-forks/","text":"Leverage Open Source Modules management. \u00b6 We\u2019ll fork every DevOps Automation Code Library dependency repo, why? Grant full governance over the lib repositories Availability: Since our project resilience and continuity (including the clients) depends on this repositories (via requirement files or imports) and we want and need total control over the repository used as a dependency. NOTE: There could be few exceptions when using official open source modules such as the ones shared and maintained by Nginx, Weave, Hashiport, etc. Reliability (Avoid unforeseen events): in the event that the original project becomes discontinued while we are still working or depending on it (the owners, generally individual maintainers of the original repository might decide to move from github, ansible galaxy, etc or even close their repo for personal reasons). Stability: Our forks form modules (ansible roles / terraform / dockerfiles, etc) are always going to be locked to fixed versions for every client so no unexpected behavior will occur. Projects that don't tag versions: having the fork protects us against breaking changes. Write access: to every Leverage library component repository ensuring at all times that we can support, update, maintain, test, customize and release a new version of this component. Centralized Org source of truth: for improved customer experience and keeping dependencies consistently imported from binbash repos at Leverage Github Scope: Binbash grants and responds for all this dependencies. Metrics: Dashboards w/ internal measurements. Automation: We\u2019ll maintain all this workflow cross-tech as standardized and automated as possible adding any extra validation like testing, security check, etc if needed -> Leverage dev-tools Licence & Ownership: Since we fork open-source and commercially reusable components w/ MIT and Apache 2.0 license . We keep full rights to all commercial, modification, distribution, and private use of the code (No Lock-In w/ owners) through forks inside our own Leverage Project repos. As a result, when time comes, we can make our libs private at any moment if necessary for the time being Open Source looks like the best option). Collaborators considerations We look forward to have every Binbash Leverage repo open sourced favoring the collaboration of the open source community. Repos that are still private must not be forked by our internal collaborators till we've done a detailed and rigorous review in order to open source them. As a result any person looking forward to use, extend or update Leverage public repos could also fork them in its personal or company Github account and create an upstream PR to contribute.","title":"Forks workflow"},{"location":"how-it-works/code-library/code-library-forks/#leverage-open-source-modules-management","text":"We\u2019ll fork every DevOps Automation Code Library dependency repo, why? Grant full governance over the lib repositories Availability: Since our project resilience and continuity (including the clients) depends on this repositories (via requirement files or imports) and we want and need total control over the repository used as a dependency. NOTE: There could be few exceptions when using official open source modules such as the ones shared and maintained by Nginx, Weave, Hashiport, etc. Reliability (Avoid unforeseen events): in the event that the original project becomes discontinued while we are still working or depending on it (the owners, generally individual maintainers of the original repository might decide to move from github, ansible galaxy, etc or even close their repo for personal reasons). Stability: Our forks form modules (ansible roles / terraform / dockerfiles, etc) are always going to be locked to fixed versions for every client so no unexpected behavior will occur. Projects that don't tag versions: having the fork protects us against breaking changes. Write access: to every Leverage library component repository ensuring at all times that we can support, update, maintain, test, customize and release a new version of this component. Centralized Org source of truth: for improved customer experience and keeping dependencies consistently imported from binbash repos at Leverage Github Scope: Binbash grants and responds for all this dependencies. Metrics: Dashboards w/ internal measurements. Automation: We\u2019ll maintain all this workflow cross-tech as standardized and automated as possible adding any extra validation like testing, security check, etc if needed -> Leverage dev-tools Licence & Ownership: Since we fork open-source and commercially reusable components w/ MIT and Apache 2.0 license . We keep full rights to all commercial, modification, distribution, and private use of the code (No Lock-In w/ owners) through forks inside our own Leverage Project repos. As a result, when time comes, we can make our libs private at any moment if necessary for the time being Open Source looks like the best option). Collaborators considerations We look forward to have every Binbash Leverage repo open sourced favoring the collaboration of the open source community. Repos that are still private must not be forked by our internal collaborators till we've done a detailed and rigorous review in order to open source them. As a result any person looking forward to use, extend or update Leverage public repos could also fork them in its personal or company Github account and create an upstream PR to contribute.","title":"Leverage Open Source Modules management."},{"location":"how-it-works/code-library/code-library-specs/","text":"Tech Specifications \u00b6 As Code: Hundred of thousands lines of code written in Terraform Groovy (Jenkinsfiles) Ansible Makefiles + Bash Dockerfiles Helm Charts Stop reinventing the wheel, automated and fully as code automated (executable from a single source). as code. parameterized variables input parameters return / output parameters \"Stop reinventing the wheel\" avoid re-building the same things more than X times. avoid wasting time. not healthy, not secure and slows us down. DoD of a highly reusable, configurable, and composible sub-modules which will be 100% modular equivalent to other programming languages functions - Example for terraform - https://www.terraform.io/docs/modules/usage.html (but can be propagated for other languages and tools): inputs, outputs parameters. code reuse (reusable): consider tf modules and sub-modules approach. testable by module / function. Since TF it's oriented to work through 3rd party API calls, then tests are more likely to be integration tests rather than unit tests . If we don't allow integration for terraform then we can't work at all. This has to be analyzed for every language we'll be using and how we implement it (terraform, cloudformation, ansible, python, bash, docker, kops and k8s kubeclt cmds) composition (composible): have multiple functions and use it together eg: def_add(x,y){return x+y} ; def_sub(x,y){return x-y}; sub(add(3,4), add(7,5)) abstraction (abstract away complexity): we have a very complex function but we only expose it's definition to the API, eg: def_ai_processing(data_set){very complex algorithm here}; ai_processing([our_data_set_here]) Avoid inline: blocks The configuration for some Terraform resources can be defined either as inline blocks or as separate resources. When creating a module, you should always use a separate resource. For example, the aws_route_table resource allows you to define routes via inline blocks. Otherwise, your module will be less flexible and configurable. So If you try to use a mix of both inline blocks and separate resources, you run into bugs where they will conflict and overwrite each other. Therefore, you must use one or the other (ref: https://blog.gruntwork.io/how-to-create-reusable-infrastructure-with-terraform-modules-25526d65f73d )) Use module-relative paths: The catch is that the file path you use has to be relative (since you could run Terraform on many different computers)\u200a\u2014\u200abut what is it relative to? By default, Terraform interprets the path relative to the working directory. That\u2019s a good default for normal Terraform templates, but it won\u2019t work if the file is part of a module. To solve this issue, always use a path variable in file paths. eg: resource \"aws_instance\" \"example\" { ami = \"ami-2d39803a\" instance_type = \"t2.micro\" user_data = \"${file(\"${path.module}/user-data.sh\")}\" } Solutions must be versioned So as to be able to manage them as a software product with releases and change log. In this way we'll be able to know which version is currently deployed in a certain client and consider it's upgrade. Env Parity Promote inmutable, versioned infra modules based across envs. Updated Continually make updates, additions, and fixes to the libraries and modules. Integrated \"push button environments (PBE)\" approach for our solutions. - TODO: FLOW DIAGRAM HERE Proven & Tested Customers & every commit goes through a suite of automated tests to gran code styling and functional testing. Develop a wrapper/jobs together with specific testing tools in order to grant the modules are working as expected. Ansible: Testing your ansible roles w/ molecule How to test ansible roles with molecule on ubuntu Terraform: gruntwork-io/terratest Cost savings The architecture for our Library / Code Modules helps an organization to analyze its current IT and DevSecOps Cloud strategy and identify areas where changes could lead to cost savings. For instance, the architecture may show that multiple database systems could be changed so only one product is used, reducing software and support costs. Provides a basis for reuse.The process of architecting can support both the use and creation of reusable assets. Reusable assets are beneficial to an organization, since they can reduce the overall cost of a system and also improve its quality, given that a reusable asset has already been proven. Full Code Access & No Lock-In You get access to 100% of the code under Open Source license ( https://choosealicense.com/ ) so If you ever choose to cancel, you keep rights to all the code. Documented Includes example code, use case and thorough documentation, such as README.md , --help command, doc-string and in line comments. Supported & Customizable Commercially maintained and supported by Binbash .","title":"Specifications"},{"location":"how-it-works/code-library/code-library-specs/#tech-specifications","text":"As Code: Hundred of thousands lines of code written in Terraform Groovy (Jenkinsfiles) Ansible Makefiles + Bash Dockerfiles Helm Charts Stop reinventing the wheel, automated and fully as code automated (executable from a single source). as code. parameterized variables input parameters return / output parameters \"Stop reinventing the wheel\" avoid re-building the same things more than X times. avoid wasting time. not healthy, not secure and slows us down. DoD of a highly reusable, configurable, and composible sub-modules which will be 100% modular equivalent to other programming languages functions - Example for terraform - https://www.terraform.io/docs/modules/usage.html (but can be propagated for other languages and tools): inputs, outputs parameters. code reuse (reusable): consider tf modules and sub-modules approach. testable by module / function. Since TF it's oriented to work through 3rd party API calls, then tests are more likely to be integration tests rather than unit tests . If we don't allow integration for terraform then we can't work at all. This has to be analyzed for every language we'll be using and how we implement it (terraform, cloudformation, ansible, python, bash, docker, kops and k8s kubeclt cmds) composition (composible): have multiple functions and use it together eg: def_add(x,y){return x+y} ; def_sub(x,y){return x-y}; sub(add(3,4), add(7,5)) abstraction (abstract away complexity): we have a very complex function but we only expose it's definition to the API, eg: def_ai_processing(data_set){very complex algorithm here}; ai_processing([our_data_set_here]) Avoid inline: blocks The configuration for some Terraform resources can be defined either as inline blocks or as separate resources. When creating a module, you should always use a separate resource. For example, the aws_route_table resource allows you to define routes via inline blocks. Otherwise, your module will be less flexible and configurable. So If you try to use a mix of both inline blocks and separate resources, you run into bugs where they will conflict and overwrite each other. Therefore, you must use one or the other (ref: https://blog.gruntwork.io/how-to-create-reusable-infrastructure-with-terraform-modules-25526d65f73d )) Use module-relative paths: The catch is that the file path you use has to be relative (since you could run Terraform on many different computers)\u200a\u2014\u200abut what is it relative to? By default, Terraform interprets the path relative to the working directory. That\u2019s a good default for normal Terraform templates, but it won\u2019t work if the file is part of a module. To solve this issue, always use a path variable in file paths. eg: resource \"aws_instance\" \"example\" { ami = \"ami-2d39803a\" instance_type = \"t2.micro\" user_data = \"${file(\"${path.module}/user-data.sh\")}\" } Solutions must be versioned So as to be able to manage them as a software product with releases and change log. In this way we'll be able to know which version is currently deployed in a certain client and consider it's upgrade. Env Parity Promote inmutable, versioned infra modules based across envs. Updated Continually make updates, additions, and fixes to the libraries and modules. Integrated \"push button environments (PBE)\" approach for our solutions. - TODO: FLOW DIAGRAM HERE Proven & Tested Customers & every commit goes through a suite of automated tests to gran code styling and functional testing. Develop a wrapper/jobs together with specific testing tools in order to grant the modules are working as expected. Ansible: Testing your ansible roles w/ molecule How to test ansible roles with molecule on ubuntu Terraform: gruntwork-io/terratest Cost savings The architecture for our Library / Code Modules helps an organization to analyze its current IT and DevSecOps Cloud strategy and identify areas where changes could lead to cost savings. For instance, the architecture may show that multiple database systems could be changed so only one product is used, reducing software and support costs. Provides a basis for reuse.The process of architecting can support both the use and creation of reusable assets. Reusable assets are beneficial to an organization, since they can reduce the overall cost of a system and also improve its quality, given that a reusable asset has already been proven. Full Code Access & No Lock-In You get access to 100% of the code under Open Source license ( https://choosealicense.com/ ) so If you ever choose to cancel, you keep rights to all the code. Documented Includes example code, use case and thorough documentation, such as README.md , --help command, doc-string and in line comments. Supported & Customizable Commercially maintained and supported by Binbash .","title":"Tech Specifications"},{"location":"how-it-works/code-library/code-library/","text":"Leverage DevOps Automation Code Library \u00b6 Overview \u00b6 A collection of reusable, tested, production-ready E2E infrastructure as code solutions, leveraged by modules written in Terraform, Ansible, Jenkinsfiles, Dockerfiles, Helm charts and Makefiles). Model \u00b6 Our development model is strongly based on code reusability Reusability \u00b6 High level summary of the the code reusability efficiency Considerations Above detailed % are to be seen as estimates AWS PCI-DSS Reference article AWS HIPAA Reference article AWS GDPR Reference article Modules \u00b6 DevOps Automation Code Library development and implementation workflow","title":"Overview"},{"location":"how-it-works/code-library/code-library/#leverage-devops-automation-code-library","text":"","title":"Leverage DevOps Automation Code Library"},{"location":"how-it-works/code-library/code-library/#overview","text":"A collection of reusable, tested, production-ready E2E infrastructure as code solutions, leveraged by modules written in Terraform, Ansible, Jenkinsfiles, Dockerfiles, Helm charts and Makefiles).","title":"Overview"},{"location":"how-it-works/code-library/code-library/#model","text":"Our development model is strongly based on code reusability","title":"Model"},{"location":"how-it-works/code-library/code-library/#reusability","text":"High level summary of the the code reusability efficiency Considerations Above detailed % are to be seen as estimates AWS PCI-DSS Reference article AWS HIPAA Reference article AWS GDPR Reference article","title":"Reusability"},{"location":"how-it-works/code-library/code-library/#modules","text":"DevOps Automation Code Library development and implementation workflow","title":"Modules"},{"location":"how-it-works/code-library/modules-library-per-tech/","text":"DevOps Code Automation Library Modules \u00b6 Open Source Modules Repos \u00b6 Category URLs Ansible Galaxy Roles bb-leverage-ansible-roles-list Dockerfiles bb-leverage-dockerfiles-list Helm Charts bb-leverage-helm-charts-list Jenkinsfiles Library bb-leverage-jenkinsfiles-lib Terraform Modules bb-leverage-terraform-modules-list Open Source + Private Modules Repos (via GitHub Teams) \u00b6 Repositories Details Reference Architecture Most of the AWS resources are here, divided by account. Dockerfiles These are Terraform module we created/imported to build reusable resources / stacks. Ansible Playbooks & Roles Playbooks we use for provisioning servers such as Jenkins, Spinnaker, Vault, and so on. Jenkins Modules Module we use in our Jenkins pipelines to perform repeated tasks such as posting to Slack, interacting with AWS CLI, etc. Helm Charts Complementary Jenkins pipelines to clean docker images, unseal Vault, and more. Also SecOps jobs can be found here. Terraform Modules Jenkins pipelines, docker images, and other resources used for load testing.","title":"Modules per Tech"},{"location":"how-it-works/code-library/modules-library-per-tech/#devops-code-automation-library-modules","text":"","title":"DevOps Code Automation Library Modules"},{"location":"how-it-works/code-library/modules-library-per-tech/#open-source-modules-repos","text":"Category URLs Ansible Galaxy Roles bb-leverage-ansible-roles-list Dockerfiles bb-leverage-dockerfiles-list Helm Charts bb-leverage-helm-charts-list Jenkinsfiles Library bb-leverage-jenkinsfiles-lib Terraform Modules bb-leverage-terraform-modules-list","title":"Open Source Modules Repos"},{"location":"how-it-works/code-library/modules-library-per-tech/#open-source-private-modules-repos-via-github-teams","text":"Repositories Details Reference Architecture Most of the AWS resources are here, divided by account. Dockerfiles These are Terraform module we created/imported to build reusable resources / stacks. Ansible Playbooks & Roles Playbooks we use for provisioning servers such as Jenkins, Spinnaker, Vault, and so on. Jenkins Modules Module we use in our Jenkins pipelines to perform repeated tasks such as posting to Slack, interacting with AWS CLI, etc. Helm Charts Complementary Jenkins pipelines to clean docker images, unseal Vault, and more. Also SecOps jobs can be found here. Terraform Modules Jenkins pipelines, docker images, and other resources used for load testing.","title":"Open Source + Private Modules Repos (via GitHub Teams)"},{"location":"how-it-works/compute/k8s-eks/","text":"Kubernetes AWS EKS \u00b6 Amazon Elastic Kubernetes Services (EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes. Core Feautres Highly Secure: EKS automatically applies the latest security patches to your cluster control plane. Multiple Availability Zones: EKS auto-detects and replaces unhealthy control plane nodes and provides on-demand, zero downtime upgrades and patching. Serverless Compute: EKS supports AWS Fargate to remove the need to provision and manage servers, improving security through application isolation by design. Built with the Community: AWS actively works with the Kubernetes community, including making contributions to the Kubernetes code base helping you take advantage of AWS services. Figure: AWS K8s EKS architecture diagram (just as reference). (Source: Jay McConnell, \"A tale from the trenches: The CloudBees Core on AWS Quick Start\" , AWS Infrastructure & Automation Blog post, accessed November 18th 2020). Version support convention \u00b6 At Leverage we support the last 3 latest stable Kubernetes version releases https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html We think this is a good balance between management overhead and an acceptable level of supported versions. If your project have and older legacy version we could work along your CloudOps team to safely migrate it to a Leverage supported EKS version.","title":"K8s EKS"},{"location":"how-it-works/compute/k8s-eks/#kubernetes-aws-eks","text":"Amazon Elastic Kubernetes Services (EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane or worker nodes. Core Feautres Highly Secure: EKS automatically applies the latest security patches to your cluster control plane. Multiple Availability Zones: EKS auto-detects and replaces unhealthy control plane nodes and provides on-demand, zero downtime upgrades and patching. Serverless Compute: EKS supports AWS Fargate to remove the need to provision and manage servers, improving security through application isolation by design. Built with the Community: AWS actively works with the Kubernetes community, including making contributions to the Kubernetes code base helping you take advantage of AWS services. Figure: AWS K8s EKS architecture diagram (just as reference). (Source: Jay McConnell, \"A tale from the trenches: The CloudBees Core on AWS Quick Start\" , AWS Infrastructure & Automation Blog post, accessed November 18th 2020).","title":"Kubernetes AWS EKS"},{"location":"how-it-works/compute/k8s-eks/#version-support-convention","text":"At Leverage we support the last 3 latest stable Kubernetes version releases https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html We think this is a good balance between management overhead and an acceptable level of supported versions. If your project have and older legacy version we could work along your CloudOps team to safely migrate it to a Leverage supported EKS version.","title":"Version support convention"},{"location":"how-it-works/compute/k8s-kops/","text":"Kubernetes Kops \u00b6 Kops is an official Kubernetes project for managing production-grade Kubernetes clusters. Kops is currently the best tool to deploy Kubernetes clusters to Amazon Web Services. The project describes itself as kubectl for clusters. Core Features Open-source & supports AWS and GCE Deploy clusters to existing virtual private clouds (VPC) or create a new VPC from scratch Supports public & private topologies Provisions single or multiple master clusters Configurable bastion machines for SSH access to individual cluster nodes Built on a state-sync model for dry-runs and automatic idempotency Direct infrastructure manipulation, or works with CloudFormation and Terraform Rolling cluster updates Supports heterogeneous clusters by creating multiple instance groups Figure: AWS K8s Kops architecture diagram (just as reference). (Source: Carlos Rodriguez, \"How to deploy a Kubernetes cluster on AWS with Terraform & kops\" , Nclouds.com Blog post, accessed November 18th 2020).","title":"K8s Kops"},{"location":"how-it-works/compute/k8s-kops/#kubernetes-kops","text":"Kops is an official Kubernetes project for managing production-grade Kubernetes clusters. Kops is currently the best tool to deploy Kubernetes clusters to Amazon Web Services. The project describes itself as kubectl for clusters. Core Features Open-source & supports AWS and GCE Deploy clusters to existing virtual private clouds (VPC) or create a new VPC from scratch Supports public & private topologies Provisions single or multiple master clusters Configurable bastion machines for SSH access to individual cluster nodes Built on a state-sync model for dry-runs and automatic idempotency Direct infrastructure manipulation, or works with CloudFormation and Terraform Rolling cluster updates Supports heterogeneous clusters by creating multiple instance groups Figure: AWS K8s Kops architecture diagram (just as reference). (Source: Carlos Rodriguez, \"How to deploy a Kubernetes cluster on AWS with Terraform & kops\" , Nclouds.com Blog post, accessed November 18th 2020).","title":"Kubernetes Kops"},{"location":"how-it-works/compute/k8s-service-mesh/","text":"Service Mesh \u00b6 Overview \u00b6 Ultra light, ultra simple, ultra powerful. Linkerd adds security, observability, and reliability to Kubernetes, without the complexity. CNCF-hosted and 100% open source. How it works \u00b6 How Linkerd works Linkerd works by installing a set of ultralight, transparent proxies next to each service instance. These proxies automatically handle all traffic to and from the service. Because they\u2019re transparent, these proxies act as highly instrumented out-of-process network stacks, sending telemetry to, and receiving control signals from, the control plane. This design allows Linkerd to measure and manipulate traffic to and from your service without introducing excessive latency. Architecture \u00b6 Figure: Figure: Linkerd v2.10 architecture diagram. (Source: Linkerd official documentation, \"High level Linkerd control plane and a data plane.\" , Linkerd Doc, accessed June 14th 2021). Dashboard \u00b6 Figure: Figure: Linkerd v2.10 dashboard. (Source: Linkerd official documentation, \"Linkerd dashboard\" , Linkerd Doc, accessed June 14th 2021). Read more \u00b6 Related resources Linkerd vs Istio benchamrks","title":"K8s Service Mesh"},{"location":"how-it-works/compute/k8s-service-mesh/#service-mesh","text":"","title":"Service Mesh"},{"location":"how-it-works/compute/k8s-service-mesh/#overview","text":"Ultra light, ultra simple, ultra powerful. Linkerd adds security, observability, and reliability to Kubernetes, without the complexity. CNCF-hosted and 100% open source.","title":"Overview"},{"location":"how-it-works/compute/k8s-service-mesh/#how-it-works","text":"How Linkerd works Linkerd works by installing a set of ultralight, transparent proxies next to each service instance. These proxies automatically handle all traffic to and from the service. Because they\u2019re transparent, these proxies act as highly instrumented out-of-process network stacks, sending telemetry to, and receiving control signals from, the control plane. This design allows Linkerd to measure and manipulate traffic to and from your service without introducing excessive latency.","title":"How it works"},{"location":"how-it-works/compute/k8s-service-mesh/#architecture","text":"Figure: Figure: Linkerd v2.10 architecture diagram. (Source: Linkerd official documentation, \"High level Linkerd control plane and a data plane.\" , Linkerd Doc, accessed June 14th 2021).","title":"Architecture"},{"location":"how-it-works/compute/k8s-service-mesh/#dashboard","text":"Figure: Figure: Linkerd v2.10 dashboard. (Source: Linkerd official documentation, \"Linkerd dashboard\" , Linkerd Doc, accessed June 14th 2021).","title":"Dashboard"},{"location":"how-it-works/compute/k8s-service-mesh/#read-more","text":"Related resources Linkerd vs Istio benchamrks","title":"Read more"},{"location":"how-it-works/compute/overview/","text":"Compute \u00b6 Containers and Serverless \u00b6 Overview In order to serve Client application workloads we propose to implement Kubernetes , and proceed to containerize all application stacks whenever it\u2019s the best solution (we\u2019ll also consider AWS Lambda for a Serverless approach when it fits better). Kubernetes is an open source container orchestration platform that eases the process of running containers across many different machines, scaling up or down by adding or removing containers when demand changes and provides high availability features. Also, it serves as an abstraction layer that will give Client the possibility, with minimal effort, to move the apps to other Kubernetes clusters running elsewhere, or a managed Kubernetes service such as AWS EKS, GCP GKE or others. Clusters will be provisioned with Kops and/or AWS EKS , which are solutions meant to orchestrate this compute engine in AWS. Whenever possible the initial version deployed will be the latest stable release. Figure: Kubernetes high level components architecture. (Source: Andrew Martin, \"11 Ways (Not) to Get Hacked\" , Kubernetes.io Blog post, accessed November 18th 2020). Kubernetes addons \u00b6 Some possible K8s addons could be Security IAM Authenticator Networking Kubernetes Nginx Ingress Controller Linked2 (Service Mesh) Monitoring & Logs fluentd daemonset for elasticsearch logs kube-state-metrics prometheus node-exporter Distributed Tracing jaeger opencensus UI Dashboard kube-ops-view kubernetes-dashboard weave-scope Availability & Reliability autoscaler Velero (Backups) Utilities onetimesecret","title":"Overview"},{"location":"how-it-works/compute/overview/#compute","text":"","title":"Compute"},{"location":"how-it-works/compute/overview/#containers-and-serverless","text":"Overview In order to serve Client application workloads we propose to implement Kubernetes , and proceed to containerize all application stacks whenever it\u2019s the best solution (we\u2019ll also consider AWS Lambda for a Serverless approach when it fits better). Kubernetes is an open source container orchestration platform that eases the process of running containers across many different machines, scaling up or down by adding or removing containers when demand changes and provides high availability features. Also, it serves as an abstraction layer that will give Client the possibility, with minimal effort, to move the apps to other Kubernetes clusters running elsewhere, or a managed Kubernetes service such as AWS EKS, GCP GKE or others. Clusters will be provisioned with Kops and/or AWS EKS , which are solutions meant to orchestrate this compute engine in AWS. Whenever possible the initial version deployed will be the latest stable release. Figure: Kubernetes high level components architecture. (Source: Andrew Martin, \"11 Ways (Not) to Get Hacked\" , Kubernetes.io Blog post, accessed November 18th 2020).","title":"Containers and Serverless"},{"location":"how-it-works/compute/overview/#kubernetes-addons","text":"Some possible K8s addons could be Security IAM Authenticator Networking Kubernetes Nginx Ingress Controller Linked2 (Service Mesh) Monitoring & Logs fluentd daemonset for elasticsearch logs kube-state-metrics prometheus node-exporter Distributed Tracing jaeger opencensus UI Dashboard kube-ops-view kubernetes-dashboard weave-scope Availability & Reliability autoscaler Velero (Backups) Utilities onetimesecret","title":"Kubernetes addons"},{"location":"how-it-works/compute/serverless/","text":"Serverless Compute \u00b6 As stated by AWS Serverless definitions What is serverless? Serverless is the native architecture of the cloud that enables you to shift more of your operational responsibilities to AWS, increasing your agility and innovation. Serverless allows you to build and run applications and services without thinking about servers. It eliminates infrastructure management tasks such as server or cluster provisioning, patching, operating system maintenance, and capacity provisioning. You can build them for nearly any type of application or backend service, and everything required to run and scale your application with high availability is handled for you. Why use serverless? Serverless enables you to build modern applications with increased agility and lower total cost of ownership. Building serverless applications means that your developers can focus on their core product instead of worrying about managing and operating servers or runtimes, either in the cloud or on-premises. This reduced overhead lets developers reclaim time and energy that can be spent on developing great products which scale and that are reliable. Figure: AWS serverless architecture diagram (just as reference). (Source: Nathan Peck, \"Designing a modern serverless application with AWS Lambda and AWS Fargate\" , Containers-on-AWS Medium Blog post, accessed November 18th 2020). Serverless Compute Services AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume - there is no charge when your code is not running. Lambda@Edge allows you to run Lambda functions at AWS Edge locations in response to Amazon CloudFront events. AWS Fargate is a purpose-built serverless compute engine for containers. Fargate scales and manages the infrastructure required to run your containers.","title":"Serverless"},{"location":"how-it-works/compute/serverless/#serverless-compute","text":"As stated by AWS Serverless definitions What is serverless? Serverless is the native architecture of the cloud that enables you to shift more of your operational responsibilities to AWS, increasing your agility and innovation. Serverless allows you to build and run applications and services without thinking about servers. It eliminates infrastructure management tasks such as server or cluster provisioning, patching, operating system maintenance, and capacity provisioning. You can build them for nearly any type of application or backend service, and everything required to run and scale your application with high availability is handled for you. Why use serverless? Serverless enables you to build modern applications with increased agility and lower total cost of ownership. Building serverless applications means that your developers can focus on their core product instead of worrying about managing and operating servers or runtimes, either in the cloud or on-premises. This reduced overhead lets developers reclaim time and energy that can be spent on developing great products which scale and that are reliable. Figure: AWS serverless architecture diagram (just as reference). (Source: Nathan Peck, \"Designing a modern serverless application with AWS Lambda and AWS Fargate\" , Containers-on-AWS Medium Blog post, accessed November 18th 2020). Serverless Compute Services AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume - there is no charge when your code is not running. Lambda@Edge allows you to run Lambda functions at AWS Edge locations in response to Amazon CloudFront events. AWS Fargate is a purpose-built serverless compute engine for containers. Fargate scales and manages the infrastructure required to run your containers.","title":"Serverless Compute"},{"location":"how-it-works/costs/costs/","text":"Cost Estmation & Optimization \u00b6 Opportunity to optimize resources \u00b6 Compute Usage of reserved EC2 instances for stable workloads (AWS Cost Explorer Reserved Optimization | Compute Optimizer - get a -$ of up to 42% vs On-Demand) Usage of Spot EC2 instances for fault-tolerant workloads (-$ by up to 90%). Use ASG to allow your EC2 fleet to +/- based on demand. Id EC2 w/ low-utiliz and -$ by stop / rightsize them. Compute Savings Plans to reduce EC2, Fargate and Lambda $ (Compute Savings Plans OK regardless of EC2 family, size, AZ, reg, OS or tenancy, OK for Fargate / Lambda too). Databases Usage of reserved RDS instances for stable workload databases. Monitoring & Automation AWS billing alarms + AWS Budget (forecasted account cost / RI Coverage) Notifications to Slack Activate AWS Trusted Advisor cost related results Id EBS w/ low-utiliz and -$ by snapshotting and then rm them Check underutilized EBS to be possibly shrinked or removed. Networking -> deleting idle LB -> Use LB check w/ RequestCount of > 100 past 7d. Setup Lambda nuke to automatically clean up AWS account resources. Setup lambda scheduler for stop and start resources on AWS (EC2, ASG & RDS) Storage & Network Traffic Check S3 usage and -$ by leveraging lower $ storage tiers. Use S3 Analytics, or automate mv for these objects into lower $ storage tier w/ Life Cycle Policies or w/ S3 Intelligent-Tiering. If DataTransferOut from EC2 to the public internet is significant $, consider implementing CloudFront. Consideration \u00b6 Reserved Instances Stable workloads will always run on reserved instances, the following calculation only considers 1yr. No Upfront mode, in which Client will not have to pay in advance but commits to this monthly usage and will be billed so, even if the instance type is not used. More aggressive Reservation strategies can be implemented to further reduce costs, these will have to be analyzed by business in conjunction with operations. Read more \u00b6 Reference links Consider the following extra links as reference: AWS Ramp-Up Guide: Cost Management A Guide to Cloud Cost Optimization with HashiCorp Terraform FinOps: How Cloud Finance Management Can Save Your Cloud Programme From Extinction","title":"Costs"},{"location":"how-it-works/costs/costs/#cost-estmation-optimization","text":"","title":"Cost Estmation &amp; Optimization"},{"location":"how-it-works/costs/costs/#opportunity-to-optimize-resources","text":"Compute Usage of reserved EC2 instances for stable workloads (AWS Cost Explorer Reserved Optimization | Compute Optimizer - get a -$ of up to 42% vs On-Demand) Usage of Spot EC2 instances for fault-tolerant workloads (-$ by up to 90%). Use ASG to allow your EC2 fleet to +/- based on demand. Id EC2 w/ low-utiliz and -$ by stop / rightsize them. Compute Savings Plans to reduce EC2, Fargate and Lambda $ (Compute Savings Plans OK regardless of EC2 family, size, AZ, reg, OS or tenancy, OK for Fargate / Lambda too). Databases Usage of reserved RDS instances for stable workload databases. Monitoring & Automation AWS billing alarms + AWS Budget (forecasted account cost / RI Coverage) Notifications to Slack Activate AWS Trusted Advisor cost related results Id EBS w/ low-utiliz and -$ by snapshotting and then rm them Check underutilized EBS to be possibly shrinked or removed. Networking -> deleting idle LB -> Use LB check w/ RequestCount of > 100 past 7d. Setup Lambda nuke to automatically clean up AWS account resources. Setup lambda scheduler for stop and start resources on AWS (EC2, ASG & RDS) Storage & Network Traffic Check S3 usage and -$ by leveraging lower $ storage tiers. Use S3 Analytics, or automate mv for these objects into lower $ storage tier w/ Life Cycle Policies or w/ S3 Intelligent-Tiering. If DataTransferOut from EC2 to the public internet is significant $, consider implementing CloudFront.","title":"Opportunity to optimize resources"},{"location":"how-it-works/costs/costs/#consideration","text":"Reserved Instances Stable workloads will always run on reserved instances, the following calculation only considers 1yr. No Upfront mode, in which Client will not have to pay in advance but commits to this monthly usage and will be billed so, even if the instance type is not used. More aggressive Reservation strategies can be implemented to further reduce costs, these will have to be analyzed by business in conjunction with operations.","title":"Consideration"},{"location":"how-it-works/costs/costs/#read-more","text":"Reference links Consider the following extra links as reference: AWS Ramp-Up Guide: Cost Management A Guide to Cloud Cost Optimization with HashiCorp Terraform FinOps: How Cloud Finance Management Can Save Your Cloud Programme From Extinction","title":"Read more"},{"location":"how-it-works/database/database/","text":"Databases \u00b6 Overview \u00b6 Will implement AWS RDS databases matching the requirements of the current application stacks. If the region selected is the same your're actually using for your legacy AWS RDS instances we will be able to create a peering connection to existing databases in order to migrate the application stacks first, then databases. AWS RDS Specs RDS Instance Size Multi AZ Encryption: Yes Auto Minor version update Automated snapshots Snapshot retention","title":"Databases"},{"location":"how-it-works/database/database/#databases","text":"","title":"Databases"},{"location":"how-it-works/database/database/#overview","text":"Will implement AWS RDS databases matching the requirements of the current application stacks. If the region selected is the same your're actually using for your legacy AWS RDS instances we will be able to create a peering connection to existing databases in order to migrate the application stacks first, then databases. AWS RDS Specs RDS Instance Size Multi AZ Encryption: Yes Auto Minor version update Automated snapshots Snapshot retention","title":"Overview"},{"location":"how-it-works/database/mysql/","text":"RDS | MySQL \u00b6","title":"MySQL"},{"location":"how-it-works/database/mysql/#rds-mysql","text":"","title":"RDS | MySQL"},{"location":"how-it-works/database/postgres/","text":"RDS | PostgresSQL \u00b6","title":"PostgresSQL"},{"location":"how-it-works/database/postgres/#rds-postgressql","text":"","title":"RDS | PostgresSQL"},{"location":"how-it-works/identities/identities/","text":"Identity and Access Management (IAM) Layer \u00b6 Summary \u00b6 Having this official AWS resource as reference we've define a security account structure for managing multiple accounts. User Management Definitions IAM users will strictly be created and centralized in the Security account (member accounts IAM Users could be exceptionally created for very specific tools that still don\u2019t support IAM roles for cross-account auth). All access to resources within the Client organization will be assigned via policy documents attached to IAM roles or groups. All IAM roles and groups will have the least privileges required to properly work. IAM AWS and Customer managed policies will be defined, inline policies will be avoided whenever possible. All user management will be maintained as code and will reside in the DevOps repository. All users will have MFA enabled whenever possible (VPN and AWS Web Console). Root user credentials will be rotated and secured. MFA for root will be enabled. IAM Access Keys for root will be disabled. IAM root access will be monitored via CloudWatch Alerts. Why multi account IAM strategy? Creating a security relationship between accounts makes it even easier for companies to assess the security of AWS-based deployments, centralize security monitoring and management, manage identity and access, and provide audit and compliance monitoring services Figure: AWS Organization Security account structure for managing multiple accounts (just as reference). (Source: Yoriyasu Yano, \"How to Build an End to End Production-Grade Architecture on AWS Part 2\" , Gruntwork.io Blog, accessed November 18th 2020). IAM Groups & Roles definition \u00b6 AWS Org member accounts IAM groups : Account Name AWS Org Member Accounts IAM Groups Admin Auditor DevOps DeployMaster project-root x project-security x x x x AWS Org member accounts IAM roles : Account Name AWS Org Member Accounts IAM Roles Admin Auditor DevOps DeployMaster OrganizationAccountAccessRole project-root x project-security x x x x project-shared x x x x x project-legacy x x x project-apps-devstg x x x x x project-apps-prd x x x x x","title":"Overview"},{"location":"how-it-works/identities/identities/#identity-and-access-management-iam-layer","text":"","title":"Identity and Access Management (IAM) Layer"},{"location":"how-it-works/identities/identities/#summary","text":"Having this official AWS resource as reference we've define a security account structure for managing multiple accounts. User Management Definitions IAM users will strictly be created and centralized in the Security account (member accounts IAM Users could be exceptionally created for very specific tools that still don\u2019t support IAM roles for cross-account auth). All access to resources within the Client organization will be assigned via policy documents attached to IAM roles or groups. All IAM roles and groups will have the least privileges required to properly work. IAM AWS and Customer managed policies will be defined, inline policies will be avoided whenever possible. All user management will be maintained as code and will reside in the DevOps repository. All users will have MFA enabled whenever possible (VPN and AWS Web Console). Root user credentials will be rotated and secured. MFA for root will be enabled. IAM Access Keys for root will be disabled. IAM root access will be monitored via CloudWatch Alerts. Why multi account IAM strategy? Creating a security relationship between accounts makes it even easier for companies to assess the security of AWS-based deployments, centralize security monitoring and management, manage identity and access, and provide audit and compliance monitoring services Figure: AWS Organization Security account structure for managing multiple accounts (just as reference). (Source: Yoriyasu Yano, \"How to Build an End to End Production-Grade Architecture on AWS Part 2\" , Gruntwork.io Blog, accessed November 18th 2020).","title":"Summary"},{"location":"how-it-works/identities/identities/#iam-groups-roles-definition","text":"AWS Org member accounts IAM groups : Account Name AWS Org Member Accounts IAM Groups Admin Auditor DevOps DeployMaster project-root x project-security x x x x AWS Org member accounts IAM roles : Account Name AWS Org Member Accounts IAM Roles Admin Auditor DevOps DeployMaster OrganizationAccountAccessRole project-root x project-security x x x x project-shared x x x x x project-legacy x x x project-apps-devstg x x x x x project-apps-prd x x x x x","title":"IAM Groups &amp; Roles definition"},{"location":"how-it-works/identities/roles/","text":"IAM roles \u00b6 What are AWS IAM Roles? For the Leverage AWS Reference Architecture we heavily depend on AWS IAM roles , which is a standalone IAM entity that: Allows you to attach IAM policies to it, Specify which other IAM entities to trust, and then Those other IAM entities can assume the IAM role to be temporarily get access to the permissions in those IAM policies. The two most common use cases for IAM roles are Service roles: Whereas an IAM user allows a human being to access AWS resources, one of the most common use cases for an IAM role is to allow a service\u2014e.g., one of your applications, a CI server, or an AWS service\u2014to access specific resources in your AWS account. For example, you could create an IAM role that gives access to a specific S3 bucket and allow that role to be assumed by one of your EC2 instances or Lambda functions. The code running on that AWS compute service will then be able to access that S3 bucket (or any other service you granted through this IAM roles) without you having to manually copy AWS credentials (i.e., access keys) onto that instance. Cross account access: Allow to grant an IAM entity in one AWS account access to specific resources in another AWS account. For example, if you have an IAM user in AWS account A, then by default, that IAM user cannot access anything in AWS account B. However, you could create an IAM role in account B that gives access to a specific S3 bucket (or any necessary AWS services) in AWS account B and allow that role to be assumed by an IAM user in account A. That IAM user will then be able to access the contents of the S3 bucket by assuming the IAM role in account B. This ability to assume IAM roles across different AWS accounts is the critical glue that truly makes a multi AWS account structure possible. How IAM roles work? \u00b6 Figure: Example of AWS cross-account AWS access. (Source: Kai Zhao, \"AWS CloudTrail Now Tracks Cross-Account Activity to Its Origin\" , AWS Security Blog, accessed November 17th 2020). Main IAM Roles related entities IAM policies \u00b6 Just as you can attach IAM policies to an IAM user and IAM group, you can attach IAM policies to an IAM role. Trust policy \u00b6 You must define a trust policy for each IAM role, which is a JSON document (very similar to an IAM policy) that specifies who can assume this IAM role. For example, we present below a trust policy that allows this IAM role to be assumed by an IAM user named John in AWS account 111111111111: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::111111111111:user/John\" } } ] } Note that a trust policy alone does NOT automatically give John permissions to assume this IAM role. Cross-account access always requires permissions in both accounts (2 way authorization). So, if John is in AWS account 111111111111 and you want him to have access to an IAM role called DevOps in account B ID 222222222222, then you need to configure permissions in both accounts: 1. In account 222222222222, the DevOps IAM role must have a trust policy that gives sts:AssumeRole permissions to AWS account A ID 111111111111 (as shown above). 2. 2nd, in account A 111111111111, you also need to attach an IAM policy to John\u2019s IAM user that allows him to assume the DevOps IAM role, which might look like this: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : \"arn:aws:iam::222222222222:role/DevOps\" } ] } Assuming an AWS IAM role \u00b6 How does it work? IAM roles do not have a user name, password, or permanent access keys. To use an IAM role, you must assume it by making an AssumeRole API call (v\u00eda SDKs API , CLI or Web Console , which will return temporary access keys you can use in follow-up API calls to authenticate as the IAM role. The temporary access keys will be valid for 1-12 hours (depending on your current validity expiration config), after which you must call AssumeRole again to fetch new temporary keys. Note that to make the AssumeRole API call, you must first authenticate to AWS using some other mechanism. For example, for an IAM user to assume an IAM role, the workflow looks like this: Figure: Assuming an AWS IAM role. (Source: Gruntwork.io, \"How to configure a production-grade AWS account structure using Gruntwork AWS Landing Zone\" , Gruntwork.io Production deployment guides, accessed November 17th 2020). Basic AssumeRole workflow Authenticate using the IAM user\u2019s permanent AWS access keys Make the AssumeRole API call AWS sends back temporary access keys You authenticate using those temporary access keys Now all of your subsequent API calls will be on behalf of the assumed IAM role, with access to whatever permissions are attached to that role IAM roles and AWS services Most AWS services have native support built-in for assuming IAM roles. For example: You can associate an IAM role directly with an EC2 instance (instance profile), and that instance will automatically assume the IAM role every few hours, making the temporary credentials available in EC2 instance metadata. Just about every AWS CLI and SDK tool knows how to read and periodically update temporary credentials from EC2 instance metadata, so in practice, as soon as you attach an IAM role to an EC2 instance, any code running on that EC2 instance can automatically make API calls on behalf of that IAM role, with whatever permissions are attached to that role. This allows you to give code on your EC2 instances IAM permissions without having to manually figure out how to copy credentials (access keys) onto that instance. The same strategy works with many other AWS services: e.g., you use IAM roles as a secure way to give your Lambda functions, ECS services, Step Functions, and many other AWS services permissions to access specific resources in your AWS account. Read more \u00b6 AWS reference links Consider the following AWS official links as reference: AWS Identities | Roles terms and concepts AWS Identities | Common scenarios","title":"IAM Roles"},{"location":"how-it-works/identities/roles/#iam-roles","text":"What are AWS IAM Roles? For the Leverage AWS Reference Architecture we heavily depend on AWS IAM roles , which is a standalone IAM entity that: Allows you to attach IAM policies to it, Specify which other IAM entities to trust, and then Those other IAM entities can assume the IAM role to be temporarily get access to the permissions in those IAM policies. The two most common use cases for IAM roles are Service roles: Whereas an IAM user allows a human being to access AWS resources, one of the most common use cases for an IAM role is to allow a service\u2014e.g., one of your applications, a CI server, or an AWS service\u2014to access specific resources in your AWS account. For example, you could create an IAM role that gives access to a specific S3 bucket and allow that role to be assumed by one of your EC2 instances or Lambda functions. The code running on that AWS compute service will then be able to access that S3 bucket (or any other service you granted through this IAM roles) without you having to manually copy AWS credentials (i.e., access keys) onto that instance. Cross account access: Allow to grant an IAM entity in one AWS account access to specific resources in another AWS account. For example, if you have an IAM user in AWS account A, then by default, that IAM user cannot access anything in AWS account B. However, you could create an IAM role in account B that gives access to a specific S3 bucket (or any necessary AWS services) in AWS account B and allow that role to be assumed by an IAM user in account A. That IAM user will then be able to access the contents of the S3 bucket by assuming the IAM role in account B. This ability to assume IAM roles across different AWS accounts is the critical glue that truly makes a multi AWS account structure possible.","title":"IAM roles"},{"location":"how-it-works/identities/roles/#how-iam-roles-work","text":"Figure: Example of AWS cross-account AWS access. (Source: Kai Zhao, \"AWS CloudTrail Now Tracks Cross-Account Activity to Its Origin\" , AWS Security Blog, accessed November 17th 2020). Main IAM Roles related entities","title":"How IAM roles work?"},{"location":"how-it-works/identities/roles/#iam-policies","text":"Just as you can attach IAM policies to an IAM user and IAM group, you can attach IAM policies to an IAM role.","title":"IAM policies"},{"location":"how-it-works/identities/roles/#trust-policy","text":"You must define a trust policy for each IAM role, which is a JSON document (very similar to an IAM policy) that specifies who can assume this IAM role. For example, we present below a trust policy that allows this IAM role to be assumed by an IAM user named John in AWS account 111111111111: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::111111111111:user/John\" } } ] } Note that a trust policy alone does NOT automatically give John permissions to assume this IAM role. Cross-account access always requires permissions in both accounts (2 way authorization). So, if John is in AWS account 111111111111 and you want him to have access to an IAM role called DevOps in account B ID 222222222222, then you need to configure permissions in both accounts: 1. In account 222222222222, the DevOps IAM role must have a trust policy that gives sts:AssumeRole permissions to AWS account A ID 111111111111 (as shown above). 2. 2nd, in account A 111111111111, you also need to attach an IAM policy to John\u2019s IAM user that allows him to assume the DevOps IAM role, which might look like this: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : \"arn:aws:iam::222222222222:role/DevOps\" } ] }","title":"Trust policy"},{"location":"how-it-works/identities/roles/#assuming-an-aws-iam-role","text":"How does it work? IAM roles do not have a user name, password, or permanent access keys. To use an IAM role, you must assume it by making an AssumeRole API call (v\u00eda SDKs API , CLI or Web Console , which will return temporary access keys you can use in follow-up API calls to authenticate as the IAM role. The temporary access keys will be valid for 1-12 hours (depending on your current validity expiration config), after which you must call AssumeRole again to fetch new temporary keys. Note that to make the AssumeRole API call, you must first authenticate to AWS using some other mechanism. For example, for an IAM user to assume an IAM role, the workflow looks like this: Figure: Assuming an AWS IAM role. (Source: Gruntwork.io, \"How to configure a production-grade AWS account structure using Gruntwork AWS Landing Zone\" , Gruntwork.io Production deployment guides, accessed November 17th 2020). Basic AssumeRole workflow Authenticate using the IAM user\u2019s permanent AWS access keys Make the AssumeRole API call AWS sends back temporary access keys You authenticate using those temporary access keys Now all of your subsequent API calls will be on behalf of the assumed IAM role, with access to whatever permissions are attached to that role IAM roles and AWS services Most AWS services have native support built-in for assuming IAM roles. For example: You can associate an IAM role directly with an EC2 instance (instance profile), and that instance will automatically assume the IAM role every few hours, making the temporary credentials available in EC2 instance metadata. Just about every AWS CLI and SDK tool knows how to read and periodically update temporary credentials from EC2 instance metadata, so in practice, as soon as you attach an IAM role to an EC2 instance, any code running on that EC2 instance can automatically make API calls on behalf of that IAM role, with whatever permissions are attached to that role. This allows you to give code on your EC2 instances IAM permissions without having to manually figure out how to copy credentials (access keys) onto that instance. The same strategy works with many other AWS services: e.g., you use IAM roles as a secure way to give your Lambda functions, ECS services, Step Functions, and many other AWS services permissions to access specific resources in your AWS account.","title":"Assuming an AWS IAM role"},{"location":"how-it-works/identities/roles/#read-more","text":"AWS reference links Consider the following AWS official links as reference: AWS Identities | Roles terms and concepts AWS Identities | Common scenarios","title":"Read more"},{"location":"how-it-works/monitoring/apm/","text":"Application Performance Monitoring (APM) and Business Performance \u00b6 Custom Prometheus BlackBox Exporter + Grafana & Elastic Application performance monitoring (APM) delivers real-time and trending data about your web application's performance and the level of satisfaction that your end users experience. With end to end transaction tracing and a variety of color-coded charts and reports, APM visualizes your data, down to the deepest code levels. Your DevOps teams don't need to guess whether a performance blocker comes from the app itself, CPU availability, database loads, or something else entirely unexpected. With APM, you can quickly identify potential problems before they affect your end users. APM's user interface provides both current and historical information about memory usage, CPU utilization, database query performance, web browser rendering performance, app availability and error analysis, external services, and other useful metrics. SLIs / KPIs \u00b6 Service Level Indicators (SLIs) latency throughput availability error rate KPI for business performance General DOM readiness Page render Apdex Mobile crash rate Web Session count Session duration Page views Error % Mobile App launches User counts Load time Crash rates Crash locations Error rates API errors KPI for app and infrastructure teams App/Infra Availability Throughput App/Api/Db Response time Memory footprint CPU workload DevOps Builds Commits Deploys Errors Support incidents MTTR Read More \u00b6 NewRelic | Optimize customer experience (KPIs)","title":"APM"},{"location":"how-it-works/monitoring/apm/#application-performance-monitoring-apm-and-business-performance","text":"Custom Prometheus BlackBox Exporter + Grafana & Elastic Application performance monitoring (APM) delivers real-time and trending data about your web application's performance and the level of satisfaction that your end users experience. With end to end transaction tracing and a variety of color-coded charts and reports, APM visualizes your data, down to the deepest code levels. Your DevOps teams don't need to guess whether a performance blocker comes from the app itself, CPU availability, database loads, or something else entirely unexpected. With APM, you can quickly identify potential problems before they affect your end users. APM's user interface provides both current and historical information about memory usage, CPU utilization, database query performance, web browser rendering performance, app availability and error analysis, external services, and other useful metrics.","title":"Application Performance Monitoring (APM) and Business Performance"},{"location":"how-it-works/monitoring/apm/#slis-kpis","text":"Service Level Indicators (SLIs) latency throughput availability error rate KPI for business performance General DOM readiness Page render Apdex Mobile crash rate Web Session count Session duration Page views Error % Mobile App launches User counts Load time Crash rates Crash locations Error rates API errors KPI for app and infrastructure teams App/Infra Availability Throughput App/Api/Db Response time Memory footprint CPU workload DevOps Builds Commits Deploys Errors Support incidents MTTR","title":"SLIs / KPIs"},{"location":"how-it-works/monitoring/apm/#read-more","text":"NewRelic | Optimize customer experience (KPIs)","title":"Read More"},{"location":"how-it-works/monitoring/logs/","text":"Logs \u00b6 Overview \u00b6 Centralized Logs Solution For this purpose we propose the usage of Elasticsearch + Kibana for database and visualization respectively. By deploying the Fluentd daemonset on the Kubernetes clusters we can send all logs from running pods to Elasticsearch, and with \u2018beat\u2019 we can send specific logs for resources outside of Kubernetes. There will be many components across the environment generating different types of logs: ALB access logs, s3 access logs, cloudfront access logs, application request logs, application error logs. Access logs on AWS based resources can be stored in a centralized bucket for that purpose, on the security account and given the need these can be streamed to Elasticsearch as well if needed. Figure: Monitoring metrics and log architecture diagram (just as reference). (Source: Binbash Leverage, \"AWS Well Architected Reliability Report example\" , Binbash Leverage Doc, accessed November 18th 2020). Alerting based on Logs Certain features that were only available under licence were recently made available by Elastic, and included in the open source project of Elasticsearch. Elastalert allow us to generate alerts based on certain log entries or even after counting a certain amount of a type of entry, providing great flexibility. -- Alternatives Comparison Table \u00b6 Leverage Confluence Documentation You'll find here a detailed comparison table between EC2 Self-hosted and AWS ElasticSearch Elastic-Kibana Stack.","title":"Logs"},{"location":"how-it-works/monitoring/logs/#logs","text":"","title":"Logs"},{"location":"how-it-works/monitoring/logs/#overview","text":"Centralized Logs Solution For this purpose we propose the usage of Elasticsearch + Kibana for database and visualization respectively. By deploying the Fluentd daemonset on the Kubernetes clusters we can send all logs from running pods to Elasticsearch, and with \u2018beat\u2019 we can send specific logs for resources outside of Kubernetes. There will be many components across the environment generating different types of logs: ALB access logs, s3 access logs, cloudfront access logs, application request logs, application error logs. Access logs on AWS based resources can be stored in a centralized bucket for that purpose, on the security account and given the need these can be streamed to Elasticsearch as well if needed. Figure: Monitoring metrics and log architecture diagram (just as reference). (Source: Binbash Leverage, \"AWS Well Architected Reliability Report example\" , Binbash Leverage Doc, accessed November 18th 2020). Alerting based on Logs Certain features that were only available under licence were recently made available by Elastic, and included in the open source project of Elasticsearch. Elastalert allow us to generate alerts based on certain log entries or even after counting a certain amount of a type of entry, providing great flexibility. --","title":"Overview"},{"location":"how-it-works/monitoring/logs/#alternatives-comparison-table","text":"Leverage Confluence Documentation You'll find here a detailed comparison table between EC2 Self-hosted and AWS ElasticSearch Elastic-Kibana Stack.","title":"Alternatives Comparison Table"},{"location":"how-it-works/monitoring/metrics/","text":"Metrics \u00b6 There are metrics that are going to be of interest both in the infrastructure itself (CPU, Memory, disk) and also on application level (amount of non 200 responses, latency, % of errors) and we will have two key sources for this: Prometheus and AWS CloudWatch metrics . Metric collectors CloudWatch metrics: Is where amazon stores a great number of default metrics for each of its services. Useful data here can be interpreted and alerts can be generated with Cloudwatch alerts and can also be used as a source for Grafana. Although this is a very good offering, we have found it to be incomplete and highly bound to AWS services but not integrated enough with the rest of the ecosystem. Prometheus: This is an open source tool (by Soundcloud) that is essentially a time-series database. It stores metrics, and it has the advantage of being highly integrated with all Kubernetes things. In fact, Kubernetes is already publishing various metrics in Prometheus format \u201cout of the box\u201d. It\u2019s alerting capabilities are also remarkable, and it can all be kept as code in a repository. It has a big community behind it, and it\u2019s not far fetched at this point to include a library in your own application that provides you with the ability to create an endpoint that publishes certain metrics about your own application, that we can graph or alert based on them. Figure: Monitoring metrics and log architecture diagram (just as reference). (Source: Binbash Leverage, \"AWS Well Architected Reliability Report example\" , Binbash Leverage Doc, accessed November 18th 2020). Graphing metrics Grafana is the standard open source visualization tool which can be used on top of a variety of different data stores. It can use prometheus as a source, and there are many open source dashboards and plugins available that provide great visualization of how things are running, and we can also build our own if necessary. If something is left out of prometheus and already available in Cloudwatch metrics we can easily integrate it as a source for Grafana as well, and build dashboards that integrate these metrics and even do some intelligence on them coming from multiple origins. Figure: Grafana K8s cluster metrics monitoring dashboard reference screenshot. (Source: DevOpsProdigy, \"Grafana DevOpsProdigy KubeGraf Plugin\" , Grafana plugins, accessed November 18th 2020). Figure: Grafana K8s cluster metrics monitoring dashboard reference screenshot. (Source: DevOpsProdigy, \"Grafana DevOpsProdigy KubeGraf Plugin\" , Grafana plugins, accessed November 18th 2020). Alerting based on metrics Although Grafana already has alerting capabilities built in, we rather (most of the times) have Prometheus alerting engine configured, because we can have really customize and specify alerts. We can have them as code in their extremely readable syntax. Example: Figure: Prometheus Alert Manager `CriticalRamUsage` alert screenshot (just as reference). (Source: Binbash Leverage).","title":"Metrics"},{"location":"how-it-works/monitoring/metrics/#metrics","text":"There are metrics that are going to be of interest both in the infrastructure itself (CPU, Memory, disk) and also on application level (amount of non 200 responses, latency, % of errors) and we will have two key sources for this: Prometheus and AWS CloudWatch metrics . Metric collectors CloudWatch metrics: Is where amazon stores a great number of default metrics for each of its services. Useful data here can be interpreted and alerts can be generated with Cloudwatch alerts and can also be used as a source for Grafana. Although this is a very good offering, we have found it to be incomplete and highly bound to AWS services but not integrated enough with the rest of the ecosystem. Prometheus: This is an open source tool (by Soundcloud) that is essentially a time-series database. It stores metrics, and it has the advantage of being highly integrated with all Kubernetes things. In fact, Kubernetes is already publishing various metrics in Prometheus format \u201cout of the box\u201d. It\u2019s alerting capabilities are also remarkable, and it can all be kept as code in a repository. It has a big community behind it, and it\u2019s not far fetched at this point to include a library in your own application that provides you with the ability to create an endpoint that publishes certain metrics about your own application, that we can graph or alert based on them. Figure: Monitoring metrics and log architecture diagram (just as reference). (Source: Binbash Leverage, \"AWS Well Architected Reliability Report example\" , Binbash Leverage Doc, accessed November 18th 2020). Graphing metrics Grafana is the standard open source visualization tool which can be used on top of a variety of different data stores. It can use prometheus as a source, and there are many open source dashboards and plugins available that provide great visualization of how things are running, and we can also build our own if necessary. If something is left out of prometheus and already available in Cloudwatch metrics we can easily integrate it as a source for Grafana as well, and build dashboards that integrate these metrics and even do some intelligence on them coming from multiple origins. Figure: Grafana K8s cluster metrics monitoring dashboard reference screenshot. (Source: DevOpsProdigy, \"Grafana DevOpsProdigy KubeGraf Plugin\" , Grafana plugins, accessed November 18th 2020). Figure: Grafana K8s cluster metrics monitoring dashboard reference screenshot. (Source: DevOpsProdigy, \"Grafana DevOpsProdigy KubeGraf Plugin\" , Grafana plugins, accessed November 18th 2020). Alerting based on metrics Although Grafana already has alerting capabilities built in, we rather (most of the times) have Prometheus alerting engine configured, because we can have really customize and specify alerts. We can have them as code in their extremely readable syntax. Example: Figure: Prometheus Alert Manager `CriticalRamUsage` alert screenshot (just as reference). (Source: Binbash Leverage).","title":"Metrics"},{"location":"how-it-works/monitoring/monitoring/","text":"SRE & Monitoring: Metrics, Logs & Tracing \u00b6 Overview \u00b6 There are two key approaches that we will cover with the proposed tools, Logs based monitoring and Metrics based monitoring. Monitoring tools Metrics: Prometheus - node-exporter - blackbox-exporter - alert-manager Metrics Dashboard: Grafana Data Sources Prometheus CloudWatch Plugins piechart-panel devopsprodigy-kubegraf-app Centralized Logs: Elasticsearch-Fluent-Kibana (EFK) Query Logs Dashboards Alerts based on logs Distributed Tracing: Jaeger + Opensensus","title":"Monitoring"},{"location":"how-it-works/monitoring/monitoring/#sre-monitoring-metrics-logs-tracing","text":"","title":"SRE &amp; Monitoring: Metrics, Logs &amp; Tracing"},{"location":"how-it-works/monitoring/monitoring/#overview","text":"There are two key approaches that we will cover with the proposed tools, Logs based monitoring and Metrics based monitoring. Monitoring tools Metrics: Prometheus - node-exporter - blackbox-exporter - alert-manager Metrics Dashboard: Grafana Data Sources Prometheus CloudWatch Plugins piechart-panel devopsprodigy-kubegraf-app Centralized Logs: Elasticsearch-Fluent-Kibana (EFK) Query Logs Dashboards Alerts based on logs Distributed Tracing: Jaeger + Opensensus","title":"Overview"},{"location":"how-it-works/monitoring/notification_escalation/","text":"Notification & Escalation Procedure \u00b6 Overview \u00b6 Urgency Service Notification Setting Use When Response High 24/7 High-priority PagerDuty Alert 24/7/365 Issue is in Production Or affects the applications/services and in turn affects the normal operation of the clinics Or prevents clinic patients to interact with the applications/services Requires immediate human action Escalate as needed The engineer should be woken up High during support hours High-priority Slack Notifications during support hours Issue impacts development team productivity Issue impacts the normal business operation Requires immediate human action ONLY during business hours Low Low Priority Slack Notification Any issue, on any environment, that occurs during working hours Requires human action at some point Do not escalate An engineer should not be woken up Service Notification Settings \u00b6 Service Notification Setting Description High-priority PagerDuty Alert 24/7/365 Notify on-call engineers --- At first, notify via SMS/Push --- Notify via Phone Call if after 10 minutes the previous has not acknowledged Notify person X (this is a person who needs to be aware of any of these issues always) Notify to Slack => engineering-urgent-alerts channel High-priority Slack Notifications during support hours Notify to Slack => engineering-alerts channel Low Priority Slack Notification Notify to Slack => engineering-alerts channel Alert Types \u00b6 UpTimeRobot (black box) https://uptimerobot.com/ Sites or APIs are down Prometheus Alert Manager (black box, metrics-based) http://prometheus.aws.domain.com/ Clusters issues (masters/nodes high resources usage) Instance issues (Pritunl VPN, Jenkins, Spinnaker, Grafana, Kibana, etc) Alerts from Prometheus Blackbox Exporter Kibana ElastAlert (black box, logs-based) Intended for applications/services logs Applications/services issues (frontends, backend services) Cluster components issues (nginx-ingress, cert-manager, linkerd, etc) PagerDuty https://domain.pagerduty.com/ Incident management Implementation Reference Example \u00b6 Slack All alerts are sent to #engineering-urgent-alerts channel. Members that are online can have visibility from there. AlertManager takes care of sending such alerts according to the rules defined here: TODO Note: there is a channel named engineering-alerts but is used for Github notifications. It didn\u2019t make sense to mix real alerts with that, that is why a new engineering-urgent-alerts channel was created. As a recommendation, Github notifications should be sent to a channel named like #engineering-notifications and leave engineering-alerts for real alerts. PagerDuty AlertManager only sends to PagerDuty alerts that are labeled as severity: critical . PagerDuty is configured to turn these into incidents according to the settings defined here for the Prometheus Critical Alerts service . The aforementioned service uses HiPriorityAllYearRound escalation policy to define who gets notified and how. Note: currently only the TechOwnership role gets notified as we don\u2019t have agreements or rules about on-call support but this can be easily changed in the future to accommodate business decisions. UpTimeRobot We are doing basic http monitoring on the following sites: * www.domain_1.com * www.domain_2.com * www.domain_3.com Note: a personal account has been set up for this. As a recommendation, an new account should be created using an email account that belongs to your project.","title":"Notifications"},{"location":"how-it-works/monitoring/notification_escalation/#notification-escalation-procedure","text":"","title":"Notification &amp; Escalation Procedure"},{"location":"how-it-works/monitoring/notification_escalation/#overview","text":"Urgency Service Notification Setting Use When Response High 24/7 High-priority PagerDuty Alert 24/7/365 Issue is in Production Or affects the applications/services and in turn affects the normal operation of the clinics Or prevents clinic patients to interact with the applications/services Requires immediate human action Escalate as needed The engineer should be woken up High during support hours High-priority Slack Notifications during support hours Issue impacts development team productivity Issue impacts the normal business operation Requires immediate human action ONLY during business hours Low Low Priority Slack Notification Any issue, on any environment, that occurs during working hours Requires human action at some point Do not escalate An engineer should not be woken up","title":"Overview"},{"location":"how-it-works/monitoring/notification_escalation/#service-notification-settings","text":"Service Notification Setting Description High-priority PagerDuty Alert 24/7/365 Notify on-call engineers --- At first, notify via SMS/Push --- Notify via Phone Call if after 10 minutes the previous has not acknowledged Notify person X (this is a person who needs to be aware of any of these issues always) Notify to Slack => engineering-urgent-alerts channel High-priority Slack Notifications during support hours Notify to Slack => engineering-alerts channel Low Priority Slack Notification Notify to Slack => engineering-alerts channel","title":"Service Notification Settings"},{"location":"how-it-works/monitoring/notification_escalation/#alert-types","text":"UpTimeRobot (black box) https://uptimerobot.com/ Sites or APIs are down Prometheus Alert Manager (black box, metrics-based) http://prometheus.aws.domain.com/ Clusters issues (masters/nodes high resources usage) Instance issues (Pritunl VPN, Jenkins, Spinnaker, Grafana, Kibana, etc) Alerts from Prometheus Blackbox Exporter Kibana ElastAlert (black box, logs-based) Intended for applications/services logs Applications/services issues (frontends, backend services) Cluster components issues (nginx-ingress, cert-manager, linkerd, etc) PagerDuty https://domain.pagerduty.com/ Incident management","title":"Alert Types"},{"location":"how-it-works/monitoring/notification_escalation/#implementation-reference-example","text":"Slack All alerts are sent to #engineering-urgent-alerts channel. Members that are online can have visibility from there. AlertManager takes care of sending such alerts according to the rules defined here: TODO Note: there is a channel named engineering-alerts but is used for Github notifications. It didn\u2019t make sense to mix real alerts with that, that is why a new engineering-urgent-alerts channel was created. As a recommendation, Github notifications should be sent to a channel named like #engineering-notifications and leave engineering-alerts for real alerts. PagerDuty AlertManager only sends to PagerDuty alerts that are labeled as severity: critical . PagerDuty is configured to turn these into incidents according to the settings defined here for the Prometheus Critical Alerts service . The aforementioned service uses HiPriorityAllYearRound escalation policy to define who gets notified and how. Note: currently only the TechOwnership role gets notified as we don\u2019t have agreements or rules about on-call support but this can be easily changed in the future to accommodate business decisions. UpTimeRobot We are doing basic http monitoring on the following sites: * www.domain_1.com * www.domain_2.com * www.domain_3.com Note: a personal account has been set up for this. As a recommendation, an new account should be created using an email account that belongs to your project.","title":"Implementation Reference Example"},{"location":"how-it-works/monitoring/tracing/","text":"Distributed Tracing \u00b6 Solution Design Distributed tracing, also called distributed request tracing, is a method used to profile and monitor applications, especially those built using a microservices architecture. Distributed tracing helps pinpoint where failures occur and what causes poor performance. Figure: Figure: Distributed tracing architecture diagram (just as reference). (Source: Binbash Leverage, \"AWS Well Architected Reliability Report example\" , Binbash Leverage Doc, accessed November 18th 2020). Read more \u00b6 Related resources Jaeger Opensensus","title":"Tracing"},{"location":"how-it-works/monitoring/tracing/#distributed-tracing","text":"Solution Design Distributed tracing, also called distributed request tracing, is a method used to profile and monitor applications, especially those built using a microservices architecture. Distributed tracing helps pinpoint where failures occur and what causes poor performance. Figure: Figure: Distributed tracing architecture diagram (just as reference). (Source: Binbash Leverage, \"AWS Well Architected Reliability Report example\" , Binbash Leverage Doc, accessed November 18th 2020).","title":"Distributed Tracing"},{"location":"how-it-works/monitoring/tracing/#read-more","text":"Related resources Jaeger Opensensus","title":"Read more"},{"location":"how-it-works/network/dns/","text":"Route53 DNS hosted zones \u00b6 Route53 Considerations Route53 private hosted zone will have associations with VPCs on different AWS organization accounts Route53 should ideally be hosted in the Shared account, although sometimes Route53 is already deployed in a Legacy account where it can be imported and fully supported as code. Route53 zero downtime migration (active-active hosted zones) is completely possible and achieviable with Leverage terraform code Figure: AWS Organization shared account Route53 DNS diagram. (Source: Cristian Southall, \"Using CloudFormation Custom Resources to Configure Route53 Aliases\" , Abstractable.io Blog post, accessed November 18th 2020).","title":"DNS"},{"location":"how-it-works/network/dns/#route53-dns-hosted-zones","text":"Route53 Considerations Route53 private hosted zone will have associations with VPCs on different AWS organization accounts Route53 should ideally be hosted in the Shared account, although sometimes Route53 is already deployed in a Legacy account where it can be imported and fully supported as code. Route53 zero downtime migration (active-active hosted zones) is completely possible and achieviable with Leverage terraform code Figure: AWS Organization shared account Route53 DNS diagram. (Source: Cristian Southall, \"Using CloudFormation Custom Resources to Configure Route53 Aliases\" , Abstractable.io Blog post, accessed November 18th 2020).","title":"Route53 DNS hosted zones"},{"location":"how-it-works/network/vpc-addressing/","text":"Network Layer \u00b6 In this section we detail all the network design related specifications VPCs CIDR blocks VPC Gateways: Internet, NAT, VPN. VPC Peerings VPC DNS Private Hosted Zones Associations. Network ACLS (NACLs) VPCs IP Addressing Plan (CIDR blocks sizing) \u00b6 Introduction VPCs can vary in size from 16 addresses (/28 netmask) to 65,536 addresses (/16 netmask). In order to size a VPC correctly, it is important to understand the number, types, and sizes of workloads expected to run in it, as well as workload elasticity and load balancing requirements. Keep in mind that there is no charge for using Amazon VPC (aside from EC2 charges), therefore cost should not be a factor when determining the appropriate size for your VPC, so make sure you size your VPC for growth. Moving workloads or AWS resources between networks is not a trivial task, so be generous in your IP address estimates to give yourself plenty of room to grow, deploy new workloads, or change your VPC design configuration from one to another. The majority of AWS customers use VPCs with a /16 netmask and subnets with /24 netmasks. The primary reason AWS customers select smaller VPC and subnet sizes is to avoid overlapping network addresses with existing networks. So having AWS single VPC Design we've choosen a Medium/Small VPC/Subnet addressing plan which would probably fit a broad range variety of use cases Networking - IP Addressing \u00b6 Starting CIDR Segment (AWS Org) AWS Org IP Addressing calculation is presented below based on segment 172.16.0.0.0/12 We started from 172.16.0.0.0/12 and subnetted to /20 Resulting in Total Subnets: 256 2 x AWS Account with Hosts/SubNet: 4094 1ry VPC + 2ry VPC 1ry VPC DR + 2ry VPC DR Individual CIDR Segments (VPCs) Then each of these are /20 to /24 Considering the whole Starting CIDR Segment (AWS Org) before declared, we'll start at 172.18.0.0/20 shared 1ry VPC CIDR: 172.18.0.0/24 2ry VPC CIDR: 172.18.16.0/24 1ry VPC DR CIDR: 172.18.32.0/24 2ry VPC DR CIDR: 172.18.48.0/24 apps-devstg 1ry VPC CIDR: 172.18.64.0/24 2ry VPC CIDR: 172.18.80.0/24 1ry VPC DR CIDR: 172.18.96.0/24 2ry VPC DR CIDR: 172.18.112.0/24 apps-prd 1ry VPC CIDR: 172.18.128.0/24 2ry VPC CIDR: 172.18.144.0/24 1ry VPC DR CIDR: 172.18.160.0/24 2ry VPC DR CIDR: 172.18.176.0/24 Resulting in Subnets: 16 x VPC VPC Subnets with Hosts/Net: 256. Eg: apps-devstg account \u2192 us-east-1 w/ 3 AZs \u2192 3 x Private Subnets /az + 3 x Public Subnets /az 1ry VPC CIDR: 172.18.64.0/24 Subnets: Private 172.18.64.0/24, 172.18.66.0/24 and 172.18.68.0/24 Public 172.18.65.0/24, 172.18.67.0/24 and 172.18.69.0/24 Planned VPCs \u00b6 Having defined the initial VPC that will be created in the different accounts that were defined, we are going to create subnets in each of these VPCs defining Private and Public subnets split among different availability zones: Subnet address Netmask Range of addresses Hosts Assignment 172.18.0.0/20 255.255.240.0 172.18.0.0 - 172.18.15.255 4094 1ry VPC: shared 172.18.16.0/20 255.255.240.0 172.18.16.0 - 172.18.31.255 4094 2ry VPC: shared 172.18.32.0/20 255.255.240.0 172.18.32.0 - 172.18.47.255 4094 1ry VPC DR: shared 172.18.48.0/20 255.255.240.0 172.18.48.0 - 172.18.63.255 4094 2ry VPC DR: shared 172.18.64.0/20 255.255.240.0 172.18.64.0 - 172.18.79.255 4094 1ry VPC: apps-devstg 172.18.80.0/20 255.255.240.0 172.18.80.0 - 172.18.95.255 4094 2ry VPC: apps-devstg 172.18.96.0/20 255.255.240.0 172.18.96.0 - 172.18.111.255 4094 1ry VPC DR: apps-devstg 172.18.112.0/20 255.255.240.0 172.18.112.0 - 172.18.127.255 4094 2ry VPC DR: apps-devstg 172.18.128.0/20 255.255.240.0 172.18.128.0 - 172.18.143.255 4094 1ry VPC: apps-prd 172.18.144.0/20 255.255.240.0 172.18.144.0 - 172.18.159.255 4094 2ry VPC: apps-prd 172.18.160.0/20 255.255.240.0 172.18.160.0 - 172.18.175.255 4094 1ry VPC DR: apps-prd 172.18.176.0/20 255.255.240.0 172.18.176.0 - 172.18.191.255 4094 2ry VPC DR: apps-prd Considerations \u00b6 Design considerations AWS EKS: Docker runs in the 172.17.0.0/16 CIDR range in Amazon EKS clusters. We recommend that your cluster's VPC subnets do not overlap this range. Otherwise, you will receive the following error: Error: : error upgrading connection: error dialing backend: dial tcp 172.17.nn.nn:10250: getsockopt: no route to host Read more: AWS EKS network requirements Reserved IP Addresses The first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance. For example, in a subnet with CIDR block 10.0.0.0/24, the following five IP addresses are reserved. For more AWS VPC Subnets IP addressing","title":"VPC Adressing"},{"location":"how-it-works/network/vpc-addressing/#network-layer","text":"In this section we detail all the network design related specifications VPCs CIDR blocks VPC Gateways: Internet, NAT, VPN. VPC Peerings VPC DNS Private Hosted Zones Associations. Network ACLS (NACLs)","title":"Network Layer"},{"location":"how-it-works/network/vpc-addressing/#vpcs-ip-addressing-plan-cidr-blocks-sizing","text":"Introduction VPCs can vary in size from 16 addresses (/28 netmask) to 65,536 addresses (/16 netmask). In order to size a VPC correctly, it is important to understand the number, types, and sizes of workloads expected to run in it, as well as workload elasticity and load balancing requirements. Keep in mind that there is no charge for using Amazon VPC (aside from EC2 charges), therefore cost should not be a factor when determining the appropriate size for your VPC, so make sure you size your VPC for growth. Moving workloads or AWS resources between networks is not a trivial task, so be generous in your IP address estimates to give yourself plenty of room to grow, deploy new workloads, or change your VPC design configuration from one to another. The majority of AWS customers use VPCs with a /16 netmask and subnets with /24 netmasks. The primary reason AWS customers select smaller VPC and subnet sizes is to avoid overlapping network addresses with existing networks. So having AWS single VPC Design we've choosen a Medium/Small VPC/Subnet addressing plan which would probably fit a broad range variety of use cases","title":"VPCs IP Addressing Plan (CIDR blocks sizing)"},{"location":"how-it-works/network/vpc-addressing/#networking-ip-addressing","text":"Starting CIDR Segment (AWS Org) AWS Org IP Addressing calculation is presented below based on segment 172.16.0.0.0/12 We started from 172.16.0.0.0/12 and subnetted to /20 Resulting in Total Subnets: 256 2 x AWS Account with Hosts/SubNet: 4094 1ry VPC + 2ry VPC 1ry VPC DR + 2ry VPC DR Individual CIDR Segments (VPCs) Then each of these are /20 to /24 Considering the whole Starting CIDR Segment (AWS Org) before declared, we'll start at 172.18.0.0/20 shared 1ry VPC CIDR: 172.18.0.0/24 2ry VPC CIDR: 172.18.16.0/24 1ry VPC DR CIDR: 172.18.32.0/24 2ry VPC DR CIDR: 172.18.48.0/24 apps-devstg 1ry VPC CIDR: 172.18.64.0/24 2ry VPC CIDR: 172.18.80.0/24 1ry VPC DR CIDR: 172.18.96.0/24 2ry VPC DR CIDR: 172.18.112.0/24 apps-prd 1ry VPC CIDR: 172.18.128.0/24 2ry VPC CIDR: 172.18.144.0/24 1ry VPC DR CIDR: 172.18.160.0/24 2ry VPC DR CIDR: 172.18.176.0/24 Resulting in Subnets: 16 x VPC VPC Subnets with Hosts/Net: 256. Eg: apps-devstg account \u2192 us-east-1 w/ 3 AZs \u2192 3 x Private Subnets /az + 3 x Public Subnets /az 1ry VPC CIDR: 172.18.64.0/24 Subnets: Private 172.18.64.0/24, 172.18.66.0/24 and 172.18.68.0/24 Public 172.18.65.0/24, 172.18.67.0/24 and 172.18.69.0/24","title":"Networking - IP Addressing"},{"location":"how-it-works/network/vpc-addressing/#planned-vpcs","text":"Having defined the initial VPC that will be created in the different accounts that were defined, we are going to create subnets in each of these VPCs defining Private and Public subnets split among different availability zones: Subnet address Netmask Range of addresses Hosts Assignment 172.18.0.0/20 255.255.240.0 172.18.0.0 - 172.18.15.255 4094 1ry VPC: shared 172.18.16.0/20 255.255.240.0 172.18.16.0 - 172.18.31.255 4094 2ry VPC: shared 172.18.32.0/20 255.255.240.0 172.18.32.0 - 172.18.47.255 4094 1ry VPC DR: shared 172.18.48.0/20 255.255.240.0 172.18.48.0 - 172.18.63.255 4094 2ry VPC DR: shared 172.18.64.0/20 255.255.240.0 172.18.64.0 - 172.18.79.255 4094 1ry VPC: apps-devstg 172.18.80.0/20 255.255.240.0 172.18.80.0 - 172.18.95.255 4094 2ry VPC: apps-devstg 172.18.96.0/20 255.255.240.0 172.18.96.0 - 172.18.111.255 4094 1ry VPC DR: apps-devstg 172.18.112.0/20 255.255.240.0 172.18.112.0 - 172.18.127.255 4094 2ry VPC DR: apps-devstg 172.18.128.0/20 255.255.240.0 172.18.128.0 - 172.18.143.255 4094 1ry VPC: apps-prd 172.18.144.0/20 255.255.240.0 172.18.144.0 - 172.18.159.255 4094 2ry VPC: apps-prd 172.18.160.0/20 255.255.240.0 172.18.160.0 - 172.18.175.255 4094 1ry VPC DR: apps-prd 172.18.176.0/20 255.255.240.0 172.18.176.0 - 172.18.191.255 4094 2ry VPC DR: apps-prd","title":"Planned VPCs"},{"location":"how-it-works/network/vpc-addressing/#considerations","text":"Design considerations AWS EKS: Docker runs in the 172.17.0.0/16 CIDR range in Amazon EKS clusters. We recommend that your cluster's VPC subnets do not overlap this range. Otherwise, you will receive the following error: Error: : error upgrading connection: error dialing backend: dial tcp 172.17.nn.nn:10250: getsockopt: no route to host Read more: AWS EKS network requirements Reserved IP Addresses The first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance. For example, in a subnet with CIDR block 10.0.0.0/24, the following five IP addresses are reserved. For more AWS VPC Subnets IP addressing","title":"Considerations"},{"location":"how-it-works/network/vpc-peering/","text":"Diagram: Network Service (cross-account VPC peering ) \u00b6 Figure: AWS multi account Organization VPC peering diagram. (Source: AWS, \"Amazon Virtual Private Cloud VPC Peering\" , AWS Documentation Amazon VPC User Guide, accessed November 18th 2020). Figure: AWS multi account Organization peering detailed diagram. (Source: AWS, \"Amazon Virtual Private Cloud VPC Peering\" , AWS Documentation Amazon VPC User Guide, accessed November 18th 2020).","title":"VPC Peering"},{"location":"how-it-works/network/vpc-peering/#diagram-network-service-cross-account-vpc-peering","text":"Figure: AWS multi account Organization VPC peering diagram. (Source: AWS, \"Amazon Virtual Private Cloud VPC Peering\" , AWS Documentation Amazon VPC User Guide, accessed November 18th 2020). Figure: AWS multi account Organization peering detailed diagram. (Source: AWS, \"Amazon Virtual Private Cloud VPC Peering\" , AWS Documentation Amazon VPC User Guide, accessed November 18th 2020).","title":"Diagram: Network Service (cross-account VPC peering)"},{"location":"how-it-works/network/vpc-topology/","text":"Network Layer \u00b6 Network Topology \u00b6 VPC with public and private subnets (NAT) The configuration for this scenario includes a virtual private cloud (VPC) with public subnets and a private subnets (it's number will change depending on our specific needs). We recommend this scenario if you want to run a public-facing web application, while maintaining back-end servers that aren't publicly accessible. A common example is a multi-tier website, with a Load Balancer (ALB | NLB) in a public subnet, or other public facing routing service like AWS CloudFront or Api Gateway, and our web servers (Lambda, EKS, ECS, EC2) and database (RDS, DynamoDB, etc) servers in private subnets. You can set up security (SGs, ACLs, WAF) and routing so that the web servers can communicate internally (even between VPC accounts or VPN Endpoints) with all necessary services and components such as databases, cache, queues, among others. The services running in the public subnet, like an ALB or NLB can send outbound traffic directly to the Internet, whereas the instances in the private subnet can't. Instead, the instances in the private subnet can access the Internet by using a network address translation (NAT) gateway that resides in the public subnet. The database servers can connect to the Internet for software updates using the NAT gateway (if using RDS this is transparently provided by AWS), but the Internet cannot establish connections to the database servers. So, whenever possible all our AWS resources like EC2, EKS, RDS, Lambda, SQS will be deployed in VPC private subnets and we'll use a NAT device (Nat Gateway) to enable instances in a private subnet to connect to the internet (for example, for software updates) or other AWS services, but prevent the internet from initiating connections with the instances. A NAT device forwards traffic from the instances in the private subnet to the internet (via the VPC Internet Gateway) or other AWS services, and then sends the response back to the instances. When traffic goes to the internet, the source IPv4 address is replaced with the NAT device\u2019s address and similarly, when the response traffic goes to those instances, the NAT device translates the address back to those instances\u2019 private IPv4 addresses. Figure: VPC topology diagram. (Source: AWS, \"VPC with public and private subnets (NAT)\" , AWS Documentation Amazon VPC User Guide, accessed November 18th 2020). Figure: VPC topology diagram with mutiple Nat Gateways for HA. (Source: Andreas Wittig, \"Advanced AWS Networking: Pitfalls That You Should Avoid\" , Cloudonaut.io Blog, accessed November 18th 2020). Read more \u00b6 AWS reference links Consider the following AWS official links as reference: VPC with public and private subnets (NAT) AWS Elastic Load Balancing","title":"VPC Topology"},{"location":"how-it-works/network/vpc-topology/#network-layer","text":"","title":"Network Layer"},{"location":"how-it-works/network/vpc-topology/#network-topology","text":"VPC with public and private subnets (NAT) The configuration for this scenario includes a virtual private cloud (VPC) with public subnets and a private subnets (it's number will change depending on our specific needs). We recommend this scenario if you want to run a public-facing web application, while maintaining back-end servers that aren't publicly accessible. A common example is a multi-tier website, with a Load Balancer (ALB | NLB) in a public subnet, or other public facing routing service like AWS CloudFront or Api Gateway, and our web servers (Lambda, EKS, ECS, EC2) and database (RDS, DynamoDB, etc) servers in private subnets. You can set up security (SGs, ACLs, WAF) and routing so that the web servers can communicate internally (even between VPC accounts or VPN Endpoints) with all necessary services and components such as databases, cache, queues, among others. The services running in the public subnet, like an ALB or NLB can send outbound traffic directly to the Internet, whereas the instances in the private subnet can't. Instead, the instances in the private subnet can access the Internet by using a network address translation (NAT) gateway that resides in the public subnet. The database servers can connect to the Internet for software updates using the NAT gateway (if using RDS this is transparently provided by AWS), but the Internet cannot establish connections to the database servers. So, whenever possible all our AWS resources like EC2, EKS, RDS, Lambda, SQS will be deployed in VPC private subnets and we'll use a NAT device (Nat Gateway) to enable instances in a private subnet to connect to the internet (for example, for software updates) or other AWS services, but prevent the internet from initiating connections with the instances. A NAT device forwards traffic from the instances in the private subnet to the internet (via the VPC Internet Gateway) or other AWS services, and then sends the response back to the instances. When traffic goes to the internet, the source IPv4 address is replaced with the NAT device\u2019s address and similarly, when the response traffic goes to those instances, the NAT device translates the address back to those instances\u2019 private IPv4 addresses. Figure: VPC topology diagram. (Source: AWS, \"VPC with public and private subnets (NAT)\" , AWS Documentation Amazon VPC User Guide, accessed November 18th 2020). Figure: VPC topology diagram with mutiple Nat Gateways for HA. (Source: Andreas Wittig, \"Advanced AWS Networking: Pitfalls That You Should Avoid\" , Cloudonaut.io Blog, accessed November 18th 2020).","title":"Network Topology"},{"location":"how-it-works/network/vpc-topology/#read-more","text":"AWS reference links Consider the following AWS official links as reference: VPC with public and private subnets (NAT) AWS Elastic Load Balancing","title":"Read more"},{"location":"how-it-works/network/vpc-traffic-out/","text":"Network Security \u00b6 Control Internet access outbound traffic \u00b6 Goals \u00b6 Review and analyse available alternatives for controlling outbound traffic in VPCs. All possible candidates need to offer a reasonable balance between features and pricing. Solutions Leverage currently supports \u00b6 Network ACL (Subnet firewall) Security Groups (Instance firewall) What alternatives do we have? \u00b6 Pre-considerations \u00b6 First of all, keep in mind the following points before and while you go through the data in the table: 1 EBS pricing at the moment of this writing: GP2: $0.10 per GB-month GP3: $0.08 per GB-month) 2 DataTransfer costs will be incurred in all options Comparison of the alternatives analysed \u00b6 Leverage Confluence Documentation You'll find here a detailed comparison table including the alternative product and solution types, pricing model, features, pros & cons.","title":"VPC Outbound Traffic"},{"location":"how-it-works/network/vpc-traffic-out/#network-security","text":"","title":"Network Security"},{"location":"how-it-works/network/vpc-traffic-out/#control-internet-access-outbound-traffic","text":"","title":"Control Internet access outbound traffic"},{"location":"how-it-works/network/vpc-traffic-out/#goals","text":"Review and analyse available alternatives for controlling outbound traffic in VPCs. All possible candidates need to offer a reasonable balance between features and pricing. Solutions","title":"Goals"},{"location":"how-it-works/network/vpc-traffic-out/#leverage-currently-supports","text":"Network ACL (Subnet firewall) Security Groups (Instance firewall)","title":"Leverage currently supports"},{"location":"how-it-works/network/vpc-traffic-out/#what-alternatives-do-we-have","text":"","title":"What alternatives do we have?"},{"location":"how-it-works/network/vpc-traffic-out/#pre-considerations","text":"First of all, keep in mind the following points before and while you go through the data in the table: 1 EBS pricing at the moment of this writing: GP2: $0.10 per GB-month GP3: $0.08 per GB-month) 2 DataTransfer costs will be incurred in all options","title":"Pre-considerations"},{"location":"how-it-works/network/vpc-traffic-out/#comparison-of-the-alternatives-analysed","text":"Leverage Confluence Documentation You'll find here a detailed comparison table including the alternative product and solution types, pricing model, features, pros & cons.","title":"Comparison of the alternatives analysed"},{"location":"how-it-works/organization/accounts/","text":"AWS Organization Accounts description \u00b6 Our default AWS Organizations terraform layout solution includes 5 accounts + 1 or N Accts (if you invite pre-existing AWS Account/s). Account Description Root Organizations Used to manage configuration and access to AWS Org managed accounts. The AWS Organizations account provides the ability to create and financially manage member accounts, it contains AWS Organizations Service Control Policies(SCPs). Shared Services / Resources Reference for creating infrastructure shared services such as directory services, DNS, VPN Solution, Monitoring tools like Prometheus and Grafana, CI/CD server (Jenkins, Drone, Spinnaker, etc), centralized logging solution like ELK and Vault Server (Hashicorp Vault) Security Intended for centralized user management via IAM roles based cross-org auth approach (IAM roles per account to be assumed still needed. Also to centralize AWS CloudTrail and AWS Config logs, and used as the master AWS GuardDuty Account Legacy Your pre existing AWS Accounts to be invited as members of the new AWS Organization, probably several services and workloads are going to be progressively migrated to your new Accounts. Apps DevStg Host your DEV, QA and STG environment workloads Compute / Web App Servers (K8s Clusters and Lambda Functions), Load Balancers, DB Servers, Caching Services, Job queues & Servers, Data, Storage, CDN Apps Prod Host your PROD environment workloads Compute / Web App Servers (K8s Clusters and Lambda Functions), Load Balancers, DB Servers, Caching Services, Job queues & Servers, Data, Storage, CDN Account: Shared Services | Resources \u00b6 Account: Apps DevStg | Prod \u00b6","title":"Accounts"},{"location":"how-it-works/organization/accounts/#aws-organization-accounts-description","text":"Our default AWS Organizations terraform layout solution includes 5 accounts + 1 or N Accts (if you invite pre-existing AWS Account/s). Account Description Root Organizations Used to manage configuration and access to AWS Org managed accounts. The AWS Organizations account provides the ability to create and financially manage member accounts, it contains AWS Organizations Service Control Policies(SCPs). Shared Services / Resources Reference for creating infrastructure shared services such as directory services, DNS, VPN Solution, Monitoring tools like Prometheus and Grafana, CI/CD server (Jenkins, Drone, Spinnaker, etc), centralized logging solution like ELK and Vault Server (Hashicorp Vault) Security Intended for centralized user management via IAM roles based cross-org auth approach (IAM roles per account to be assumed still needed. Also to centralize AWS CloudTrail and AWS Config logs, and used as the master AWS GuardDuty Account Legacy Your pre existing AWS Accounts to be invited as members of the new AWS Organization, probably several services and workloads are going to be progressively migrated to your new Accounts. Apps DevStg Host your DEV, QA and STG environment workloads Compute / Web App Servers (K8s Clusters and Lambda Functions), Load Balancers, DB Servers, Caching Services, Job queues & Servers, Data, Storage, CDN Apps Prod Host your PROD environment workloads Compute / Web App Servers (K8s Clusters and Lambda Functions), Load Balancers, DB Servers, Caching Services, Job queues & Servers, Data, Storage, CDN","title":"AWS Organization Accounts description"},{"location":"how-it-works/organization/accounts/#account-shared-services-resources","text":"","title":"Account: Shared Services | Resources"},{"location":"how-it-works/organization/accounts/#account-apps-devstg-prod","text":"","title":"Account: Apps DevStg | Prod"},{"location":"how-it-works/organization/billing/","text":"AWS Organizations Billing \u00b6 Overview \u00b6 Each month AWS charges your payer Root Account for all the linked accounts in a consolidated bill. The following illustration shows an example of a consolidated bill. Figure: AWS Organization Multi-Account structure (just as reference). (Source: Andreas Wittig, \"AWS Account Structure: Think twice before using AWS Organizations\" , Cloudonaut.io Blog, accessed November 18th 2020). Figure: AWS Organization Multi-Account billing structure (just as reference). (Source: AWS, \"Consolidated billing process\" , AWS Documentation AWS Billing and Cost Management User Guide, accessed November 18th 2020). Reference Architecture AWS Organizations features AWS Multiple Account Billing Strategy: consolidated billing for all your accounts within organization, enhanced per account cost filtering and RI usage A single monthly bill accumulates the spending among many AWS accounts. Benefit from volume pricing across more than one AWS account. AWS Organizations Billing FAQs What does AWS Organizations cost? AWS Organizations is offered at no additional charge. Who pays for usage incurred by users under an AWS member account in my organization? The owner of the master account is responsible for paying for all usage, data, and resources used by the accounts in the organization. Will my bill reflect the organizational unit structure that I created in my organization? No. For now, your bill will not reflect the structure that you have defined in your organization. You can use cost allocation tags in individual AWS accounts to categorize and track your AWS costs, and this allocation will be visible in the consolidated bill for your organization. Source | AWS Organizations FAQs Read more \u00b6 Reference links Consider the following extra links as reference: Cloudnout.io | AWS Account Structure AWS Ramp-Up Guide: Cost Management","title":"Billing"},{"location":"how-it-works/organization/billing/#aws-organizations-billing","text":"","title":"AWS Organizations Billing"},{"location":"how-it-works/organization/billing/#overview","text":"Each month AWS charges your payer Root Account for all the linked accounts in a consolidated bill. The following illustration shows an example of a consolidated bill. Figure: AWS Organization Multi-Account structure (just as reference). (Source: Andreas Wittig, \"AWS Account Structure: Think twice before using AWS Organizations\" , Cloudonaut.io Blog, accessed November 18th 2020). Figure: AWS Organization Multi-Account billing structure (just as reference). (Source: AWS, \"Consolidated billing process\" , AWS Documentation AWS Billing and Cost Management User Guide, accessed November 18th 2020). Reference Architecture AWS Organizations features AWS Multiple Account Billing Strategy: consolidated billing for all your accounts within organization, enhanced per account cost filtering and RI usage A single monthly bill accumulates the spending among many AWS accounts. Benefit from volume pricing across more than one AWS account. AWS Organizations Billing FAQs What does AWS Organizations cost? AWS Organizations is offered at no additional charge. Who pays for usage incurred by users under an AWS member account in my organization? The owner of the master account is responsible for paying for all usage, data, and resources used by the accounts in the organization. Will my bill reflect the organizational unit structure that I created in my organization? No. For now, your bill will not reflect the structure that you have defined in your organization. You can use cost allocation tags in individual AWS accounts to categorize and track your AWS costs, and this allocation will be visible in the consolidated bill for your organization. Source | AWS Organizations FAQs","title":"Overview"},{"location":"how-it-works/organization/billing/#read-more","text":"Reference links Consider the following extra links as reference: Cloudnout.io | AWS Account Structure AWS Ramp-Up Guide: Cost Management","title":"Read more"},{"location":"how-it-works/organization/organization/","text":"Reference Architecture: Terraform AWS Organizations Account Baseline \u00b6 Overview \u00b6 This repository contains all Terraform configuration files used to create Binbash Leverage Reference AWS Organizations Multi-Account baseline layout. Why AWS Organizations? This approach allows it to have a hierarchical structure of AWS accounts, providing additional security isolation and the ability to separate resources into Organizational Units with it associated Service Control Policies (SCP). Considering that a current AWS account/s was/were already active (Client AWS Legacy Account), this one will then be invited to be a \u201cmember account\u201d of the AWS Organization architecture. In the future, once all Client\u2019s Legacy dev, stage, prod and other resources for the Project applications are running in the new accounts architecture, meaning a full AWS Organizations approach, all the already migrated assets from the \u2018Legacy\u2019 account should be decommissioned. This account will remain with the necessary services, such as DNS, among others. AWS Organization Accounts Layout \u00b6 The following block provides a brief explanation of the chosen AWS Organization Accounts layout: + devstg/ (resources for dev apps/services account) ... + prod/ (resources for prod apps/services account) ... + root/ (resources for the root-org account) ... + security/ (resources for the security + users account) ... + shared/ (resources for the shared account) ... + legacy/ (resources for the legacy/pre-existing account) ... NOTE: Image just as reference Benefits of AWS Organizations Billing: Consolidated billing for all your accounts within organization, enhanced per account cost filtering and RI usage Security I: Extra security layer: You get fully isolated infrastructure for different organizations units in your projects, eg: Dev, Prod, Shared Resources, Security, Users, BI, etc. Security II: Using AWS Organization you may use Service Control Policies (SCPs) to control which AWS services are available within different accounts. Networking: Connectivity and access will be securely setup via VPC peering + NACLS + Sec Groups everything with private endpoints only accessible v\u00eda Pritunl VPN significantly reducing the surface of attack. User Mgmt: You can manage all your IAM resources (users/groups/roles) and policies in one place (usually, security/users account) and use AssumeRole to works with org accounts. Operations: Will reduce the blast radius to the maximum possible. Compatibility: Legacy accounts can (probably should) be invited as a member of the new Organization and afterwards even imported into your terraform code . Migration: After having your baseline AWS Org reference cloud solutions architecture deployed (IAM, VPC, NACLS, VPC-Peering, DNS Cross-Org, CloudTrail, etc) you're ready to start progressively orchestrating new resources in order to segregate different Environment and Services per account. This approach will allow you to start a 1 by 1 Blue/Green (Red/Black) migration without affecting any of your services at all . You would like to take advantage of an Active-Active DNS switchover approach (nice as DR exercise too). EXAMPLE: Jenkins CI Server Migration steps: Let's say you have your EC2_A ( jenkins.aws.domain.com ) in Account_A (Legacy), so you could deploy a brand new EC2_B Jenkins Instance in Account_B (Shared Resources). Temporally associated with jenkins2.aws.domain.com Sync it's current data ( /var/lib/jenkins ) Test and fully validate every job and pipeline works as expected. In case you haven't finished your validations we highly recommend to declare everything as code and fully automated so as to destroy and re-create your under development env on demand to save costs. Finally switch jenkins2.aws.domain.com -> to -> jenkins.aws.domain.com Stop your old EC2_A. If everything looks fine after after 2/4 weeks you could terminate your EC2_A (hope everything is as code and just terraform destroy ) Considering the previously detailed steps plan your roadmap to move forward with every other component to be migrated. Read more \u00b6 AWS reference links Consider the following AWS official links as reference: Why should I set up a multi-account AWS environment? AWS Multiple Account User Management Strategy AWS Muttiple Account Security Strategy AWS Multiple Account Billing Strategy AWS Secure Account Setup Authentication and Access Control for AWS Organizations (keep in mind EC2 and other services can also use AWS IAM Roles to get secure cross-account access)","title":"Organization"},{"location":"how-it-works/organization/organization/#reference-architecture-terraform-aws-organizations-account-baseline","text":"","title":"Reference Architecture: Terraform AWS Organizations Account Baseline"},{"location":"how-it-works/organization/organization/#overview","text":"This repository contains all Terraform configuration files used to create Binbash Leverage Reference AWS Organizations Multi-Account baseline layout. Why AWS Organizations? This approach allows it to have a hierarchical structure of AWS accounts, providing additional security isolation and the ability to separate resources into Organizational Units with it associated Service Control Policies (SCP). Considering that a current AWS account/s was/were already active (Client AWS Legacy Account), this one will then be invited to be a \u201cmember account\u201d of the AWS Organization architecture. In the future, once all Client\u2019s Legacy dev, stage, prod and other resources for the Project applications are running in the new accounts architecture, meaning a full AWS Organizations approach, all the already migrated assets from the \u2018Legacy\u2019 account should be decommissioned. This account will remain with the necessary services, such as DNS, among others.","title":"Overview"},{"location":"how-it-works/organization/organization/#aws-organization-accounts-layout","text":"The following block provides a brief explanation of the chosen AWS Organization Accounts layout: + devstg/ (resources for dev apps/services account) ... + prod/ (resources for prod apps/services account) ... + root/ (resources for the root-org account) ... + security/ (resources for the security + users account) ... + shared/ (resources for the shared account) ... + legacy/ (resources for the legacy/pre-existing account) ... NOTE: Image just as reference Benefits of AWS Organizations Billing: Consolidated billing for all your accounts within organization, enhanced per account cost filtering and RI usage Security I: Extra security layer: You get fully isolated infrastructure for different organizations units in your projects, eg: Dev, Prod, Shared Resources, Security, Users, BI, etc. Security II: Using AWS Organization you may use Service Control Policies (SCPs) to control which AWS services are available within different accounts. Networking: Connectivity and access will be securely setup via VPC peering + NACLS + Sec Groups everything with private endpoints only accessible v\u00eda Pritunl VPN significantly reducing the surface of attack. User Mgmt: You can manage all your IAM resources (users/groups/roles) and policies in one place (usually, security/users account) and use AssumeRole to works with org accounts. Operations: Will reduce the blast radius to the maximum possible. Compatibility: Legacy accounts can (probably should) be invited as a member of the new Organization and afterwards even imported into your terraform code . Migration: After having your baseline AWS Org reference cloud solutions architecture deployed (IAM, VPC, NACLS, VPC-Peering, DNS Cross-Org, CloudTrail, etc) you're ready to start progressively orchestrating new resources in order to segregate different Environment and Services per account. This approach will allow you to start a 1 by 1 Blue/Green (Red/Black) migration without affecting any of your services at all . You would like to take advantage of an Active-Active DNS switchover approach (nice as DR exercise too). EXAMPLE: Jenkins CI Server Migration steps: Let's say you have your EC2_A ( jenkins.aws.domain.com ) in Account_A (Legacy), so you could deploy a brand new EC2_B Jenkins Instance in Account_B (Shared Resources). Temporally associated with jenkins2.aws.domain.com Sync it's current data ( /var/lib/jenkins ) Test and fully validate every job and pipeline works as expected. In case you haven't finished your validations we highly recommend to declare everything as code and fully automated so as to destroy and re-create your under development env on demand to save costs. Finally switch jenkins2.aws.domain.com -> to -> jenkins.aws.domain.com Stop your old EC2_A. If everything looks fine after after 2/4 weeks you could terminate your EC2_A (hope everything is as code and just terraform destroy ) Considering the previously detailed steps plan your roadmap to move forward with every other component to be migrated.","title":"AWS Organization Accounts Layout"},{"location":"how-it-works/organization/organization/#read-more","text":"AWS reference links Consider the following AWS official links as reference: Why should I set up a multi-account AWS environment? AWS Multiple Account User Management Strategy AWS Muttiple Account Security Strategy AWS Multiple Account Billing Strategy AWS Secure Account Setup Authentication and Access Control for AWS Organizations (keep in mind EC2 and other services can also use AWS IAM Roles to get secure cross-account access)","title":"Read more"},{"location":"how-it-works/reliability/backups/","text":"Backups \u00b6 AWS Backup \u00b6 As defined by AWS AWS Backup is a fully managed backup service that makes it easy to centralize and automate the backup of data across AWS services. Using AWS Backup, you can centrally configure backup policies and monitor backup activity for AWS resources, such as: Amazon EBS volumes , Amazon EC2 instances , Amazon RDS databases , Amazon DynamoDB tables , Amazon EFS file systems , and AWS Storage Gateway volumes . AWS Backup automates and consolidates backup tasks previously performed service-by-service, removing the need to create custom scripts and manual processes. With just a few clicks in the AWS Backup console, you can create backup policies that automate backup schedules and retention management. AWS Backup provides a fully managed, policy-based backup solution, simplifying your backup management, enabling you to meet your business and regulatory backup compliance requirements. Figure: AWS Backup service diagram (just as reference). (Source: AWS, \"AWS Backup - Centrally manage and automate backups across AWS services\" , AWS Documentation, accessed November 18th 2020). S3 bucket region replication \u00b6 Buckets that hold data critical to business or to application operation can be replicated to another region almost synchronously. This can be setup on request to increase durability and along with database backup can constitute the base for a Business Continuity strategy.","title":"Backups"},{"location":"how-it-works/reliability/backups/#backups","text":"","title":"Backups"},{"location":"how-it-works/reliability/backups/#aws-backup","text":"As defined by AWS AWS Backup is a fully managed backup service that makes it easy to centralize and automate the backup of data across AWS services. Using AWS Backup, you can centrally configure backup policies and monitor backup activity for AWS resources, such as: Amazon EBS volumes , Amazon EC2 instances , Amazon RDS databases , Amazon DynamoDB tables , Amazon EFS file systems , and AWS Storage Gateway volumes . AWS Backup automates and consolidates backup tasks previously performed service-by-service, removing the need to create custom scripts and manual processes. With just a few clicks in the AWS Backup console, you can create backup policies that automate backup schedules and retention management. AWS Backup provides a fully managed, policy-based backup solution, simplifying your backup management, enabling you to meet your business and regulatory backup compliance requirements. Figure: AWS Backup service diagram (just as reference). (Source: AWS, \"AWS Backup - Centrally manage and automate backups across AWS services\" , AWS Documentation, accessed November 18th 2020).","title":"AWS Backup"},{"location":"how-it-works/reliability/backups/#s3-bucket-region-replication","text":"Buckets that hold data critical to business or to application operation can be replicated to another region almost synchronously. This can be setup on request to increase durability and along with database backup can constitute the base for a Business Continuity strategy.","title":"S3 bucket region replication"},{"location":"how-it-works/reliability/dr/","text":"Disaster Recovery & Business Continuity Plan \u00b6 Overview \u00b6 Applications that are business critical should always have a plan in place to recover in case of a catastrophic failure or disaster. There are many strategies that can be implemented to achieve this, and deciding between them is a matter of analyzing how much is worth to invest based on calculation of damages suffered if the application is not available for a given period of time. It is based on this factor (time) that disaster recovery plans are based on. Factors that need to be determined per application are: RTO and RPO Recovery time objective (RTO): This represents the time it takes after a disruption to restore a business process to its service level. For example, if a disaster occurs at 12:00 PM (noon) and the RTO is eight hours, the DR process should restore the business process to the acceptable service level by 8:00 PM. Recovery point objective (RPO): This is the acceptable amount of data loss measured in time. For example, if a disaster occurs at 12:00 PM (noon) and the RPO is one hour, the system should recover all data that was in the system before that hour. High Availability Configuration Strategies \u00b6 After deciding RTO and RPO we have options available to achieve the time objectives: HA Strategies Backup and restore: In most traditional environments, data is backed up to tape and sent off-site regularly. The equivalent in AWS would be to take backups in the form of snapshots and copy them to another region for RDS instances, EBS volumes, EFS and S3 buckets. The plan details the step-by-step procedure to recover a fully working production environment based on these backups being restored on freshly provisioned infrastructure, and how to rollback to a regular production site once the emergency is over. Pilot Light Method: The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in AWS. Very similar to \u201cBackup and restore\u201d except a minimal version of key infrastructure components is provisioned in a separate region and then scaled up in case of disaster declaration. Warm standby active-passive method: The term warm-standby is used to describe a DR scenario in which a scaled-down version of a fully-functional environment is always running in the cloud. Enhancement of Pilot Light in which a minimal version is created of all components, not just critical ones. Multi-Region active-active method: By architecting multi region applications and using DNS to balance between them in normal production status, you can adjust the DNS weighting and send all traffic to the AWS region that is available, this can even be performed automatically with Route53 or other DNS services that provide health check mechanisms as well as load balancing. Figure: 2 sets of app instances, each behind an elastic load balancer in two separate regions (just as reference). (Source: Randika Rathugamage, \"High Availability with Route53 DNS Failover\" , Medium blogpost, accessed December 1st 2020). Figure: AWS calculated \u2014 or parent \u2014 health check, we can fail on any number of child health checks (just as reference). (Source: Simon Tabor, \"How to implement the perfect failover strategy using Amazon Route53\" , Medium blogpost, accessed December 1st 2020). Read more \u00b6 AWS reference links Consider the following AWS official links as reference: AWS Documentation Amazon Route 53 Developer Guide | Configuring DNS failover","title":"Disaster Recovery"},{"location":"how-it-works/reliability/dr/#disaster-recovery-business-continuity-plan","text":"","title":"Disaster Recovery &amp; Business Continuity Plan"},{"location":"how-it-works/reliability/dr/#overview","text":"Applications that are business critical should always have a plan in place to recover in case of a catastrophic failure or disaster. There are many strategies that can be implemented to achieve this, and deciding between them is a matter of analyzing how much is worth to invest based on calculation of damages suffered if the application is not available for a given period of time. It is based on this factor (time) that disaster recovery plans are based on. Factors that need to be determined per application are: RTO and RPO Recovery time objective (RTO): This represents the time it takes after a disruption to restore a business process to its service level. For example, if a disaster occurs at 12:00 PM (noon) and the RTO is eight hours, the DR process should restore the business process to the acceptable service level by 8:00 PM. Recovery point objective (RPO): This is the acceptable amount of data loss measured in time. For example, if a disaster occurs at 12:00 PM (noon) and the RPO is one hour, the system should recover all data that was in the system before that hour.","title":"Overview"},{"location":"how-it-works/reliability/dr/#high-availability-configuration-strategies","text":"After deciding RTO and RPO we have options available to achieve the time objectives: HA Strategies Backup and restore: In most traditional environments, data is backed up to tape and sent off-site regularly. The equivalent in AWS would be to take backups in the form of snapshots and copy them to another region for RDS instances, EBS volumes, EFS and S3 buckets. The plan details the step-by-step procedure to recover a fully working production environment based on these backups being restored on freshly provisioned infrastructure, and how to rollback to a regular production site once the emergency is over. Pilot Light Method: The term pilot light is often used to describe a DR scenario in which a minimal version of an environment is always running in AWS. Very similar to \u201cBackup and restore\u201d except a minimal version of key infrastructure components is provisioned in a separate region and then scaled up in case of disaster declaration. Warm standby active-passive method: The term warm-standby is used to describe a DR scenario in which a scaled-down version of a fully-functional environment is always running in the cloud. Enhancement of Pilot Light in which a minimal version is created of all components, not just critical ones. Multi-Region active-active method: By architecting multi region applications and using DNS to balance between them in normal production status, you can adjust the DNS weighting and send all traffic to the AWS region that is available, this can even be performed automatically with Route53 or other DNS services that provide health check mechanisms as well as load balancing. Figure: 2 sets of app instances, each behind an elastic load balancer in two separate regions (just as reference). (Source: Randika Rathugamage, \"High Availability with Route53 DNS Failover\" , Medium blogpost, accessed December 1st 2020). Figure: AWS calculated \u2014 or parent \u2014 health check, we can fail on any number of child health checks (just as reference). (Source: Simon Tabor, \"How to implement the perfect failover strategy using Amazon Route53\" , Medium blogpost, accessed December 1st 2020).","title":"High Availability Configuration Strategies"},{"location":"how-it-works/reliability/dr/#read-more","text":"AWS reference links Consider the following AWS official links as reference: AWS Documentation Amazon Route 53 Developer Guide | Configuring DNS failover","title":"Read more"},{"location":"how-it-works/reliability/health-checks/","text":"","title":"Health Checks"},{"location":"how-it-works/reliability/high-availability/","text":"High Availability & Helthchecks \u00b6 Recovery from Failures \u00b6 Automatic recovery from failure It keeps an AWS environment reliable. Using logs and metrics from CloudWatch, designing a system where the failures themselves trigger recovery is the way to move forward. Figure: AWS HA architecture diagrams (just as reference). Recovery Procedures \u00b6 Test recovery procedures The risks faced by cloud environment and systems, the points of failure for systems and ecosystems, as well as details about the most probable attacks are known and can be simulated. Testing recovery procedures are something that can be done using these insights. Real points of failure are exploited and the way the environment reacts to the emergency shows just how reliable the system it. Figure: AWS HA architecture diagrams (just as reference). Scalability and Availability \u00b6 Scale horizontally to increase aggregate system availability The cloud environment needs to have multiple redundancies and additional modules as added security measures. Of course, multiple redundancies require good management and maintenance for them to remain active through the environment\u2019s lifecycle. Figure: AWS HA scalable architecture diagrams (just as reference). Helthchecks & Self-healing \u00b6 K8s and containers \u00b6 K8s readiness and liveness probes Distributed systems can be hard to manage. A big reason is that there are many moving parts that all need to work for the system to function. If a small part breaks, the system has to detect it, route around it, and fix it. And this all needs to be done automatically! Health checks are a simple way to let the system know if an instance of your app is working or not working. If an instance of your app is not working, then other services should not access it or send a request to it. Instead, requests should be sent to another instance of the app that is ready, or re-tried at a later time. The system should also bring your app back to a healthy state. By default, Kubernetes starts to send traffic to a pod when all the containers inside the pod start, and restarts containers when they crash. While this can be \u201cgood enough\u201d when you are starting out, you can make your deployments more robust by creating custom health checks. Fortunately, Kubernetes make this relatively straightforward, so there is no excuse not to!\u201d So aside from the monitoring and alerting that underlying infrastructure will have, application container will have their own mechanisms to determine readiness and liveness. These are features that our scheduler of choice Kubernetes natively allows, to read more click here .","title":"High Availability"},{"location":"how-it-works/reliability/high-availability/#high-availability-helthchecks","text":"","title":"High Availability &amp; Helthchecks"},{"location":"how-it-works/reliability/high-availability/#recovery-from-failures","text":"Automatic recovery from failure It keeps an AWS environment reliable. Using logs and metrics from CloudWatch, designing a system where the failures themselves trigger recovery is the way to move forward. Figure: AWS HA architecture diagrams (just as reference).","title":"Recovery from Failures"},{"location":"how-it-works/reliability/high-availability/#recovery-procedures","text":"Test recovery procedures The risks faced by cloud environment and systems, the points of failure for systems and ecosystems, as well as details about the most probable attacks are known and can be simulated. Testing recovery procedures are something that can be done using these insights. Real points of failure are exploited and the way the environment reacts to the emergency shows just how reliable the system it. Figure: AWS HA architecture diagrams (just as reference).","title":"Recovery Procedures"},{"location":"how-it-works/reliability/high-availability/#scalability-and-availability","text":"Scale horizontally to increase aggregate system availability The cloud environment needs to have multiple redundancies and additional modules as added security measures. Of course, multiple redundancies require good management and maintenance for them to remain active through the environment\u2019s lifecycle. Figure: AWS HA scalable architecture diagrams (just as reference).","title":"Scalability and Availability"},{"location":"how-it-works/reliability/high-availability/#helthchecks-self-healing","text":"","title":"Helthchecks &amp; Self-healing"},{"location":"how-it-works/reliability/high-availability/#k8s-and-containers","text":"K8s readiness and liveness probes Distributed systems can be hard to manage. A big reason is that there are many moving parts that all need to work for the system to function. If a small part breaks, the system has to detect it, route around it, and fix it. And this all needs to be done automatically! Health checks are a simple way to let the system know if an instance of your app is working or not working. If an instance of your app is not working, then other services should not access it or send a request to it. Instead, requests should be sent to another instance of the app that is ready, or re-tried at a later time. The system should also bring your app back to a healthy state. By default, Kubernetes starts to send traffic to a pod when all the containers inside the pod start, and restarts containers when they crash. While this can be \u201cgood enough\u201d when you are starting out, you can make your deployments more robust by creating custom health checks. Fortunately, Kubernetes make this relatively straightforward, so there is no excuse not to!\u201d So aside from the monitoring and alerting that underlying infrastructure will have, application container will have their own mechanisms to determine readiness and liveness. These are features that our scheduler of choice Kubernetes natively allows, to read more click here .","title":"K8s and containers"},{"location":"how-it-works/secrets/secrets/","text":"Secret and password mgmt tools \u00b6 Overview \u00b6 Ensure scalability, availability and persistence, as well as secure, hierarchical storage to manage configuration and secret data for: Secret Managers AWS KMS AWS SSM Parameter Store Ansible Vault Hashicorp Vault Strengths Improve the level of security by validating separation of environment variables and code secrets. Control and audit granular access in detail Store secure chain and configuration data in hierarchies and track versions. Configure integration with AWS KMS, Amazon SNS, Amazon CloudWatch, and AWS CloudTrail to notify, monitor, and audit functionality. Read more \u00b6 Related articles A Comparison of Secrets Managers for AWS Clean Up Your Secrets & Credential Management","title":"Secrets"},{"location":"how-it-works/secrets/secrets/#secret-and-password-mgmt-tools","text":"","title":"Secret and password mgmt tools"},{"location":"how-it-works/secrets/secrets/#overview","text":"Ensure scalability, availability and persistence, as well as secure, hierarchical storage to manage configuration and secret data for: Secret Managers AWS KMS AWS SSM Parameter Store Ansible Vault Hashicorp Vault Strengths Improve the level of security by validating separation of environment variables and code secrets. Control and audit granular access in detail Store secure chain and configuration data in hierarchies and track versions. Configure integration with AWS KMS, Amazon SNS, Amazon CloudWatch, and AWS CloudTrail to notify, monitor, and audit functionality.","title":"Overview"},{"location":"how-it-works/secrets/secrets/#read-more","text":"Related articles A Comparison of Secrets Managers for AWS Clean Up Your Secrets & Credential Management","title":"Read more"},{"location":"how-it-works/security/iam-access-analyzer/","text":"IAM Access Analyzer \u00b6 Overview \u00b6 Access Analyzer analyzes the resource-based policies that are applied to AWS resources in the Region where you enabled Access Analyzer. Only resource-based policies are analyzed. Supported resource types: Amazon Simple Storage Service buckets AWS Identity and Access Management roles AWS Key Management Service keys AWS Lambda functions and layers Amazon Simple Queue Service queues AWS Secrets Manager secrets Figure: AWS IAM access analysis features. (Source: AWS, \"How it works - monitoring external access to resources\" , AWS Documentation, accessed June 11th 2021). AWS Organizations \u00b6 CONSIDERATION: AWS Organization integration In order to enable AccessAnalyzer with the Organization at the zone of of trust in the Security account, this account needs to be set as a delegated administrator . Such step cannot be performed by Terraform yet so it was set up manually as described below: https://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer-settings.html If you're configuring AWS IAM Access Analyzer in your AWS Organizations management account, you can add a member account in the organization as the delegated administrator to manage Access Analyzer for your organization. The delegated administrator has permissions to create and manage analyzers with the organization as the zone of trust. Only the management account can add a delegated administrator. Reference Architecture implementation code \u00b6 Reference Architecture Code: le-tf-infra-aws/security/security-base/iam_access_analizer.tf resource \"aws_accessanalyzer_analyzer\" \"default\" { analyzer_name = \"ConsoleAnalyzer-bc3bc4d6-09cb-XXXX-XXXX-XXXXXXXXXX\" type = \"ORGANIZATION\" tags = local.tags } AWS Web Console \u00b6 Figure: AWS Web Console screenshot. (Source: Binbash, \"IAM access analyzer service\", accessed June 11th 2021).","title":"IAM Access Analyzer"},{"location":"how-it-works/security/iam-access-analyzer/#iam-access-analyzer","text":"","title":"IAM Access Analyzer"},{"location":"how-it-works/security/iam-access-analyzer/#overview","text":"Access Analyzer analyzes the resource-based policies that are applied to AWS resources in the Region where you enabled Access Analyzer. Only resource-based policies are analyzed. Supported resource types: Amazon Simple Storage Service buckets AWS Identity and Access Management roles AWS Key Management Service keys AWS Lambda functions and layers Amazon Simple Queue Service queues AWS Secrets Manager secrets Figure: AWS IAM access analysis features. (Source: AWS, \"How it works - monitoring external access to resources\" , AWS Documentation, accessed June 11th 2021).","title":"Overview"},{"location":"how-it-works/security/iam-access-analyzer/#aws-organizations","text":"CONSIDERATION: AWS Organization integration In order to enable AccessAnalyzer with the Organization at the zone of of trust in the Security account, this account needs to be set as a delegated administrator . Such step cannot be performed by Terraform yet so it was set up manually as described below: https://docs.aws.amazon.com/IAM/latest/UserGuide/access-analyzer-settings.html If you're configuring AWS IAM Access Analyzer in your AWS Organizations management account, you can add a member account in the organization as the delegated administrator to manage Access Analyzer for your organization. The delegated administrator has permissions to create and manage analyzers with the organization as the zone of trust. Only the management account can add a delegated administrator.","title":"AWS Organizations"},{"location":"how-it-works/security/iam-access-analyzer/#reference-architecture-implementation-code","text":"Reference Architecture Code: le-tf-infra-aws/security/security-base/iam_access_analizer.tf resource \"aws_accessanalyzer_analyzer\" \"default\" { analyzer_name = \"ConsoleAnalyzer-bc3bc4d6-09cb-XXXX-XXXX-XXXXXXXXXX\" type = \"ORGANIZATION\" tags = local.tags }","title":"Reference Architecture implementation code"},{"location":"how-it-works/security/iam-access-analyzer/#aws-web-console","text":"Figure: AWS Web Console screenshot. (Source: Binbash, \"IAM access analyzer service\", accessed June 11th 2021).","title":"AWS Web Console"},{"location":"how-it-works/security/services/","text":"AWS Security & Compliance Services \u00b6 Security Directives There will not be any instance port or service port open to general access, unless justified by business reasons, and we\u2019ll take alternative means of security to mitigate any possible risk. Every account will have a set of active services that will allow for administrative users (SecOps) to audit all actions and track potentially dangerous behavior. All services will be enabled via IaC (Terraform or SDK and tracked in the proper git repo). AWS Managed Security Services AWS IAM Access Analyzer: Generates comprehensive findings that identify resources policies for public or cross-account accessibility, monitors and helps you refine permissions. Provides the highest levels of security assurance. AWS Config: Tracks changes made to AWS resources over time, making possible to return to a previous state. Monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired compliance rule set. Adds accountability factor. AWS Cloudtrail: Stores logs over all calls made to AWS APIs, coming from web console, command line or any other. Allowing us to monitor it via CW Dashboards and notifications. AWS VPC Flow Logs: Enables us to examine individual Network Interfaces logs, to address network issues and also monitor suspicious behavior. AWS Web Application Firewall: Optional but if not used, it is recommended that a similar service is used, such as Cloudflare. When paired to an Application Load Balancer or Cloudfront distribution, it checks incoming requests to detect and block OWAPS Top10 attacks, such as SQL injection, XSS and others. AWS Inspector: Is an automated security assessment service that helps improve the security and compliance of infrastructure and applications deployed on AWS. AWS Guard Duty: Is a managed threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. Detects unusual API calls or potentially unauthorized deployments (possible account compromise) and potentially compromised instances or reconnaissance by attackers. AWS Security Logs Other access logs from client-facing resources will be stored in the Security account.","title":"Services"},{"location":"how-it-works/security/services/#aws-security-compliance-services","text":"Security Directives There will not be any instance port or service port open to general access, unless justified by business reasons, and we\u2019ll take alternative means of security to mitigate any possible risk. Every account will have a set of active services that will allow for administrative users (SecOps) to audit all actions and track potentially dangerous behavior. All services will be enabled via IaC (Terraform or SDK and tracked in the proper git repo). AWS Managed Security Services AWS IAM Access Analyzer: Generates comprehensive findings that identify resources policies for public or cross-account accessibility, monitors and helps you refine permissions. Provides the highest levels of security assurance. AWS Config: Tracks changes made to AWS resources over time, making possible to return to a previous state. Monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired compliance rule set. Adds accountability factor. AWS Cloudtrail: Stores logs over all calls made to AWS APIs, coming from web console, command line or any other. Allowing us to monitor it via CW Dashboards and notifications. AWS VPC Flow Logs: Enables us to examine individual Network Interfaces logs, to address network issues and also monitor suspicious behavior. AWS Web Application Firewall: Optional but if not used, it is recommended that a similar service is used, such as Cloudflare. When paired to an Application Load Balancer or Cloudfront distribution, it checks incoming requests to detect and block OWAPS Top10 attacks, such as SQL injection, XSS and others. AWS Inspector: Is an automated security assessment service that helps improve the security and compliance of infrastructure and applications deployed on AWS. AWS Guard Duty: Is a managed threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. Detects unusual API calls or potentially unauthorized deployments (possible account compromise) and potentially compromised instances or reconnaissance by attackers. AWS Security Logs Other access logs from client-facing resources will be stored in the Security account.","title":"AWS Security &amp; Compliance Services"},{"location":"how-it-works/security/vpn/","text":"VPN Server \u00b6 To securely and scalable privately access AWS Cross Organization resources we\u2019ll implement Pritunl VPN Server \u00b6 Security Directives Private HTTP endpoints for Applications (FrontEnd + APIs), SSH, monitoring & logging (UI / Dashboards) among others. Eg: Jenkins, DroneCI, EFK, Prometheus, Spinnaker, Grafana. K8s API via kubectl private endpoint eg: avoiding emergency K8s API vulnerability patching. Limit exposure: Limit the exposure of the workload to the internet and internal networks by only allowing minimum required access -> Avoiding exposure for Dev/QA/Stg http endpoints The Pritunl OpenVPN Linux instance is hardened and only runs this VPN solution. All other ports/access is restricted. Each VPN user can be required to use MFA to connect via VPN (as well as strong passwords). This combination makes almost impossible for an outsider to gain access via VPN. Centralized access and audit logs. Figure: Securing access to a private network with Pritunl diagram. (Source: Pritunl, \"Accessing a Private Network\" , Pritunl documentantion v1 Guides, accessed November 17th 2020). Read More \u00b6 Pritunl - Open Source Enterprise Distributed OpenVPN, IPsec and WireGuard Server Specifications","title":"VPN"},{"location":"how-it-works/security/vpn/#vpn-server","text":"","title":"VPN Server"},{"location":"how-it-works/security/vpn/#to-securely-and-scalable-privately-access-aws-cross-organization-resources-well-implement-pritunl-vpn-server","text":"Security Directives Private HTTP endpoints for Applications (FrontEnd + APIs), SSH, monitoring & logging (UI / Dashboards) among others. Eg: Jenkins, DroneCI, EFK, Prometheus, Spinnaker, Grafana. K8s API via kubectl private endpoint eg: avoiding emergency K8s API vulnerability patching. Limit exposure: Limit the exposure of the workload to the internet and internal networks by only allowing minimum required access -> Avoiding exposure for Dev/QA/Stg http endpoints The Pritunl OpenVPN Linux instance is hardened and only runs this VPN solution. All other ports/access is restricted. Each VPN user can be required to use MFA to connect via VPN (as well as strong passwords). This combination makes almost impossible for an outsider to gain access via VPN. Centralized access and audit logs. Figure: Securing access to a private network with Pritunl diagram. (Source: Pritunl, \"Accessing a Private Network\" , Pritunl documentantion v1 Guides, accessed November 17th 2020).","title":"To securely and scalable privately access AWS Cross Organization resources we\u2019ll implement Pritunl VPN Server"},{"location":"how-it-works/security/vpn/#read-more","text":"Pritunl - Open Source Enterprise Distributed OpenVPN, IPsec and WireGuard Server Specifications","title":"Read More"},{"location":"how-it-works/storage/storage/","text":"Storage \u00b6 We will review all S3 buckets in the existing account to determine if it\u2019s necessary to copy over to the new account, evaluate existing bucket policy and tightening permissions to be absolutely minimum required for users and applications. As for EBS volumes, our recommendation is to create all encrypted by default. Overhead created by this process is negligible. S3 buckets \u00b6 Tech specs Encryption: Yes (by default) Object versioning: TBD per bucket Access logs enabled: TBD per bucket MFA delete: Yes on critical buckets Replication to another region: TBD per bucket Storage class Designed for Durability (designed for) Availability (designed for) Availability Zones Min storage duration Min billable object size Other considerations S3 Standard Frequently accessed data 99.999999999% 99.99% >= 3 None None None S3 Standard-IA Long-lived, infrequently accessed data 99.999999999% 99.9% >= 3 30 days 128 KB Per GB retrieval fees apply. S3 Intelligent-Tiering Long-lived data with changing or unknown access patterns 99.999999999% 99.9% >= 3 30 days None Monitoring and automation fees per object apply. No retrieval fees. S3 One Zone-IA Long-lived, infrequently accessed, non-critical data 99.999999999% 99.5% 1 30 days 128 KB Per GB retrieval fees apply. Not resilient to the loss of the Availability Zone. S3 Glacier Long-term data archiving with retrieval times ranging from minutes to hours 99.999999999% 99.99% (after you restore objects) >= 3 90 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For more information, see Restoring archived objects. S3 Glacier Deep Archive Archiving rarely accessed data with a default retrieval time of 12 hours 99.999999999% 99.99% (after you restore objects) >= 3 180 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For more information, see Restoring archived objects. RRS (Not recommended) Frequently accessed, non-critical data 99.99% 99.99% >= 3 None None None EBS Volumes \u00b6 Tech specs Backups: Periodic EBS snapshots with retention policy Encryption: Yes (by default) Type: SSD (gp2) by default, Throughput Optimized HDD (st1) for some database workloads, if needed. Read more \u00b6 Reference links Consider the following extra links as reference: Amazon S3 FAQs Amazon S3 storage classes - Developer Guide Amazon S3 Storage Classes","title":"Storage"},{"location":"how-it-works/storage/storage/#storage","text":"We will review all S3 buckets in the existing account to determine if it\u2019s necessary to copy over to the new account, evaluate existing bucket policy and tightening permissions to be absolutely minimum required for users and applications. As for EBS volumes, our recommendation is to create all encrypted by default. Overhead created by this process is negligible.","title":"Storage"},{"location":"how-it-works/storage/storage/#s3-buckets","text":"Tech specs Encryption: Yes (by default) Object versioning: TBD per bucket Access logs enabled: TBD per bucket MFA delete: Yes on critical buckets Replication to another region: TBD per bucket Storage class Designed for Durability (designed for) Availability (designed for) Availability Zones Min storage duration Min billable object size Other considerations S3 Standard Frequently accessed data 99.999999999% 99.99% >= 3 None None None S3 Standard-IA Long-lived, infrequently accessed data 99.999999999% 99.9% >= 3 30 days 128 KB Per GB retrieval fees apply. S3 Intelligent-Tiering Long-lived data with changing or unknown access patterns 99.999999999% 99.9% >= 3 30 days None Monitoring and automation fees per object apply. No retrieval fees. S3 One Zone-IA Long-lived, infrequently accessed, non-critical data 99.999999999% 99.5% 1 30 days 128 KB Per GB retrieval fees apply. Not resilient to the loss of the Availability Zone. S3 Glacier Long-term data archiving with retrieval times ranging from minutes to hours 99.999999999% 99.99% (after you restore objects) >= 3 90 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For more information, see Restoring archived objects. S3 Glacier Deep Archive Archiving rarely accessed data with a default retrieval time of 12 hours 99.999999999% 99.99% (after you restore objects) >= 3 180 days 40 KB Per GB retrieval fees apply. You must first restore archived objects before you can access them. For more information, see Restoring archived objects. RRS (Not recommended) Frequently accessed, non-critical data 99.99% 99.99% >= 3 None None None","title":"S3 buckets"},{"location":"how-it-works/storage/storage/#ebs-volumes","text":"Tech specs Backups: Periodic EBS snapshots with retention policy Encryption: Yes (by default) Type: SSD (gp2) by default, Throughput Optimized HDD (st1) for some database workloads, if needed.","title":"EBS Volumes"},{"location":"how-it-works/storage/storage/#read-more","text":"Reference links Consider the following extra links as reference: Amazon S3 FAQs Amazon S3 storage classes - Developer Guide Amazon S3 Storage Classes","title":"Read more"},{"location":"how-it-works/tools/tools/","text":"Infrastructure Instances Tools \u00b6 Overview \u00b6 Apart from the EC2 instances that are part of Kubernetes, there are going to be other instances running tools for monitoring, logging centralization, builds/tests, deployment, among others. that are to be defined at this point. Some of them can be replaced by managed services, like: CircleCI, Snyk, etc, and this can have cons and pros that will need to be considered at the time of implementation. Any OS that is provisioned will be completely reproducible as code, in the event of migration to another vendor. Other settings for all EC2 instances Ubuntu 18.04 based (Latest AMI) EBS volumes encrypted: Yes EBS volume type: gp2 (SSD) Termination protection: Yes Infrastructure EC2 instances VPN Server Pritunl (https://vpn.domain.com) Monitoring & Alerting Prometheus (https://prometheus.domain.com) Grafana (https://grafana.domain.com) Centralized Logs Elasticsearch + Kibana (https://kibana.domain.com) CI/CD Jenkins (https://jenkins.domain.com) Spinnaker (https://spinnaker.domain.com) Droneci (https://droneci.domain.com) Webhook (https://webhook.domain.com) Secret Mgmt Hashicorp Vault (https://vault.domain.com)","title":"Tools"},{"location":"how-it-works/tools/tools/#infrastructure-instances-tools","text":"","title":"Infrastructure Instances Tools"},{"location":"how-it-works/tools/tools/#overview","text":"Apart from the EC2 instances that are part of Kubernetes, there are going to be other instances running tools for monitoring, logging centralization, builds/tests, deployment, among others. that are to be defined at this point. Some of them can be replaced by managed services, like: CircleCI, Snyk, etc, and this can have cons and pros that will need to be considered at the time of implementation. Any OS that is provisioned will be completely reproducible as code, in the event of migration to another vendor. Other settings for all EC2 instances Ubuntu 18.04 based (Latest AMI) EBS volumes encrypted: Yes EBS volume type: gp2 (SSD) Termination protection: Yes Infrastructure EC2 instances VPN Server Pritunl (https://vpn.domain.com) Monitoring & Alerting Prometheus (https://prometheus.domain.com) Grafana (https://grafana.domain.com) Centralized Logs Elasticsearch + Kibana (https://kibana.domain.com) CI/CD Jenkins (https://jenkins.domain.com) Spinnaker (https://spinnaker.domain.com) Droneci (https://droneci.domain.com) Webhook (https://webhook.domain.com) Secret Mgmt Hashicorp Vault (https://vault.domain.com)","title":"Overview"},{"location":"user-guide/","text":"Overview \u00b6 Please start by reviewing the pre-requisites Configurations \u00b6 le-tf-infra-aws le-ansible-infra Workflow \u00b6 le-tf-infra-aws le-ansible-infra","title":"Basic usage"},{"location":"user-guide/#overview","text":"Please start by reviewing the pre-requisites","title":"Overview"},{"location":"user-guide/#configurations","text":"le-tf-infra-aws le-ansible-infra","title":"Configurations"},{"location":"user-guide/#workflow","text":"le-tf-infra-aws le-ansible-infra","title":"Workflow"},{"location":"user-guide/base-configuration/overview/","text":"Overview \u00b6 Pre-requisites \u00b6 Local env pre-requeried packages GNU Make >= 4.1 (check via make --version ) Docker engine >= 19.03.12 (check via docker --version ) Python >= 3.8 (check via python3 --version || python3.8 --version ) jq >= jq-1.5-1-a5b5cbe (check via jq --version ) aws-cli >= 1.16.265 (check via aws --version ) Configurations \u00b6 Specific configuration per component could be found in the immediately following entries config | le-tf-infra-aws config | le-ansible-infra","title":"Overview"},{"location":"user-guide/base-configuration/overview/#overview","text":"","title":"Overview"},{"location":"user-guide/base-configuration/overview/#pre-requisites","text":"Local env pre-requeried packages GNU Make >= 4.1 (check via make --version ) Docker engine >= 19.03.12 (check via docker --version ) Python >= 3.8 (check via python3 --version || python3.8 --version ) jq >= jq-1.5-1-a5b5cbe (check via jq --version ) aws-cli >= 1.16.265 (check via aws --version )","title":"Pre-requisites"},{"location":"user-guide/base-configuration/overview/#configurations","text":"Specific configuration per component could be found in the immediately following entries config | le-tf-infra-aws config | le-ansible-infra","title":"Configurations"},{"location":"user-guide/base-configuration/repo-le-ansible-infra/","text":"Configuration: Ansible Playbooks \u00b6 Overview \u00b6 This repository contains all the Ansible Playbooks configuration files used to create Binbash Leverage Reference AWS Cloud Solutions Architecture. Ansible Playbook Documentation \u00b6 Check out the README.md under contained under each repo Playbooks Documentation User Management & Security sec-users VPN Server vpn-pritunl Monitoring & Alerting prometheus-grfana Centralized Logs eskibana CI/CD jenkins spinnaker droneci webhook Secret Mgmt hashicorp-vault","title":"Ansible Infra"},{"location":"user-guide/base-configuration/repo-le-ansible-infra/#configuration-ansible-playbooks","text":"","title":"Configuration: Ansible Playbooks"},{"location":"user-guide/base-configuration/repo-le-ansible-infra/#overview","text":"This repository contains all the Ansible Playbooks configuration files used to create Binbash Leverage Reference AWS Cloud Solutions Architecture.","title":"Overview"},{"location":"user-guide/base-configuration/repo-le-ansible-infra/#ansible-playbook-documentation","text":"Check out the README.md under contained under each repo Playbooks Documentation User Management & Security sec-users VPN Server vpn-pritunl Monitoring & Alerting prometheus-grfana Centralized Logs eskibana CI/CD jenkins spinnaker droneci webhook Secret Mgmt hashicorp-vault","title":"Ansible Playbook Documentation"},{"location":"user-guide/base-configuration/repo-le-tf-infra-aws/","text":"Files/Folders Organization \u00b6 The following block provides a brief explanation of the chosen files/folders layout: + apps-devstg/ (resources for Apps dev & stg account) ... + apps-prd/ (resources for Apps Prod account) ... + root-org/ (resources for the root-org account) ... + security/ (resources for the security + users account) ... + shared/ (resources for the shared account) ... Configuration files are organized by environments (e.g. dev, stg) and service type (identities, sec, network, etc) to keep any changes made to them separate. Within each of those folders you should find the Terraform files that are used to define all the resources that belong to such environment. figure 1: AWS Organization Architecture Diagram (just as reference). Under every account folder you will see a service layer structure similar to the following: . \u251c\u2500\u2500 apps-devstg \u2502 \u251c\u2500\u2500 backups -- \u2502 \u251c\u2500\u2500 base-identities \u2502 \u251c\u2500\u2500 base-network \u2502 \u251c\u2500\u2500 base-tf-backend \u2502 \u251c\u2500\u2500 cdn-s3-frontend \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 databases-mysql -- \u2502 \u251c\u2500\u2500 databases-pgsql -- \u2502 \u251c\u2500\u2500 ec2-fleet-ansible -- \u2502 \u251c\u2500\u2500 k8s-eks -- \u2502 \u251c\u2500\u2500 k8s-kops -- \u2502 \u251c\u2500\u2500 notifications \u2502 \u251c\u2500\u2500 security-audit \u2502 \u251c\u2500\u2500 security-base \u2502 \u251c\u2500\u2500 security-certs \u2502 \u251c\u2500\u2500 security-compliance -- \u2502 \u251c\u2500\u2500 security-keys \u2502 \u251c\u2500\u2500 security-keys-dr \u2502 \u251c\u2500\u2500 storage \u2502 \u2514\u2500\u2500 tools-cloud-nuke \u251c\u2500\u2500 apps-prd \u2502 \u251c\u2500\u2500 backups -- \u2502 \u251c\u2500\u2500 base-identities \u2502 \u251c\u2500\u2500 base-network \u2502 \u251c\u2500\u2500 base-tf-backend \u2502 \u251c\u2500\u2500 cdn-s3-frontend \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 ec2-fleet -- \u2502 \u251c\u2500\u2500 notifications \u2502 \u251c\u2500\u2500 security-audit \u2502 \u251c\u2500\u2500 security-base \u2502 \u251c\u2500\u2500 security-certs \u2502 \u251c\u2500\u2500 security-compliance -- \u2502 \u2514\u2500\u2500 security-keys \u251c\u2500\u2500 @bin \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 makefiles \u2502 \u2514\u2500\u2500 scripts \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 common.config \u251c\u2500\u2500 _config.yml \u251c\u2500\u2500 @doc \u2502 \u2514\u2500\u2500 figures \u251c\u2500\u2500 LICENSE.md \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 root \u2502 \u251c\u2500\u2500 base-identities \u2502 \u251c\u2500\u2500 base-tf-backend \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 cost-mgmt \u2502 \u251c\u2500\u2500 notifications \u2502 \u251c\u2500\u2500 organizations \u2502 \u251c\u2500\u2500 security-audit \u2502 \u251c\u2500\u2500 security-base \u2502 \u251c\u2500\u2500 security-compliance -- \u2502 \u251c\u2500\u2500 security-keys \u2502 \u251c\u2500\u2500 security-monitoring \u2502 \u2514\u2500\u2500 security-monitoring-dr -- \u251c\u2500\u2500 security \u2502 \u251c\u2500\u2500 base-identities \u2502 \u251c\u2500\u2500 base-tf-backend \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 notifications \u2502 \u251c\u2500\u2500 security-audit \u2502 \u251c\u2500\u2500 security-base \u2502 \u251c\u2500\u2500 security-compliance -- \u2502 \u251c\u2500\u2500 security-keys \u2502 \u251c\u2500\u2500 security-monitoring \u2502 \u2514\u2500\u2500 security-monitoring-dr -- \u2514\u2500\u2500 shared \u251c\u2500\u2500 base-dns \u251c\u2500\u2500 base-identities \u251c\u2500\u2500 base-network \u251c\u2500\u2500 base-tf-backend \u251c\u2500\u2500 config \u251c\u2500\u2500 container-registry \u251c\u2500\u2500 ec2-fleet -- \u251c\u2500\u2500 infra_prometheus \u251c\u2500\u2500 notifications \u251c\u2500\u2500 security-audit \u251c\u2500\u2500 security-base \u251c\u2500\u2500 security-compliance -- \u251c\u2500\u2500 security-keys \u251c\u2500\u2500 storage \u251c\u2500\u2500 tools-cloud-scheduler-stop-start \u251c\u2500\u2500 tools-eskibana -- \u251c\u2500\u2500 tools-jenkins -- \u2514\u2500\u2500 tools-vpn-server NOTE: As a convention folders with the -- suffix reflect that the resources are not currently created in AWS, basically they've been destroyed or not yet exist. Such separation is meant to avoid situations in which a single folder contains a lot of resources. That is important to avoid because at some point, running terraform plan or apply stats taking too long and that becomes a problem. This organization also provides a layout that is easier to navigate and discover. You simply start with the accounts at the top level and then you get to explore the resource categories within each account. Pre-requisites \u00b6 Makefile \u00b6 We rely on Makefiles as a wrapper to run terraform commands that consistently use the same config files. You are encouraged to inspect those Makefiles to understand what's going on. Terraform \u00b6 Makefiles already grant the recs via Dockerized Terraform cmds Remote State \u00b6 In the tf-backend folder you should find all setup scripts or configuration files that need to be run before you can get to work with anything else. IMPORTANT: THIS IS ONLY NEEDED IF THE BACKEND WAS NOT CREATED YET. IF THE BACKEND ALREADY EXISTS YOU JUST USE IT. Read More Terraform - S3 & DynamoDB for Remote State Storage & Locking Configuration \u00b6 Config files can be found under each config folders Global config file /config/common.config contains global context TF variables that we inject to TF commands which are used by all sub-directories such as make plan or make apply and which cannot be stored in backend.config due to TF. Account config files backend.config contains TF variables that are mainly used to configure TF backend but since profile and region are defined there, we also use them to inject those values into other TF commands. account.config contains TF variables that are specific to an AWS account. Makefile config file /@bin/config/base.mk contains global makefile-lib variables AWS Profile \u00b6 File backend.config will inject the profile name that TF will use to make changes on AWS. Such profile is usually one that relies on another profile to assume a role to get access to each corresponding account. Please follow to correctly setup your AWS Credentials user-guide/identities user-guide/identities/credentials Read the following page leverage doc to understand how to set up a profile to assume a role","title":"Terraform Infra"},{"location":"user-guide/base-configuration/repo-le-tf-infra-aws/#filesfolders-organization","text":"The following block provides a brief explanation of the chosen files/folders layout: + apps-devstg/ (resources for Apps dev & stg account) ... + apps-prd/ (resources for Apps Prod account) ... + root-org/ (resources for the root-org account) ... + security/ (resources for the security + users account) ... + shared/ (resources for the shared account) ... Configuration files are organized by environments (e.g. dev, stg) and service type (identities, sec, network, etc) to keep any changes made to them separate. Within each of those folders you should find the Terraform files that are used to define all the resources that belong to such environment. figure 1: AWS Organization Architecture Diagram (just as reference). Under every account folder you will see a service layer structure similar to the following: . \u251c\u2500\u2500 apps-devstg \u2502 \u251c\u2500\u2500 backups -- \u2502 \u251c\u2500\u2500 base-identities \u2502 \u251c\u2500\u2500 base-network \u2502 \u251c\u2500\u2500 base-tf-backend \u2502 \u251c\u2500\u2500 cdn-s3-frontend \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 databases-mysql -- \u2502 \u251c\u2500\u2500 databases-pgsql -- \u2502 \u251c\u2500\u2500 ec2-fleet-ansible -- \u2502 \u251c\u2500\u2500 k8s-eks -- \u2502 \u251c\u2500\u2500 k8s-kops -- \u2502 \u251c\u2500\u2500 notifications \u2502 \u251c\u2500\u2500 security-audit \u2502 \u251c\u2500\u2500 security-base \u2502 \u251c\u2500\u2500 security-certs \u2502 \u251c\u2500\u2500 security-compliance -- \u2502 \u251c\u2500\u2500 security-keys \u2502 \u251c\u2500\u2500 security-keys-dr \u2502 \u251c\u2500\u2500 storage \u2502 \u2514\u2500\u2500 tools-cloud-nuke \u251c\u2500\u2500 apps-prd \u2502 \u251c\u2500\u2500 backups -- \u2502 \u251c\u2500\u2500 base-identities \u2502 \u251c\u2500\u2500 base-network \u2502 \u251c\u2500\u2500 base-tf-backend \u2502 \u251c\u2500\u2500 cdn-s3-frontend \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 ec2-fleet -- \u2502 \u251c\u2500\u2500 notifications \u2502 \u251c\u2500\u2500 security-audit \u2502 \u251c\u2500\u2500 security-base \u2502 \u251c\u2500\u2500 security-certs \u2502 \u251c\u2500\u2500 security-compliance -- \u2502 \u2514\u2500\u2500 security-keys \u251c\u2500\u2500 @bin \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 makefiles \u2502 \u2514\u2500\u2500 scripts \u251c\u2500\u2500 CHANGELOG.md \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 common.config \u251c\u2500\u2500 _config.yml \u251c\u2500\u2500 @doc \u2502 \u2514\u2500\u2500 figures \u251c\u2500\u2500 LICENSE.md \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 root \u2502 \u251c\u2500\u2500 base-identities \u2502 \u251c\u2500\u2500 base-tf-backend \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 cost-mgmt \u2502 \u251c\u2500\u2500 notifications \u2502 \u251c\u2500\u2500 organizations \u2502 \u251c\u2500\u2500 security-audit \u2502 \u251c\u2500\u2500 security-base \u2502 \u251c\u2500\u2500 security-compliance -- \u2502 \u251c\u2500\u2500 security-keys \u2502 \u251c\u2500\u2500 security-monitoring \u2502 \u2514\u2500\u2500 security-monitoring-dr -- \u251c\u2500\u2500 security \u2502 \u251c\u2500\u2500 base-identities \u2502 \u251c\u2500\u2500 base-tf-backend \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 notifications \u2502 \u251c\u2500\u2500 security-audit \u2502 \u251c\u2500\u2500 security-base \u2502 \u251c\u2500\u2500 security-compliance -- \u2502 \u251c\u2500\u2500 security-keys \u2502 \u251c\u2500\u2500 security-monitoring \u2502 \u2514\u2500\u2500 security-monitoring-dr -- \u2514\u2500\u2500 shared \u251c\u2500\u2500 base-dns \u251c\u2500\u2500 base-identities \u251c\u2500\u2500 base-network \u251c\u2500\u2500 base-tf-backend \u251c\u2500\u2500 config \u251c\u2500\u2500 container-registry \u251c\u2500\u2500 ec2-fleet -- \u251c\u2500\u2500 infra_prometheus \u251c\u2500\u2500 notifications \u251c\u2500\u2500 security-audit \u251c\u2500\u2500 security-base \u251c\u2500\u2500 security-compliance -- \u251c\u2500\u2500 security-keys \u251c\u2500\u2500 storage \u251c\u2500\u2500 tools-cloud-scheduler-stop-start \u251c\u2500\u2500 tools-eskibana -- \u251c\u2500\u2500 tools-jenkins -- \u2514\u2500\u2500 tools-vpn-server NOTE: As a convention folders with the -- suffix reflect that the resources are not currently created in AWS, basically they've been destroyed or not yet exist. Such separation is meant to avoid situations in which a single folder contains a lot of resources. That is important to avoid because at some point, running terraform plan or apply stats taking too long and that becomes a problem. This organization also provides a layout that is easier to navigate and discover. You simply start with the accounts at the top level and then you get to explore the resource categories within each account.","title":"Files/Folders Organization"},{"location":"user-guide/base-configuration/repo-le-tf-infra-aws/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"user-guide/base-configuration/repo-le-tf-infra-aws/#makefile","text":"We rely on Makefiles as a wrapper to run terraform commands that consistently use the same config files. You are encouraged to inspect those Makefiles to understand what's going on.","title":"Makefile"},{"location":"user-guide/base-configuration/repo-le-tf-infra-aws/#terraform","text":"Makefiles already grant the recs via Dockerized Terraform cmds","title":"Terraform"},{"location":"user-guide/base-configuration/repo-le-tf-infra-aws/#remote-state","text":"In the tf-backend folder you should find all setup scripts or configuration files that need to be run before you can get to work with anything else. IMPORTANT: THIS IS ONLY NEEDED IF THE BACKEND WAS NOT CREATED YET. IF THE BACKEND ALREADY EXISTS YOU JUST USE IT. Read More Terraform - S3 & DynamoDB for Remote State Storage & Locking","title":"Remote State"},{"location":"user-guide/base-configuration/repo-le-tf-infra-aws/#configuration","text":"Config files can be found under each config folders Global config file /config/common.config contains global context TF variables that we inject to TF commands which are used by all sub-directories such as make plan or make apply and which cannot be stored in backend.config due to TF. Account config files backend.config contains TF variables that are mainly used to configure TF backend but since profile and region are defined there, we also use them to inject those values into other TF commands. account.config contains TF variables that are specific to an AWS account. Makefile config file /@bin/config/base.mk contains global makefile-lib variables","title":"Configuration"},{"location":"user-guide/base-configuration/repo-le-tf-infra-aws/#aws-profile","text":"File backend.config will inject the profile name that TF will use to make changes on AWS. Such profile is usually one that relies on another profile to assume a role to get access to each corresponding account. Please follow to correctly setup your AWS Credentials user-guide/identities user-guide/identities/credentials Read the following page leverage doc to understand how to set up a profile to assume a role","title":"AWS Profile"},{"location":"user-guide/base-workflow/repo-le-ansible-infra/","text":"Workflow \u00b6 Makefile We rely on Makefiles as a wrapper to run terraform commands that consistently use the same config files. You are encouraged to inspect those Makefiles to understand what's going on. Ansible Infra Get into the folder that you need to work with (e.g. ansible-playbook-vpn-pritunl ) Run make init to get all the necessary Ansible roles based on each requirements.yml Run init-ansible-py (if necessary) Make whatever changes you need to make as stated in each Playbook Documentation (check Documentation section above) Run make check if you only mean to preview those changes Run make apply if you want to apply those changes","title":"Ansible Infra"},{"location":"user-guide/base-workflow/repo-le-ansible-infra/#workflow","text":"Makefile We rely on Makefiles as a wrapper to run terraform commands that consistently use the same config files. You are encouraged to inspect those Makefiles to understand what's going on. Ansible Infra Get into the folder that you need to work with (e.g. ansible-playbook-vpn-pritunl ) Run make init to get all the necessary Ansible roles based on each requirements.yml Run init-ansible-py (if necessary) Make whatever changes you need to make as stated in each Playbook Documentation (check Documentation section above) Run make check if you only mean to preview those changes Run make apply if you want to apply those changes","title":"Workflow"},{"location":"user-guide/base-workflow/repo-le-dev-makefiles/","text":"Makefiles used to operate Binbash Leverage repositories. \u00b6 Overview \u00b6 In order to get the full automated potential of the Binbash Leverage DevOps Automation Code Library you should initialize all the necessary helper Makefiles . How? For all supported modules and infra components you must execute the make init-makefiles command at the root context \u256d\u2500delivery at delivery-I7567 in ~/terraform/terraform-aws-backup-by-tags on master\u2714 20 -09-17 \u2570\u2500\u2820\u2835 make Available Commands: - init-makefiles initialize makefiles Why? You'll get all the necessary commands to automatically operate this module via a dockerized approach, example shown below for the different tech stack components Terraform \u00b6 Modules \u256d\u2500delivery at delivery-I7567 in ~/terraform/terraform-aws-backup-by-tags on master\u2714 20 -09-17 \u2570\u2500\u2820\u2835 make Available Commands: - circleci-validate-config ## Validate A CircleCI Config (https - format-check ## The terraform fmt is used to rewrite tf conf files to a canonical format and style. - format ## The terraform fmt is used to rewrite tf conf files to a canonical format and style. - tf-dir-chmod ## run chown in ./.terraform to gran that the docker mounted dir has the right permissions - version ## Show terraform version - init-makefiles ## initialize makefiles \u256d\u2500delivery at delivery-I7567 in ~/terraform/terraform-aws-backup-by-tags on master\u2714 20 -09-17 \u2570\u2500\u2820\u2835 make format-check docker run --rm -v /home/delivery/Binbash/repos/Leverage/terraform/terraform-aws-backup-by-tags: \"/go/src/project/\" :rw -v :/config -v /common.config:/common-config/common.config -v ~/.ssh:/root/.ssh -v ~/.gitconfig:/etc/gitconfig -v ~/.aws/bb:/root/.aws/bb -e AWS_SHARED_CREDENTIALS_FILE = /root/.aws/bb/credentials -e AWS_CONFIG_FILE = /root/.aws/bb/config --entrypoint = /bin/terraform -w \"/go/src/project/\" -it binbash/terraform-awscli-slim:0.12.28 fmt -check Infra \u256d\u2500delivery at delivery-ops in ~/le-tf-infra-aws/apps-devstg/base-network on master\u2714 2020 -10-29 \u2570\u2500\u2820\u2835 make Available Commands: - apply apply-cmd tf-dir-chmod ## Make terraform apply any changes with dockerized binary - cost-estimate-plan ## Terraform plan output compatible with https - cost-estimate-state ## Terraform state output compatible with https - decrypt ## Decrypt secrets.tf via ansible-vault - destroy ## Destroy all resources managed by terraform - encrypt ## Encrypt secrets.dec.tf via ansible-vault - force-unlock ## Manually unlock the terraform state, eg - format-check ## The terraform fmt is used to rewrite tf conf files to a canonical format and style. - format ## The terraform fmt is used to rewrite tf conf files to a canonical format and style. - init init-cmd tf-dir-chmod ## Initialize terraform backend, plugins, and modules - init-reconfigure init-reconfigure-cmd tf-dir-chmod ## Initialize and reconfigure terraform backend, plugins, and modules - output ## Terraform output command is used to extract the value of an output variable from the state file. - plan-detailed ## Preview terraform changes with a more detailed output - plan ## Preview terraform changes - shell ## Initialize terraform backend, plugins, and modules - tf-dir-chmod ## run chown in ./.terraform to gran that the docker mounted dir has the right permissions - tflint-deep ## TFLint is a Terraform linter for detecting errors that can not be detected by terraform plan (tf0.12 > 0.10.x). - tflint ## TFLint is a Terraform linter for detecting errors that can not be detected by terraform plan (tf0.12 > 0.10.x). - validate-tf-layout ## Validate Terraform layout to make sure it's set up properly - version ## Show terraform version \u256d\u2500delivery at delivery-ops in ~/le-tf-infra-aws/apps-devstg/base-network on master\u2714 2020 -10-29 \u2570\u2500\u2820\u2835 make init docker run --rm -v ~/le-tf-infra-aws/apps-devstg/base-network: \"/go/src/project/\" :rw \\ -v ~/le-tf-infra-aws/apps-devstg/config:/config \\ -v ~/le-tf-infra-aws/config/common.config:/common-config/common.config \\ -v ~/.ssh:/root/.ssh -v ~/.gitconfig:/etc/gitconfig \\ -v ~/.aws/bb:/root/.aws/bb \\ -e AWS_SHARED_CREDENTIALS_FILE = /root/.aws/bb/credentials \\ -e AWS_CONFIG_FILE = /root/.aws/bb/config \\ --entrypoint = /bin/terraform \\ -w \"/go/src/project/\" \\ -it binbash/terraform-awscli-slim:0.13.2 init \\ -backend-config = /config/backend.config Initializing modules... Initializing the backend... Initializing provider plugins... - terraform.io/builtin/terraform is built in to Terraform - Using previously-installed hashicorp/aws v3.9.0 Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. LOCAL_OS_USER_ID: 1000 LOCAL_OS_GROUP_ID: 1000 sudo chown -R 1000 :1000 ./.terraform Ansible \u00b6 Roles \u256d\u2500delivery at delivery-ops \u2570\u2500\u2820\u2835 make Available Commands: - ansible-galaxy-import-role ## Run playbook tests w/ molecule using the local code - init ## Install required ansible roles - test-ansible-lint ## Ansible lint - test-molecule-galaxy ## Run playbook tests w/ molecule pulling role from ansible galaxy - test-molecule-local ## Run playbook tests w/ molecule using the local code - circleci-validate-config ## Validate A CircleCI Config (https - changelog-init ## git-chglog (https - changelog-major ## git-chglog generation for major release - changelog-minor ## git-chglog generation for minor release - changelog-patch ## git-chglog generation for path release - release-major ## releasing major (eg - release-major-with-changelog-circleci ## make changelog-major && git add && git commit && make release-major - release-major-with-changelog ## make changelog-major && git add && git commit && make release-major - release-minor ## releasing minor (eg - release-minor-with-changelog-circleci ## make changelog-minor && git add && git commit && make release-minor - release-minor-with-changelog ## make changelog-minor && git add && git commit && make release-minor - release-patch ## releasing patch (eg - release-patch-with-changelog-circleci ## make changelog-patch && git add && git commit && make release-patch - release-patch-with-changelog ## make changelog-patch && git add && git commit && make release-patch - init-makefiles ## initialize makefiles \u256d\u2500delivery at delivery-ops \u2570\u2500\u2820\u2835 make test-molecule-local ... ------------------------------- TESTING MODULE ON: ubuntu1804 ------------------------------- Using default tag: latest latest: Pulling from geerlingguy/docker-ubuntu1804-ansible Digest: sha256:1b47cbb66e819170fd3afee98db55176bc13cd12fabdbcf0183aff2582dc0254 Status: Image is up to date for geerlingguy/docker-ubuntu1804-ansible:latest docker.io/geerlingguy/docker-ubuntu1804-ansible:latest ## Starting testing stages ## --> Test matrix \u2514\u2500\u2500 default \u251c\u2500\u2500 dependency \u251c\u2500\u2500 lint \u251c\u2500\u2500 cleanup \u251c\u2500\u2500 destroy \u251c\u2500\u2500 syntax \u251c\u2500\u2500 create \u251c\u2500\u2500 prepare \u251c\u2500\u2500 converge \u251c\u2500\u2500 idempotence \u251c\u2500\u2500 side_effect \u251c\u2500\u2500 verify \u251c\u2500\u2500 cleanup \u2514\u2500\u2500 destroy --> Scenario: 'default' ... PLAY [ Destroy ] ***************************************************************** TASK [ Destroy molecule instance ( s )] ******************************************** changed: [ localhost ] = > ( item = instance ) TASK [ Wait for instance ( s ) deletion to complete ] ******************************* FAILED - RETRYING: Wait for instance ( s ) deletion to complete ( 300 retries left ) . changed: [ localhost ] = > ( item = None ) changed: [ localhost ] TASK [ Delete docker network ( s )] ************************************************ PLAY RECAP ********************************************************************* localhost : ok = 2 changed = 2 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 --> Pruning extra files from scenario ephemeral directory ------------------------------- DONE ------------------------------- TESTING MODULE ON: ubuntu1604 ------------------------------- Infra \u256d\u2500delivery at delivery-ops in ~/le-ansible-infra/vpn-pritunl on master\u2714 20 -10-21 \u2570\u2500\u2820\u2835 make Available Commands: - apply ## run ansible-playbook - check ## run ansible-playbook in Check Mode (\u201cDry Run\u201d) - decrypt ## Decrypt secrets.tf via ansible-vault - decrypt-string ## Decrypt encrypted string via ansible-vault - e.g. make ARG=\"your_encrypted_srting\" decrypt-string - encrypt ## Encrypt secrets.dec.tf via ansible-vault - init-ansible-py ## Install required ansible version - init ## Install required ansible roles - apply-users ## Run sec-users playbook on this host \u256d\u2500delivery at delivery-ops in ~/le-ansible-infra/vpn-pritunl on master\u2714 20 -10-21 \u2570\u2500\u2820\u2835 make check ansible-playbook setup.yml --check PLAY [ Provision OpenVPN Pritunl instance ] **************************************************************************************************************************************************** TASK [ Gathering Facts ] *********************************************************************************************************************************************************************** ok: [ pritunl_private ] TASK [ Check ansible version ] ***************************************************************************************************************************************************************** ok: [ pritunl_private ] = > { \"changed\" : false, \"msg\" : \"All assertions passed\" } TASK [ binbash_inc.ansible_role_common : Setup your server hostname ] ************************************************************************************************************************** ok: [ pritunl_private ] ... Helm \u00b6 Charts TODO Infra TODO","title":"Makefiles Lib"},{"location":"user-guide/base-workflow/repo-le-dev-makefiles/#makefiles-used-to-operate-binbash-leverage-repositories","text":"","title":"Makefiles used to operate Binbash Leverage repositories."},{"location":"user-guide/base-workflow/repo-le-dev-makefiles/#overview","text":"In order to get the full automated potential of the Binbash Leverage DevOps Automation Code Library you should initialize all the necessary helper Makefiles . How? For all supported modules and infra components you must execute the make init-makefiles command at the root context \u256d\u2500delivery at delivery-I7567 in ~/terraform/terraform-aws-backup-by-tags on master\u2714 20 -09-17 \u2570\u2500\u2820\u2835 make Available Commands: - init-makefiles initialize makefiles Why? You'll get all the necessary commands to automatically operate this module via a dockerized approach, example shown below for the different tech stack components","title":"Overview"},{"location":"user-guide/base-workflow/repo-le-dev-makefiles/#terraform","text":"Modules \u256d\u2500delivery at delivery-I7567 in ~/terraform/terraform-aws-backup-by-tags on master\u2714 20 -09-17 \u2570\u2500\u2820\u2835 make Available Commands: - circleci-validate-config ## Validate A CircleCI Config (https - format-check ## The terraform fmt is used to rewrite tf conf files to a canonical format and style. - format ## The terraform fmt is used to rewrite tf conf files to a canonical format and style. - tf-dir-chmod ## run chown in ./.terraform to gran that the docker mounted dir has the right permissions - version ## Show terraform version - init-makefiles ## initialize makefiles \u256d\u2500delivery at delivery-I7567 in ~/terraform/terraform-aws-backup-by-tags on master\u2714 20 -09-17 \u2570\u2500\u2820\u2835 make format-check docker run --rm -v /home/delivery/Binbash/repos/Leverage/terraform/terraform-aws-backup-by-tags: \"/go/src/project/\" :rw -v :/config -v /common.config:/common-config/common.config -v ~/.ssh:/root/.ssh -v ~/.gitconfig:/etc/gitconfig -v ~/.aws/bb:/root/.aws/bb -e AWS_SHARED_CREDENTIALS_FILE = /root/.aws/bb/credentials -e AWS_CONFIG_FILE = /root/.aws/bb/config --entrypoint = /bin/terraform -w \"/go/src/project/\" -it binbash/terraform-awscli-slim:0.12.28 fmt -check Infra \u256d\u2500delivery at delivery-ops in ~/le-tf-infra-aws/apps-devstg/base-network on master\u2714 2020 -10-29 \u2570\u2500\u2820\u2835 make Available Commands: - apply apply-cmd tf-dir-chmod ## Make terraform apply any changes with dockerized binary - cost-estimate-plan ## Terraform plan output compatible with https - cost-estimate-state ## Terraform state output compatible with https - decrypt ## Decrypt secrets.tf via ansible-vault - destroy ## Destroy all resources managed by terraform - encrypt ## Encrypt secrets.dec.tf via ansible-vault - force-unlock ## Manually unlock the terraform state, eg - format-check ## The terraform fmt is used to rewrite tf conf files to a canonical format and style. - format ## The terraform fmt is used to rewrite tf conf files to a canonical format and style. - init init-cmd tf-dir-chmod ## Initialize terraform backend, plugins, and modules - init-reconfigure init-reconfigure-cmd tf-dir-chmod ## Initialize and reconfigure terraform backend, plugins, and modules - output ## Terraform output command is used to extract the value of an output variable from the state file. - plan-detailed ## Preview terraform changes with a more detailed output - plan ## Preview terraform changes - shell ## Initialize terraform backend, plugins, and modules - tf-dir-chmod ## run chown in ./.terraform to gran that the docker mounted dir has the right permissions - tflint-deep ## TFLint is a Terraform linter for detecting errors that can not be detected by terraform plan (tf0.12 > 0.10.x). - tflint ## TFLint is a Terraform linter for detecting errors that can not be detected by terraform plan (tf0.12 > 0.10.x). - validate-tf-layout ## Validate Terraform layout to make sure it's set up properly - version ## Show terraform version \u256d\u2500delivery at delivery-ops in ~/le-tf-infra-aws/apps-devstg/base-network on master\u2714 2020 -10-29 \u2570\u2500\u2820\u2835 make init docker run --rm -v ~/le-tf-infra-aws/apps-devstg/base-network: \"/go/src/project/\" :rw \\ -v ~/le-tf-infra-aws/apps-devstg/config:/config \\ -v ~/le-tf-infra-aws/config/common.config:/common-config/common.config \\ -v ~/.ssh:/root/.ssh -v ~/.gitconfig:/etc/gitconfig \\ -v ~/.aws/bb:/root/.aws/bb \\ -e AWS_SHARED_CREDENTIALS_FILE = /root/.aws/bb/credentials \\ -e AWS_CONFIG_FILE = /root/.aws/bb/config \\ --entrypoint = /bin/terraform \\ -w \"/go/src/project/\" \\ -it binbash/terraform-awscli-slim:0.13.2 init \\ -backend-config = /config/backend.config Initializing modules... Initializing the backend... Initializing provider plugins... - terraform.io/builtin/terraform is built in to Terraform - Using previously-installed hashicorp/aws v3.9.0 Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. LOCAL_OS_USER_ID: 1000 LOCAL_OS_GROUP_ID: 1000 sudo chown -R 1000 :1000 ./.terraform","title":"Terraform"},{"location":"user-guide/base-workflow/repo-le-dev-makefiles/#ansible","text":"Roles \u256d\u2500delivery at delivery-ops \u2570\u2500\u2820\u2835 make Available Commands: - ansible-galaxy-import-role ## Run playbook tests w/ molecule using the local code - init ## Install required ansible roles - test-ansible-lint ## Ansible lint - test-molecule-galaxy ## Run playbook tests w/ molecule pulling role from ansible galaxy - test-molecule-local ## Run playbook tests w/ molecule using the local code - circleci-validate-config ## Validate A CircleCI Config (https - changelog-init ## git-chglog (https - changelog-major ## git-chglog generation for major release - changelog-minor ## git-chglog generation for minor release - changelog-patch ## git-chglog generation for path release - release-major ## releasing major (eg - release-major-with-changelog-circleci ## make changelog-major && git add && git commit && make release-major - release-major-with-changelog ## make changelog-major && git add && git commit && make release-major - release-minor ## releasing minor (eg - release-minor-with-changelog-circleci ## make changelog-minor && git add && git commit && make release-minor - release-minor-with-changelog ## make changelog-minor && git add && git commit && make release-minor - release-patch ## releasing patch (eg - release-patch-with-changelog-circleci ## make changelog-patch && git add && git commit && make release-patch - release-patch-with-changelog ## make changelog-patch && git add && git commit && make release-patch - init-makefiles ## initialize makefiles \u256d\u2500delivery at delivery-ops \u2570\u2500\u2820\u2835 make test-molecule-local ... ------------------------------- TESTING MODULE ON: ubuntu1804 ------------------------------- Using default tag: latest latest: Pulling from geerlingguy/docker-ubuntu1804-ansible Digest: sha256:1b47cbb66e819170fd3afee98db55176bc13cd12fabdbcf0183aff2582dc0254 Status: Image is up to date for geerlingguy/docker-ubuntu1804-ansible:latest docker.io/geerlingguy/docker-ubuntu1804-ansible:latest ## Starting testing stages ## --> Test matrix \u2514\u2500\u2500 default \u251c\u2500\u2500 dependency \u251c\u2500\u2500 lint \u251c\u2500\u2500 cleanup \u251c\u2500\u2500 destroy \u251c\u2500\u2500 syntax \u251c\u2500\u2500 create \u251c\u2500\u2500 prepare \u251c\u2500\u2500 converge \u251c\u2500\u2500 idempotence \u251c\u2500\u2500 side_effect \u251c\u2500\u2500 verify \u251c\u2500\u2500 cleanup \u2514\u2500\u2500 destroy --> Scenario: 'default' ... PLAY [ Destroy ] ***************************************************************** TASK [ Destroy molecule instance ( s )] ******************************************** changed: [ localhost ] = > ( item = instance ) TASK [ Wait for instance ( s ) deletion to complete ] ******************************* FAILED - RETRYING: Wait for instance ( s ) deletion to complete ( 300 retries left ) . changed: [ localhost ] = > ( item = None ) changed: [ localhost ] TASK [ Delete docker network ( s )] ************************************************ PLAY RECAP ********************************************************************* localhost : ok = 2 changed = 2 unreachable = 0 failed = 0 skipped = 1 rescued = 0 ignored = 0 --> Pruning extra files from scenario ephemeral directory ------------------------------- DONE ------------------------------- TESTING MODULE ON: ubuntu1604 ------------------------------- Infra \u256d\u2500delivery at delivery-ops in ~/le-ansible-infra/vpn-pritunl on master\u2714 20 -10-21 \u2570\u2500\u2820\u2835 make Available Commands: - apply ## run ansible-playbook - check ## run ansible-playbook in Check Mode (\u201cDry Run\u201d) - decrypt ## Decrypt secrets.tf via ansible-vault - decrypt-string ## Decrypt encrypted string via ansible-vault - e.g. make ARG=\"your_encrypted_srting\" decrypt-string - encrypt ## Encrypt secrets.dec.tf via ansible-vault - init-ansible-py ## Install required ansible version - init ## Install required ansible roles - apply-users ## Run sec-users playbook on this host \u256d\u2500delivery at delivery-ops in ~/le-ansible-infra/vpn-pritunl on master\u2714 20 -10-21 \u2570\u2500\u2820\u2835 make check ansible-playbook setup.yml --check PLAY [ Provision OpenVPN Pritunl instance ] **************************************************************************************************************************************************** TASK [ Gathering Facts ] *********************************************************************************************************************************************************************** ok: [ pritunl_private ] TASK [ Check ansible version ] ***************************************************************************************************************************************************************** ok: [ pritunl_private ] = > { \"changed\" : false, \"msg\" : \"All assertions passed\" } TASK [ binbash_inc.ansible_role_common : Setup your server hostname ] ************************************************************************************************************************** ok: [ pritunl_private ] ...","title":"Ansible"},{"location":"user-guide/base-workflow/repo-le-dev-makefiles/#helm","text":"Charts TODO Infra TODO","title":"Helm"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws-tf-state/","text":"Terraform - S3 & DynamoDB for Remote State Storage & Locking \u00b6 Overview \u00b6 Use this terraform configuration files to create the S3 bucket & DynamoDB table needed to use Terraform Remote State Storage & Locking. Figure: Terraform remote state store & locking necessary AWS S3 bucket and DynamoDB table components. (Source: Binbash Leverage, \"Terraform Module: Terraform Backend\" , Terraform modules registry, accessed December 3rd 2020). Prerequisites \u00b6 Terraform repo structure + state backend initialization Ensure you have make installed in your system Refer to Configuration Pre-requisites to understand how to set up the configuration files required for this layer. Where you must build your Terraform Reference Architecture account structure Leveraged by the DevOps Automation Code Library through the terraform-aws-tfstate-backend module /root/base-tf-backend /security/base-tf-backend /shared/base-tf-backend /apps-devstg/base-tf-backend /apps-prd/base-tf-backend Set up \u00b6 Steps to initialize your tf-backend At the root context of the Terraform Reference Architecture account structure run make init-makefiles \u256d\u2500delivery at delivery-ops in ~/Binbash/repos/Leverage/ref-architecture/le-tf-infra-aws on master\u2718\u2718\u2718 20-12-02 - 10:30:31 \u2570\u2500\u2820\u2835 make init-makefiles ... mkdir -p ./@bin/makefiles git clone https://github.com/binbashar/le-dev-makefiles.git ./@bin/makefiles -q cd ./@bin/makefiles && git checkout v0.1.7 -q At the corresponding account dir, eg: /shared/base-tf-backend then, Run make init Run make plan , review the output to understand the expected changes Run make apply , review the output once more and type yes if you are okay with that This should create a terraform.tfstate file in this directory but we don't want to push that to the repository so let's push the state to the backend we just created Open config.tf and uncomment the following lines: # backend \"s3\" { # key = \"root/tf-backend/terraform.tfstate\" # } Run make init and type yes when Terraform asks if you want to import the state to the S3 backend Done. You can remove terraform.tfstate now (and also terraform.tfstate.backup if available) Expected workflow after set up \u00b6","title":"Terraform Infra State"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws-tf-state/#terraform-s3-dynamodb-for-remote-state-storage-locking","text":"","title":"Terraform - S3 &amp; DynamoDB for Remote State Storage &amp; Locking"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws-tf-state/#overview","text":"Use this terraform configuration files to create the S3 bucket & DynamoDB table needed to use Terraform Remote State Storage & Locking. Figure: Terraform remote state store & locking necessary AWS S3 bucket and DynamoDB table components. (Source: Binbash Leverage, \"Terraform Module: Terraform Backend\" , Terraform modules registry, accessed December 3rd 2020).","title":"Overview"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws-tf-state/#prerequisites","text":"Terraform repo structure + state backend initialization Ensure you have make installed in your system Refer to Configuration Pre-requisites to understand how to set up the configuration files required for this layer. Where you must build your Terraform Reference Architecture account structure Leveraged by the DevOps Automation Code Library through the terraform-aws-tfstate-backend module /root/base-tf-backend /security/base-tf-backend /shared/base-tf-backend /apps-devstg/base-tf-backend /apps-prd/base-tf-backend","title":"Prerequisites"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws-tf-state/#set-up","text":"Steps to initialize your tf-backend At the root context of the Terraform Reference Architecture account structure run make init-makefiles \u256d\u2500delivery at delivery-ops in ~/Binbash/repos/Leverage/ref-architecture/le-tf-infra-aws on master\u2718\u2718\u2718 20-12-02 - 10:30:31 \u2570\u2500\u2820\u2835 make init-makefiles ... mkdir -p ./@bin/makefiles git clone https://github.com/binbashar/le-dev-makefiles.git ./@bin/makefiles -q cd ./@bin/makefiles && git checkout v0.1.7 -q At the corresponding account dir, eg: /shared/base-tf-backend then, Run make init Run make plan , review the output to understand the expected changes Run make apply , review the output once more and type yes if you are okay with that This should create a terraform.tfstate file in this directory but we don't want to push that to the repository so let's push the state to the backend we just created Open config.tf and uncomment the following lines: # backend \"s3\" { # key = \"root/tf-backend/terraform.tfstate\" # } Run make init and type yes when Terraform asks if you want to import the state to the S3 backend Done. You can remove terraform.tfstate now (and also terraform.tfstate.backup if available)","title":"Set up"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws-tf-state/#expected-workflow-after-set-up","text":"","title":"Expected workflow after set up"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws/","text":"Workflow \u00b6 Steps \u00b6 Terraform Workflow Make sure you've read and prepare your local development environment following the Overview base-configurations section. Review and assure you meet all the pre-requisites Remote State Configuration files AWS Profile and credentials Get into the folder that you need to work with (e.g. 2_identities ) Run make init Make whatever changes you need to make Run make plan if you only mean to preview those changes Run make apply if you want to review and likely apply those changes Note If desired at step #5 you could submit a PR, allowing you and the rest of the team to understand and review what changes would be made to your AWS Cloud Architecture components before executing make apply ( terraform apply ). This brings the huge benefit of treating changes with a GitOps oriented approach, basically as we should treat any other code & infrastructure change, and integrate it with the rest of our tools and practices like CI/CD, in Running in Automation \u00b6 Figure: Running terraform with AWS in automation (just as reference). Read More \u00b6 Extra resources Running Terraform in automation","title":"Terraform Infra"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws/#workflow","text":"","title":"Workflow"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws/#steps","text":"Terraform Workflow Make sure you've read and prepare your local development environment following the Overview base-configurations section. Review and assure you meet all the pre-requisites Remote State Configuration files AWS Profile and credentials Get into the folder that you need to work with (e.g. 2_identities ) Run make init Make whatever changes you need to make Run make plan if you only mean to preview those changes Run make apply if you want to review and likely apply those changes Note If desired at step #5 you could submit a PR, allowing you and the rest of the team to understand and review what changes would be made to your AWS Cloud Architecture components before executing make apply ( terraform apply ). This brings the huge benefit of treating changes with a GitOps oriented approach, basically as we should treat any other code & infrastructure change, and integrate it with the rest of our tools and practices like CI/CD, in","title":"Steps"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws/#running-in-automation","text":"Figure: Running terraform with AWS in automation (just as reference).","title":"Running in Automation"},{"location":"user-guide/base-workflow/repo-le-tf-infra-aws/#read-more","text":"Extra resources Running Terraform in automation","title":"Read More"},{"location":"user-guide/cdn/cdn/","text":"","title":"CDN"},{"location":"user-guide/ci-cd/ci-cd/","text":"","title":"CI/CD"},{"location":"user-guide/compute/k8s-eks/","text":"","title":"K8s EKS"},{"location":"user-guide/compute/k8s-kops/","text":"AWS Kubernetes Kops Cluster \u00b6 Kops Pre-requisites \u00b6 Important consideration K8s clusters provisioned by Kops have a number of resources that need to be available before the cluster is created. These are Kops pre-requisites and they are defined in the 1-prerequisites directory which includes all Terraform files used to create/modify these resources. The current code has been fully tested with the AWS VPC Network Module OS pre-req packages Ref Link: https://github.com/kubernetes/kops/blob/master/docs/install.md) kops >= 1.14.0 \u2570\u2500\u25cb kops version Version 1 .15.0 ( git-9992b4055 ) kubectl >= 1.14.0 \u2570\u2500\u25cb kubectl version --client + kubectl version --client Client Version: version.Info { Major: \"1\" , Minor: \"14\" , GitVersion: \"v1.14.0\" , GitCommit: \"641856db18352033a0d96dbc99153fa3b27298e5\" , GitTreeState: \"clean\" , BuildDate: \"2019-03-25T15:53:57Z\" , GoVersion: \"go1.12.1\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } terraform >= 0.12.0 \u2570\u2500\u25cb terraform version Terraform v0.12.24 NOTE1: Regarding Terraform versions please also consider https://github.com/binbashar/bb-devops-tf-aws-kops#todo NOTE2: These dependencies will me mostly covered via Makefile w/ terraform dockerized cmds (https://hub.docker.com/repository/docker/binbash/terraform-awscli) jq >= 1.5.0 \u2570\u2500\u25cb jq --version jq-1.5-1-a5b5cbe Resulting Solutions Architecture \u00b6 Figure: AWS K8s Kops architecture diagram (just as reference). Why this workflow \u00b6 The workflow follows the same approach that is used to manage other terraform resources in your AWS accounts. E.g. network, identities, and so on. So we'll use existing AWS resources to create a cluster-template.yaml containing all the resource IDs that Kops needs to create a Kubernetes cluster. Why not directly use Kops CLI to create the K8s cluster as well as the VPC and its other dependencies? While this is a valid approach, we want to manage all these building blocks independently and be able to fully customize any AWS component without having to alter our Kubernetes cluster definitions and vice-versa. This is a fully declarative coding style approach to manage your infrastructure so being able to declare the state of our cluster in YAML files fits 100% as code & GitOps based approach. Figure: [Workflow diagram](https://medium.com/bench-engineering/deploying-kubernetes-clusters-with-kops-and-terraform-832b89250e8e). Kops Cluster Management \u00b6 The 2-kops directory includes helper scripts and Terraform files in order to template our Kubernetes cluster definition. The idea is to use our Terraform outputs from 1-prerequisites to construct a cluster definition. Overview \u00b6 Cluster Management via Kops is typically carried out through the kops CLI. In this case, we use a 2-kops directory that contains a Makefile, Terraform files and other helper scripts that reinforce the workflow we use to create/update/delete the cluster. Workflow \u00b6 This workflow is a little different to the typical Terraform workflows we use. The full workflow goes as follows: Cluster: Creation & Update Modify files under 1-prerequisites Main files to update probably are locals.tf and outputs.tf Mostly before the cluster is created but could be needed afterward Modify cluster-template.yml under 2-kops folder E.g. to add or remove instance groups, upgrade k8s version, etc At 2-kops/ context run make cluster-update will follow the steps below Get Terraform outputs from 1-prerequisites Generate a Kops cluster manifest -- it uses cluster-template.yml as a template and the outputs from the point above as replacement values Update Kops state -- it uses the generated Kops cluster manifest in previous point ( cluster.yml ) Generate Kops Terraform file ( kubernetes.tf ) -- this file represents the changes that Kops needs to apply on the cloud provider. Run make plan To preview any infrastructure changes that Terraform will make. If desired we could submit a PR, allowing you and the rest of the team to understand and review what changes would be made to the Kubernetes cluster before excecuting make apply ( terraform apply ). This brings the huge benefit of treating changes to our Kubernetes clusters with a GitOps oriented approach, basically like we treat any other code & infrastructure change, and integrate it with the rest of our tools and practices like CI/CD, integration testing, replicate environments and so on. Run make apply To apply those infrastructure changes on AWS. Run make cluster-rolling-update To determine if Kops needs to trigger some changes to happen right now (dry run) These are usually changes to the EC2 instances that won't get reflected as they depend on the autoscaling Run make cluster-rolling-update-yes To actually make any changes to the cluster masters/nodes happen Cluster: Deletion To clean-up any resources created for your K8s cluster, you should run: At 2-kops folder context run make destroy This will excecute a terraform destroy of all the kubernets.tf declared AWS resources. At 2-kops folder context run cluster-destroy Will run Kops destroy cluster -- only dry run, no changes will be applied Exec cluster-destroy-yes Kops will effectively destroy all the remaining cluster resources. Finally if at 1-prerequisites exec make destroy This will remove Kops state S3 bucket + any other extra resources you've provsioned for your cluster. Tipical Workflow \u00b6 The workflow may look complicated at first but generally it boils down to these simplified steps: 1. Modify cluster-template.yml 2. Run make cluste-update 3. Run make apply 4. Run make cluster-rolling-update-yes What about persistent and stateful K8s resources? This approach will work better the more stateless your Kubernetes workloads are. Treating Kubernetes clusters as ephemeral and replaceable infrastructure requires to consider not to use persistent volumes or the drawback of difficulties when running workloads such as databases on K8s. We feel pretty confident that we can recreate our workloads by applying each of our service definitions, charts and manifests to a given Kubernetes cluster as long as we keep the persistent storage separately on AWS RDS, DynamoDB, EFS and so on. In terms of the etcd state persistency, Kops already provisions the etcd volumes (AWS EBS) independently to the master instances they get attached to. This helps to persist the etcd state after rolling update your master nodes without any user intervention. Moreover simplifying volume backups via EBS Snapshots (consider https://github.com/binbashar/terraform-aws-backup-by-tags). We also use a very valuable backup tool named Velero (formerly Heptio Ark - https://github.com/vmware-tanzu/velero) to o back up and restore our Kubernetes cluster resources and persistent volumes. TODO IMPORTANT: Kops terraform output ( kops update cluster --target terraform ) is still generated for Terraform 0.11.x (https://github.com/kubernetes/kops/issues/7052) we'll take care of the migration when tf-0.12 gets fully supported. Create a Binbash Leverage public Confluence Wiki entry detailing some more info about etcd, calico and k8s versions compatibilities","title":"K8s Kops"},{"location":"user-guide/compute/k8s-kops/#aws-kubernetes-kops-cluster","text":"","title":"AWS Kubernetes Kops Cluster"},{"location":"user-guide/compute/k8s-kops/#kops-pre-requisites","text":"Important consideration K8s clusters provisioned by Kops have a number of resources that need to be available before the cluster is created. These are Kops pre-requisites and they are defined in the 1-prerequisites directory which includes all Terraform files used to create/modify these resources. The current code has been fully tested with the AWS VPC Network Module OS pre-req packages Ref Link: https://github.com/kubernetes/kops/blob/master/docs/install.md) kops >= 1.14.0 \u2570\u2500\u25cb kops version Version 1 .15.0 ( git-9992b4055 ) kubectl >= 1.14.0 \u2570\u2500\u25cb kubectl version --client + kubectl version --client Client Version: version.Info { Major: \"1\" , Minor: \"14\" , GitVersion: \"v1.14.0\" , GitCommit: \"641856db18352033a0d96dbc99153fa3b27298e5\" , GitTreeState: \"clean\" , BuildDate: \"2019-03-25T15:53:57Z\" , GoVersion: \"go1.12.1\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } terraform >= 0.12.0 \u2570\u2500\u25cb terraform version Terraform v0.12.24 NOTE1: Regarding Terraform versions please also consider https://github.com/binbashar/bb-devops-tf-aws-kops#todo NOTE2: These dependencies will me mostly covered via Makefile w/ terraform dockerized cmds (https://hub.docker.com/repository/docker/binbash/terraform-awscli) jq >= 1.5.0 \u2570\u2500\u25cb jq --version jq-1.5-1-a5b5cbe","title":"Kops Pre-requisites"},{"location":"user-guide/compute/k8s-kops/#resulting-solutions-architecture","text":"Figure: AWS K8s Kops architecture diagram (just as reference).","title":"Resulting Solutions Architecture"},{"location":"user-guide/compute/k8s-kops/#why-this-workflow","text":"The workflow follows the same approach that is used to manage other terraform resources in your AWS accounts. E.g. network, identities, and so on. So we'll use existing AWS resources to create a cluster-template.yaml containing all the resource IDs that Kops needs to create a Kubernetes cluster. Why not directly use Kops CLI to create the K8s cluster as well as the VPC and its other dependencies? While this is a valid approach, we want to manage all these building blocks independently and be able to fully customize any AWS component without having to alter our Kubernetes cluster definitions and vice-versa. This is a fully declarative coding style approach to manage your infrastructure so being able to declare the state of our cluster in YAML files fits 100% as code & GitOps based approach. Figure: [Workflow diagram](https://medium.com/bench-engineering/deploying-kubernetes-clusters-with-kops-and-terraform-832b89250e8e).","title":"Why this workflow"},{"location":"user-guide/compute/k8s-kops/#kops-cluster-management","text":"The 2-kops directory includes helper scripts and Terraform files in order to template our Kubernetes cluster definition. The idea is to use our Terraform outputs from 1-prerequisites to construct a cluster definition.","title":"Kops Cluster Management"},{"location":"user-guide/compute/k8s-kops/#overview","text":"Cluster Management via Kops is typically carried out through the kops CLI. In this case, we use a 2-kops directory that contains a Makefile, Terraform files and other helper scripts that reinforce the workflow we use to create/update/delete the cluster.","title":"Overview"},{"location":"user-guide/compute/k8s-kops/#workflow","text":"This workflow is a little different to the typical Terraform workflows we use. The full workflow goes as follows: Cluster: Creation & Update Modify files under 1-prerequisites Main files to update probably are locals.tf and outputs.tf Mostly before the cluster is created but could be needed afterward Modify cluster-template.yml under 2-kops folder E.g. to add or remove instance groups, upgrade k8s version, etc At 2-kops/ context run make cluster-update will follow the steps below Get Terraform outputs from 1-prerequisites Generate a Kops cluster manifest -- it uses cluster-template.yml as a template and the outputs from the point above as replacement values Update Kops state -- it uses the generated Kops cluster manifest in previous point ( cluster.yml ) Generate Kops Terraform file ( kubernetes.tf ) -- this file represents the changes that Kops needs to apply on the cloud provider. Run make plan To preview any infrastructure changes that Terraform will make. If desired we could submit a PR, allowing you and the rest of the team to understand and review what changes would be made to the Kubernetes cluster before excecuting make apply ( terraform apply ). This brings the huge benefit of treating changes to our Kubernetes clusters with a GitOps oriented approach, basically like we treat any other code & infrastructure change, and integrate it with the rest of our tools and practices like CI/CD, integration testing, replicate environments and so on. Run make apply To apply those infrastructure changes on AWS. Run make cluster-rolling-update To determine if Kops needs to trigger some changes to happen right now (dry run) These are usually changes to the EC2 instances that won't get reflected as they depend on the autoscaling Run make cluster-rolling-update-yes To actually make any changes to the cluster masters/nodes happen Cluster: Deletion To clean-up any resources created for your K8s cluster, you should run: At 2-kops folder context run make destroy This will excecute a terraform destroy of all the kubernets.tf declared AWS resources. At 2-kops folder context run cluster-destroy Will run Kops destroy cluster -- only dry run, no changes will be applied Exec cluster-destroy-yes Kops will effectively destroy all the remaining cluster resources. Finally if at 1-prerequisites exec make destroy This will remove Kops state S3 bucket + any other extra resources you've provsioned for your cluster.","title":"Workflow"},{"location":"user-guide/compute/k8s-kops/#tipical-workflow","text":"The workflow may look complicated at first but generally it boils down to these simplified steps: 1. Modify cluster-template.yml 2. Run make cluste-update 3. Run make apply 4. Run make cluster-rolling-update-yes What about persistent and stateful K8s resources? This approach will work better the more stateless your Kubernetes workloads are. Treating Kubernetes clusters as ephemeral and replaceable infrastructure requires to consider not to use persistent volumes or the drawback of difficulties when running workloads such as databases on K8s. We feel pretty confident that we can recreate our workloads by applying each of our service definitions, charts and manifests to a given Kubernetes cluster as long as we keep the persistent storage separately on AWS RDS, DynamoDB, EFS and so on. In terms of the etcd state persistency, Kops already provisions the etcd volumes (AWS EBS) independently to the master instances they get attached to. This helps to persist the etcd state after rolling update your master nodes without any user intervention. Moreover simplifying volume backups via EBS Snapshots (consider https://github.com/binbashar/terraform-aws-backup-by-tags). We also use a very valuable backup tool named Velero (formerly Heptio Ark - https://github.com/vmware-tanzu/velero) to o back up and restore our Kubernetes cluster resources and persistent volumes. TODO IMPORTANT: Kops terraform output ( kops update cluster --target terraform ) is still generated for Terraform 0.11.x (https://github.com/kubernetes/kops/issues/7052) we'll take care of the migration when tf-0.12 gets fully supported. Create a Binbash Leverage public Confluence Wiki entry detailing some more info about etcd, calico and k8s versions compatibilities","title":"Tipical Workflow"},{"location":"user-guide/compute/overview/","text":"","title":"Overview"},{"location":"user-guide/compute/serverless/","text":"","title":"Serverless"},{"location":"user-guide/costs/costs/","text":"Cost Management Layer \u00b6 How it works documentation: Costs User guide \u00b6 TODO Next Steps \u00b6 TODO","title":"Costs"},{"location":"user-guide/costs/costs/#cost-management-layer","text":"How it works documentation: Costs","title":"Cost Management Layer"},{"location":"user-guide/costs/costs/#user-guide","text":"TODO","title":"User guide"},{"location":"user-guide/costs/costs/#next-steps","text":"TODO","title":"Next Steps"},{"location":"user-guide/database/database/","text":"","title":"Databases"},{"location":"user-guide/database/mysql/","text":"","title":"MySQL"},{"location":"user-guide/database/postgres/","text":"","title":"PostgresSQL"},{"location":"user-guide/identities/credentials/","text":"AWS Credentials \u00b6 Setup \u00b6 Automated AWS Credentials setup Place your shell at le-tf-infra-aws/@bin/scripts/aws_credentials_setup Depending on your python version: Run pip install --no-cache-dir -r requirements.txt Run pip3 install --no-cache-dir -r requirements.txt Run python3 aws_setup_credentials.py Follow the cli workflow shown immediately below to get a better understanding Figure: AWS credentials automated setup. Resulting Example for: ~/.aws/leverage/credentials \u00b6 #================================================================# # LEVERAGE credentials # #================================================================# #------------------------------------# # AWS OrganizationAccountAccessRole # #------------------------------------# [binbash-root] aws_access_key_id = AKIXXXXXXXXXXXXXXXXXXXXX aws_secret_access_key = cKJ2XXXXXXXXXXXXXXXXXXXXXXXXXXX region = us-east-1 #------------------------------------# # AWS DevOps Role # #------------------------------------# [binbash-security] aws_access_key_id = AKXXXXXXXXXXXXXXXXXXXXXXX aws_secret_access_key = cKJ29HXXXXXXXXXXXXXXXXXXXXXXXXX region = us-east-1 Resulting Example for: ~/.aws/leverage/cofigs \u00b6 [default] output = json region = us-east-1 #================================================================# # LEVERAGE config # #================================================================# #------------------------------------# # AWS OrganizationAccountAccessRole # #------------------------------------# [profile binbash-security-oaar] output = json region = us-east-1 role_arn = arn:aws:iam::111111111111:role/OrganizationAccountAccessRole source_profile = binbash-root [profile binbash-shared-oaar] output = json region = us-east-1 role_arn = arn:aws:iam::222222222222:role/OrganizationAccountAccessRole source_profile = binbash-root [profile binbash-apps-devstg-oaar] output = json region = us-east-1 role_arn = arn:aws:iam::333333333333:role/OrganizationAccountAccessRole source_profile = binbash-root [profile binbash-apps-prd-oaar-replication] output = json region = us-east-2 role_arn = arn:aws:iam::444444444444:role/OrganizationAccountAccessRole source_profile = binbash-root [profile binbash-legacy-oaar] output = json region = us-east-1 role_arn = arn:aws:iam::555555555555:role/OrganizationAccountAccessRole source_profile = binbash-root #------------------------------------# # AWS DevOps Role # #------------------------------------# [profile binbash-security-devops] output = json region = us-east-1 role_arn = arn:aws:iam::111111111111:role/DevOps source_profile = binbash-security [profile binbash-shared-devops] output = json region = us-east-1 role_arn = arn:aws:iam::222222222222:role/DevOps source_profile = binbash-security [profile binbash-apps-devstg-devops] output = json region = us-east-1 role_arn = arn:aws:iam::333333333333:role/DevOps source_profile = binbash-security [profile binbash-apps-prd-devops] output = json region = us-east-1 role_arn = arn:aws:iam::444444444444:role/DevOps source_profile = binbash-security [profile binbash-legacy-devops] output = json region = us-east-1 role_arn = arn:aws:iam::555555555555:role/DevOps source_profile = binbash-security Switching to a AWS Organization Member Account Role \u00b6 AWS reference links Consider the following AWS official links as reference: Access AWS Organization member account Switching to a Role (Web Console) Switching to an IAM Role (AWS CLI) [awscli roles]https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html Environment variables to configure the AWS CLI Since our AWS Credentials files location is not as default please consider the below code before using the the awscli in your terminal $ AWS_SHARED_CREDENTIALS_FILE_VAR = \"~/.aws/bb-le/credentials\" $ export AWS_SHARED_CREDENTIALS_FILE = ${ AWS_SHARED_CREDENTIALS_FILE_VAR } $ AWS_CONFIG_FILE_VAR = \"~/.aws/bb-le/config\" $ export AWS_CONFIG_FILE = ${ AWS_CONFIG_FILE_VAR } $ aws ec2 describe-instances --profile project-apps-devstg-devops Example \u256d\u2500delivery at delivery-I7567 in ~/Binbash/repos/Leverage/ref-architecture \u2570\u2500\u2820\u2835 AWS_SHARED_CREDENTIALS_FILE_VAR = \"~/.aws/bb/credentials\" export AWS_SHARED_CREDENTIALS_FILE = ${ AWS_SHARED_CREDENTIALS_FILE_VAR } AWS_CONFIG_FILE_VAR = \"~/.aws/bb/config\" export AWS_CONFIG_FILE = ${ AWS_CONFIG_FILE_VAR } \u256d\u2500delivery at delivery-I7567 in ~/Binbash/repos/Leverage/ref-architecture \u2570\u2500\u2820\u2835 aws ec2 describe-instances --profile bb-apps-devstg-devops { \"Reservations\" : [] } Accessing a member account as the root user Read More \u00b6 AWS reference links Consider the following AWS official links as reference: Best practices for managing AWS access keys Cloud.gov | Secret key management - AWS credentials","title":"credentials"},{"location":"user-guide/identities/credentials/#aws-credentials","text":"","title":"AWS Credentials"},{"location":"user-guide/identities/credentials/#setup","text":"Automated AWS Credentials setup Place your shell at le-tf-infra-aws/@bin/scripts/aws_credentials_setup Depending on your python version: Run pip install --no-cache-dir -r requirements.txt Run pip3 install --no-cache-dir -r requirements.txt Run python3 aws_setup_credentials.py Follow the cli workflow shown immediately below to get a better understanding Figure: AWS credentials automated setup.","title":"Setup"},{"location":"user-guide/identities/credentials/#resulting-example-for-awsleveragecredentials","text":"#================================================================# # LEVERAGE credentials # #================================================================# #------------------------------------# # AWS OrganizationAccountAccessRole # #------------------------------------# [binbash-root] aws_access_key_id = AKIXXXXXXXXXXXXXXXXXXXXX aws_secret_access_key = cKJ2XXXXXXXXXXXXXXXXXXXXXXXXXXX region = us-east-1 #------------------------------------# # AWS DevOps Role # #------------------------------------# [binbash-security] aws_access_key_id = AKXXXXXXXXXXXXXXXXXXXXXXX aws_secret_access_key = cKJ29HXXXXXXXXXXXXXXXXXXXXXXXXX region = us-east-1","title":"Resulting Example for: ~/.aws/leverage/credentials"},{"location":"user-guide/identities/credentials/#resulting-example-for-awsleveragecofigs","text":"[default] output = json region = us-east-1 #================================================================# # LEVERAGE config # #================================================================# #------------------------------------# # AWS OrganizationAccountAccessRole # #------------------------------------# [profile binbash-security-oaar] output = json region = us-east-1 role_arn = arn:aws:iam::111111111111:role/OrganizationAccountAccessRole source_profile = binbash-root [profile binbash-shared-oaar] output = json region = us-east-1 role_arn = arn:aws:iam::222222222222:role/OrganizationAccountAccessRole source_profile = binbash-root [profile binbash-apps-devstg-oaar] output = json region = us-east-1 role_arn = arn:aws:iam::333333333333:role/OrganizationAccountAccessRole source_profile = binbash-root [profile binbash-apps-prd-oaar-replication] output = json region = us-east-2 role_arn = arn:aws:iam::444444444444:role/OrganizationAccountAccessRole source_profile = binbash-root [profile binbash-legacy-oaar] output = json region = us-east-1 role_arn = arn:aws:iam::555555555555:role/OrganizationAccountAccessRole source_profile = binbash-root #------------------------------------# # AWS DevOps Role # #------------------------------------# [profile binbash-security-devops] output = json region = us-east-1 role_arn = arn:aws:iam::111111111111:role/DevOps source_profile = binbash-security [profile binbash-shared-devops] output = json region = us-east-1 role_arn = arn:aws:iam::222222222222:role/DevOps source_profile = binbash-security [profile binbash-apps-devstg-devops] output = json region = us-east-1 role_arn = arn:aws:iam::333333333333:role/DevOps source_profile = binbash-security [profile binbash-apps-prd-devops] output = json region = us-east-1 role_arn = arn:aws:iam::444444444444:role/DevOps source_profile = binbash-security [profile binbash-legacy-devops] output = json region = us-east-1 role_arn = arn:aws:iam::555555555555:role/DevOps source_profile = binbash-security","title":"Resulting Example for: ~/.aws/leverage/cofigs"},{"location":"user-guide/identities/credentials/#switching-to-a-aws-organization-member-account-role","text":"AWS reference links Consider the following AWS official links as reference: Access AWS Organization member account Switching to a Role (Web Console) Switching to an IAM Role (AWS CLI) [awscli roles]https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html Environment variables to configure the AWS CLI Since our AWS Credentials files location is not as default please consider the below code before using the the awscli in your terminal $ AWS_SHARED_CREDENTIALS_FILE_VAR = \"~/.aws/bb-le/credentials\" $ export AWS_SHARED_CREDENTIALS_FILE = ${ AWS_SHARED_CREDENTIALS_FILE_VAR } $ AWS_CONFIG_FILE_VAR = \"~/.aws/bb-le/config\" $ export AWS_CONFIG_FILE = ${ AWS_CONFIG_FILE_VAR } $ aws ec2 describe-instances --profile project-apps-devstg-devops Example \u256d\u2500delivery at delivery-I7567 in ~/Binbash/repos/Leverage/ref-architecture \u2570\u2500\u2820\u2835 AWS_SHARED_CREDENTIALS_FILE_VAR = \"~/.aws/bb/credentials\" export AWS_SHARED_CREDENTIALS_FILE = ${ AWS_SHARED_CREDENTIALS_FILE_VAR } AWS_CONFIG_FILE_VAR = \"~/.aws/bb/config\" export AWS_CONFIG_FILE = ${ AWS_CONFIG_FILE_VAR } \u256d\u2500delivery at delivery-I7567 in ~/Binbash/repos/Leverage/ref-architecture \u2570\u2500\u2820\u2835 aws ec2 describe-instances --profile bb-apps-devstg-devops { \"Reservations\" : [] } Accessing a member account as the root user","title":"Switching to a AWS Organization Member Account Role"},{"location":"user-guide/identities/credentials/#read-more","text":"AWS reference links Consider the following AWS official links as reference: Best practices for managing AWS access keys Cloud.gov | Secret key management - AWS credentials","title":"Read More"},{"location":"user-guide/identities/gpg/","text":"PGP keys helper \u00b6 Why to use PGP? \u00b6 By default our Leverage Reference Architectre base-identities layer approach is to use IAM module to manage AWS IAM Users credentials with encryption to grant strong security . This module outputs commands and PGP messages which can be decrypted either using command line to get AWS Web Connsole user's password and user's secret key. Notes for keybase users If possible, always use PGP encryption to prevent Terraform from keeping unencrypted password and access secret key in state file. Keybase pre-requisites When pgp_key is specified as keybase:username , make sure that the user public key has already been uploaded to the Reference Architecture base-identities layer keys folder How to manage your GPG keys? \u00b6 Create a key pair NOTE: the user for whom this account is being created needs to do this Install gpg Run gpg --version to confirm Run gpg --gen-key and provide \"Your Name\" and \"Your Email\" as instructed -- you must also provide a passphrase Run gpg --list-keys to check that your key was generated Delete a key pair Run gpg --list-keys to check your key id Run gpg --delete-secret-keys \"Your Name\" to delete your private gpg key Run gpg --delete-key \"Your Name\" to delete your public gpg key Export your public key NOTE: the user must have created a key pair before doing this Run gpg --export \"Your Name\" | base64 Now the user can share her/his public key for creating her/his account Decrypt your encrypted password The user should copy the encrypted password from whatever media it was provided to her/him Run echo \"YOUR ENCRYPTED STRING PASSWORD HERE\" | base64 --decode > a_file_with_your_pass $ echo \"wcBMA/ujy1wF7UPcAQgASLL/x8zz7OHIP+EHU7IAZfa1A9qD9ScP5orK1M473WlXVgPrded0iHpyZRwsJRS8Xe38AHZ65O6CnywdR522MbD\\ RD6Yz+Bfc9NwO316bfSoTpyROXvMi+cfMEcihInHaCIP9YWBaI3eJ6VFdn90g9of00HYehBux7E2VitMuWo+v46W1p8/pw0b0H5qcppnUYYOjjSbjzzAuMF\\ yNB5M1K8av61bPQPQTxBH3SFaM0B4RNmUl1bHKDIcdESYyIP/PRLQ45Rs5MzGgALIxBy24qdPNjHJQR48/5QV4nzB9qeEe4eWDB4ynSEfLsXggiz8fsbajV\\ gSLNsdpqP9lYaueFdLgAeR6a+EjvqZfq0hZAgoiymsb4Qtn4A7gmeGmNeDE4td1mVfgzuTZ9zhnSbAYlXNIiM4b0MeX4HrjFkT/Aq+A/rvgBeKhszWD4Ibh\\ A4PgC+QPiJRb5kQ/mX8DheQfAHJ24iUZk1jh6AsA\" | base64 --decode > encrypted_pass Run gpg --decrypt a_file_with_your_pass (in the path you've executed 2.) to effectively decrypt your pass using your gpg key and its passphrase $ gpg --decrypt encrypted_pass You need a passphrase to unlock the secret key for user: \"Demo User (AWS org project-user acct gpg key w/ passphrase) <username.lastname@domain.com>\" 2048 -bit RSA key, ID 05ED43DC, created 2019 -03-15 ( main key ID D64DD59F ) gpg: encrypted with 2048 -bit RSA key, ID 05ED43DC, created 2019 -03-15 \"Demo User (AWS org project-user acct gpg key w/ passphrase) <username.lastname@domain.com>\" Vi0JA | c%fP*FhL } CE-D7ssp_TVGlf#% Depending on your shell version an extra % character could appear as shown below, you must disregard this character since it's not part of the Initial (one time) AWS Web Console password. If all went well, the decrypted password should be there","title":"gpg"},{"location":"user-guide/identities/gpg/#pgp-keys-helper","text":"","title":"PGP keys helper"},{"location":"user-guide/identities/gpg/#why-to-use-pgp","text":"By default our Leverage Reference Architectre base-identities layer approach is to use IAM module to manage AWS IAM Users credentials with encryption to grant strong security . This module outputs commands and PGP messages which can be decrypted either using command line to get AWS Web Connsole user's password and user's secret key. Notes for keybase users If possible, always use PGP encryption to prevent Terraform from keeping unencrypted password and access secret key in state file. Keybase pre-requisites When pgp_key is specified as keybase:username , make sure that the user public key has already been uploaded to the Reference Architecture base-identities layer keys folder","title":"Why to use PGP?"},{"location":"user-guide/identities/gpg/#how-to-manage-your-gpg-keys","text":"Create a key pair NOTE: the user for whom this account is being created needs to do this Install gpg Run gpg --version to confirm Run gpg --gen-key and provide \"Your Name\" and \"Your Email\" as instructed -- you must also provide a passphrase Run gpg --list-keys to check that your key was generated Delete a key pair Run gpg --list-keys to check your key id Run gpg --delete-secret-keys \"Your Name\" to delete your private gpg key Run gpg --delete-key \"Your Name\" to delete your public gpg key Export your public key NOTE: the user must have created a key pair before doing this Run gpg --export \"Your Name\" | base64 Now the user can share her/his public key for creating her/his account Decrypt your encrypted password The user should copy the encrypted password from whatever media it was provided to her/him Run echo \"YOUR ENCRYPTED STRING PASSWORD HERE\" | base64 --decode > a_file_with_your_pass $ echo \"wcBMA/ujy1wF7UPcAQgASLL/x8zz7OHIP+EHU7IAZfa1A9qD9ScP5orK1M473WlXVgPrded0iHpyZRwsJRS8Xe38AHZ65O6CnywdR522MbD\\ RD6Yz+Bfc9NwO316bfSoTpyROXvMi+cfMEcihInHaCIP9YWBaI3eJ6VFdn90g9of00HYehBux7E2VitMuWo+v46W1p8/pw0b0H5qcppnUYYOjjSbjzzAuMF\\ yNB5M1K8av61bPQPQTxBH3SFaM0B4RNmUl1bHKDIcdESYyIP/PRLQ45Rs5MzGgALIxBy24qdPNjHJQR48/5QV4nzB9qeEe4eWDB4ynSEfLsXggiz8fsbajV\\ gSLNsdpqP9lYaueFdLgAeR6a+EjvqZfq0hZAgoiymsb4Qtn4A7gmeGmNeDE4td1mVfgzuTZ9zhnSbAYlXNIiM4b0MeX4HrjFkT/Aq+A/rvgBeKhszWD4Ibh\\ A4PgC+QPiJRb5kQ/mX8DheQfAHJ24iUZk1jh6AsA\" | base64 --decode > encrypted_pass Run gpg --decrypt a_file_with_your_pass (in the path you've executed 2.) to effectively decrypt your pass using your gpg key and its passphrase $ gpg --decrypt encrypted_pass You need a passphrase to unlock the secret key for user: \"Demo User (AWS org project-user acct gpg key w/ passphrase) <username.lastname@domain.com>\" 2048 -bit RSA key, ID 05ED43DC, created 2019 -03-15 ( main key ID D64DD59F ) gpg: encrypted with 2048 -bit RSA key, ID 05ED43DC, created 2019 -03-15 \"Demo User (AWS org project-user acct gpg key w/ passphrase) <username.lastname@domain.com>\" Vi0JA | c%fP*FhL } CE-D7ssp_TVGlf#% Depending on your shell version an extra % character could appear as shown below, you must disregard this character since it's not part of the Initial (one time) AWS Web Console password. If all went well, the decrypted password should be there","title":"How to manage your GPG keys?"},{"location":"user-guide/identities/identities/","text":"Identity and Access Management (IAM) Layer \u00b6 How it works documentation: identities User guide \u00b6 Please follow the steps below to orchestrate your base-identities layer 1st in your project-root AWS account and afterwards in your project-security account. IAM user standard creation workflow Pre-requisite add Public PGP Key following the documentation For steps 3. and 4. consider following Terraform make workflow Update (add | remove) your IAM Users associated code and deploy security/base-identities/users.tf Consider customizing your account Alias and Password Policy Update (add | remove | edit) your IAM Groups associated code and deploy security/base-identities/groups.tf Get and share the IAM Users AWS Console user id and its OTP associated password from the make apply outputs temporally set sensitive = false to get the encrypted outputs in your prompt output. Each user will need to decrypt its AWS Console Password, you could share the associated documentation with them. Users must login to the AWS Web Console (https://project-security.signin.aws.amazon.com/console) with their decrypted password and create new pass Activate MFA for Web Console (Optional but strongly recommended) User should create his AWS ACCESS KEYS if needed User could optionally set up ~/.aws/project/credentials + ~/.aws/project/config following the immediately below AWS Credentials Setup sub-section To allow users to Access AWS Organization member account consider repeating step 3. but for the corresponding member accounts: shared/base-identities apps-devstg/base-identities app-prd/base-identities Recommended post-task \u00b6 Deactivating AWS STS in not in use AWS Region When you activate STS endpoints for a Region, AWS STS can issue temporary credentials to users and roles in your account that make an AWS STS request. Those credentials can then be used in any Region that is enabled by default or is manually enabled. You must activate the Region in the account where the temporary credentials are generated. It does not matter whether a user is signed into the same account or a different account when they make the request. To activate or deactivate AWS STS in a Region that is enabled by default (console) Sign in as a root user or an IAM user with permissions to perform IAM administration tasks. Open the IAM console and in the navigation pane choose Account settings. If necessary, expand Security Token Service (STS), find the Region that you want to activate, and then choose Activate or Deactivate. For Regions that must be enabled, we activate STS automatically when you enable the Region. After you enable a Region, AWS STS is always active for the Region and you cannot deactivate it. To learn how to enable a Region, see Managing AWS Regions in the AWS General Reference. Source | AWS Documentation IAM User Guide | Activating and deactivating AWS STS in an AWS Region Figure: Deactivating AWS STS in not in use AWS Region. Only in used Regions must have STS activated. Next Steps \u00b6 Setup your AWS Credentials","title":"identities"},{"location":"user-guide/identities/identities/#identity-and-access-management-iam-layer","text":"How it works documentation: identities","title":"Identity and Access Management (IAM) Layer"},{"location":"user-guide/identities/identities/#user-guide","text":"Please follow the steps below to orchestrate your base-identities layer 1st in your project-root AWS account and afterwards in your project-security account. IAM user standard creation workflow Pre-requisite add Public PGP Key following the documentation For steps 3. and 4. consider following Terraform make workflow Update (add | remove) your IAM Users associated code and deploy security/base-identities/users.tf Consider customizing your account Alias and Password Policy Update (add | remove | edit) your IAM Groups associated code and deploy security/base-identities/groups.tf Get and share the IAM Users AWS Console user id and its OTP associated password from the make apply outputs temporally set sensitive = false to get the encrypted outputs in your prompt output. Each user will need to decrypt its AWS Console Password, you could share the associated documentation with them. Users must login to the AWS Web Console (https://project-security.signin.aws.amazon.com/console) with their decrypted password and create new pass Activate MFA for Web Console (Optional but strongly recommended) User should create his AWS ACCESS KEYS if needed User could optionally set up ~/.aws/project/credentials + ~/.aws/project/config following the immediately below AWS Credentials Setup sub-section To allow users to Access AWS Organization member account consider repeating step 3. but for the corresponding member accounts: shared/base-identities apps-devstg/base-identities app-prd/base-identities","title":"User guide"},{"location":"user-guide/identities/identities/#recommended-post-task","text":"Deactivating AWS STS in not in use AWS Region When you activate STS endpoints for a Region, AWS STS can issue temporary credentials to users and roles in your account that make an AWS STS request. Those credentials can then be used in any Region that is enabled by default or is manually enabled. You must activate the Region in the account where the temporary credentials are generated. It does not matter whether a user is signed into the same account or a different account when they make the request. To activate or deactivate AWS STS in a Region that is enabled by default (console) Sign in as a root user or an IAM user with permissions to perform IAM administration tasks. Open the IAM console and in the navigation pane choose Account settings. If necessary, expand Security Token Service (STS), find the Region that you want to activate, and then choose Activate or Deactivate. For Regions that must be enabled, we activate STS automatically when you enable the Region. After you enable a Region, AWS STS is always active for the Region and you cannot deactivate it. To learn how to enable a Region, see Managing AWS Regions in the AWS General Reference. Source | AWS Documentation IAM User Guide | Activating and deactivating AWS STS in an AWS Region Figure: Deactivating AWS STS in not in use AWS Region. Only in used Regions must have STS activated.","title":"Recommended post-task"},{"location":"user-guide/identities/identities/#next-steps","text":"Setup your AWS Credentials","title":"Next Steps"},{"location":"user-guide/monitoring/apm/","text":"Application Performance Monitoring (APM) and Business Performance \u00b6 How it works documentation: APM User guide \u00b6 TODO","title":"APM"},{"location":"user-guide/monitoring/apm/#application-performance-monitoring-apm-and-business-performance","text":"How it works documentation: APM","title":"Application Performance Monitoring (APM) and Business Performance"},{"location":"user-guide/monitoring/apm/#user-guide","text":"TODO","title":"User guide"},{"location":"user-guide/monitoring/logs/","text":"","title":"Logs"},{"location":"user-guide/monitoring/metrics/","text":"","title":"Metrics"},{"location":"user-guide/monitoring/monitoring/","text":"","title":"Monitoring"},{"location":"user-guide/monitoring/tracing/","text":"","title":"Tracing"},{"location":"user-guide/network/dns/","text":"Route53 DNS hosted zones \u00b6 How it works documentation: DNS User guide \u00b6 pre-requisites Review & update configs Review & understand the workflow Steps DNS service has to be orchestrated from /shared/base-dns layer following the standard workflow To associate AWS Org Member accounts to your DNS Private Hosted Zones eg: *.aws.domain.com (previously deployed) proceed applying the corresponding layers apps-devstg/base-dns apps-prd/base-dns repeat for every other necessary AWS Org Memeber account. Migrated AWS Route53 Hosted Zones between AWS Accounts \u00b6 We'll need to setup the Route53 DNS service with an active-active config to avoid any type of service disruption and downtime. This would then allow the Name Servers of both AWS Accounts to be added to your domain provider (eg: namecheap.com ) and have for example: 4 x ns ( project-legacy Route53 Account) 4 x ns ( project-shared Route53 Account) After the records have propagated and everything looks OK we could remove the project-legacy Route53 ns from your domain provider (eg: namecheap.com ) and leave only the of project-shared ones. This official Migrating a hosted zone to a different AWS account - Amazon Route 53 article explains this procedure step by step: AWS Route53 hosted zone migration steps Create records in the new hosted zone (bb-shared) Compare records in the old and new hosted zones (bb-legacy) Update the domain registration to use name servers for the new hosted zone (NIC updated to use both bb-legacy + bb-shared) Wait for DNS resolvers to start using the new hosted zone (Optional) delete the old hosted zone (bb-legacy), remember you'll need to delete the ns delegation records from your domain registration (NIC) too.","title":"DNS"},{"location":"user-guide/network/dns/#route53-dns-hosted-zones","text":"How it works documentation: DNS","title":"Route53 DNS hosted zones"},{"location":"user-guide/network/dns/#user-guide","text":"pre-requisites Review & update configs Review & understand the workflow Steps DNS service has to be orchestrated from /shared/base-dns layer following the standard workflow To associate AWS Org Member accounts to your DNS Private Hosted Zones eg: *.aws.domain.com (previously deployed) proceed applying the corresponding layers apps-devstg/base-dns apps-prd/base-dns repeat for every other necessary AWS Org Memeber account.","title":"User guide"},{"location":"user-guide/network/dns/#migrated-aws-route53-hosted-zones-between-aws-accounts","text":"We'll need to setup the Route53 DNS service with an active-active config to avoid any type of service disruption and downtime. This would then allow the Name Servers of both AWS Accounts to be added to your domain provider (eg: namecheap.com ) and have for example: 4 x ns ( project-legacy Route53 Account) 4 x ns ( project-shared Route53 Account) After the records have propagated and everything looks OK we could remove the project-legacy Route53 ns from your domain provider (eg: namecheap.com ) and leave only the of project-shared ones. This official Migrating a hosted zone to a different AWS account - Amazon Route 53 article explains this procedure step by step: AWS Route53 hosted zone migration steps Create records in the new hosted zone (bb-shared) Compare records in the old and new hosted zones (bb-legacy) Update the domain registration to use name servers for the new hosted zone (NIC updated to use both bb-legacy + bb-shared) Wait for DNS resolvers to start using the new hosted zone (Optional) delete the old hosted zone (bb-legacy), remember you'll need to delete the ns delegation records from your domain registration (NIC) too.","title":"Migrated AWS Route53 Hosted Zones between AWS Accounts"},{"location":"user-guide/network/vpc-addressing/","text":"Network Layer \u00b6 How it works documentation: Networking User guide \u00b6 Please follow the steps below to orchestrate your base-network layer, 1st in your project-shared AWS account and afterwards in the necessary member accounts which will host network connected resources (EC2, Lambda, EKS, RDS, ALB, NLB, etc): project-apps-devstg account. project-apps-prd account. Network layer standard creation workflow Please follow Terraform make workflow for each of your accounts. We'll start by project-shared AWS Account Update (add | remove | customize) your VPC associated code before deploying this layer shared/base-network Main files network.tf locals.tf Repeat for every AWS member Account that needs its own VPC Access AWS Organization member account consider repeating step 3. but for the corresponding member accounts. Next Steps \u00b6 AWS VPC Peering","title":"VPC"},{"location":"user-guide/network/vpc-addressing/#network-layer","text":"How it works documentation: Networking","title":"Network Layer"},{"location":"user-guide/network/vpc-addressing/#user-guide","text":"Please follow the steps below to orchestrate your base-network layer, 1st in your project-shared AWS account and afterwards in the necessary member accounts which will host network connected resources (EC2, Lambda, EKS, RDS, ALB, NLB, etc): project-apps-devstg account. project-apps-prd account. Network layer standard creation workflow Please follow Terraform make workflow for each of your accounts. We'll start by project-shared AWS Account Update (add | remove | customize) your VPC associated code before deploying this layer shared/base-network Main files network.tf locals.tf Repeat for every AWS member Account that needs its own VPC Access AWS Organization member account consider repeating step 3. but for the corresponding member accounts.","title":"User guide"},{"location":"user-guide/network/vpc-addressing/#next-steps","text":"AWS VPC Peering","title":"Next Steps"},{"location":"user-guide/network/vpc-peering/","text":"Diagram: Network Service (cross-account VPC peering ) \u00b6 How it works \u00b6 TODO User guide \u00b6 TODO","title":"VPC Peering"},{"location":"user-guide/network/vpc-peering/#diagram-network-service-cross-account-vpc-peering","text":"","title":"Diagram: Network Service (cross-account VPC peering)"},{"location":"user-guide/network/vpc-peering/#how-it-works","text":"TODO","title":"How it works"},{"location":"user-guide/network/vpc-peering/#user-guide","text":"TODO","title":"User guide"},{"location":"user-guide/organization/organization/","text":"Reference Architecture: Terraform AWS Organizations Account Baseline \u00b6 How it works documentation: organization documentation: organization accounts User guide \u00b6 Pre-requisites \u00b6 You'll need an email to create and register your AWS Organization Root Account . For this purpose we recommend to avoid using an individual nominated email user. Instead, whenever possible, it should ideally be associated, with a distribution list email such as a GSuite Group to ensure the proper admins member's team (DevOps | SecOps | Cloud Engineering Team) to manage its notifications avoiding a single point of contact (constraint). Email setup example \u00b6 GSuite Group Email address: aws@domain.com (to which admins / owners belong), and then using the + we generate the aliases automatically implicitly when running Terraform's Leverage code. aws+security@binbash.com.ar aws+shared@binbash.com.ar aws+apps-devstg@binbash.com.ar aws+apps-prd@binbash.com.ar Important # # Project Prd: services and resources related to production are placed and # maintained here. # resource \"aws_organizations_account\" \"apps_prd\" { name = \"apps-prd\" email = \"aws+apps-prd@doamin.ar\" parent_id = aws_organizations_organizational_unit.apps_prd.id } Example \u00b6 Steps Create a brand new AWS Account , intended to be our AWS Organization Root Account Via AWS Web Console: in project-root create the root-org IAM user with Admin privileges, which will be use for the initial AWS Org bootstrapping. After it\u2019s 1st execution only nominated Org admin users will persist in the project-root account. The AWS Org will be orchestrated -> Leverage Ref Code Via AWS Web Console: from your project-root acconunt invite the pre-existing project-legacy (1 to n accounts). Via AWS Web Console: in project-legacy create the OrganizationAccountAccessRole IAM Role with Admin permissions. Should follow Creating the OrganizationAccountAccessRole in an invited member account section. Import your project-legacy account as code. Update the following variables in ./@bin/makefiles/terraform12/Makefile.terraform12-import-rm TF_IMPORT_RESOURCE := \"aws_organizations_organizational_unit.bbl_apps_devstg\" TF_IMPORT_RESOURCE_ID := \"ou-oz9d-yl3npduj\" TF_RM_RESOURCE := \"aws_organizations_organizational_unit.bbl_apps_devstg\" Then from the root context -> cd ./root/organizaton make import Recommended following configurations Identities Switch to the project-security account for consolidated and centralized User Mgmt and access to the AWS Org.","title":"Organization"},{"location":"user-guide/organization/organization/#reference-architecture-terraform-aws-organizations-account-baseline","text":"How it works documentation: organization documentation: organization accounts","title":"Reference Architecture: Terraform AWS Organizations Account Baseline"},{"location":"user-guide/organization/organization/#user-guide","text":"","title":"User guide"},{"location":"user-guide/organization/organization/#pre-requisites","text":"You'll need an email to create and register your AWS Organization Root Account . For this purpose we recommend to avoid using an individual nominated email user. Instead, whenever possible, it should ideally be associated, with a distribution list email such as a GSuite Group to ensure the proper admins member's team (DevOps | SecOps | Cloud Engineering Team) to manage its notifications avoiding a single point of contact (constraint).","title":"Pre-requisites"},{"location":"user-guide/organization/organization/#email-setup-example","text":"GSuite Group Email address: aws@domain.com (to which admins / owners belong), and then using the + we generate the aliases automatically implicitly when running Terraform's Leverage code. aws+security@binbash.com.ar aws+shared@binbash.com.ar aws+apps-devstg@binbash.com.ar aws+apps-prd@binbash.com.ar Important # # Project Prd: services and resources related to production are placed and # maintained here. # resource \"aws_organizations_account\" \"apps_prd\" { name = \"apps-prd\" email = \"aws+apps-prd@doamin.ar\" parent_id = aws_organizations_organizational_unit.apps_prd.id }","title":"Email setup example"},{"location":"user-guide/organization/organization/#example","text":"Steps Create a brand new AWS Account , intended to be our AWS Organization Root Account Via AWS Web Console: in project-root create the root-org IAM user with Admin privileges, which will be use for the initial AWS Org bootstrapping. After it\u2019s 1st execution only nominated Org admin users will persist in the project-root account. The AWS Org will be orchestrated -> Leverage Ref Code Via AWS Web Console: from your project-root acconunt invite the pre-existing project-legacy (1 to n accounts). Via AWS Web Console: in project-legacy create the OrganizationAccountAccessRole IAM Role with Admin permissions. Should follow Creating the OrganizationAccountAccessRole in an invited member account section. Import your project-legacy account as code. Update the following variables in ./@bin/makefiles/terraform12/Makefile.terraform12-import-rm TF_IMPORT_RESOURCE := \"aws_organizations_organizational_unit.bbl_apps_devstg\" TF_IMPORT_RESOURCE_ID := \"ou-oz9d-yl3npduj\" TF_RM_RESOURCE := \"aws_organizations_organizational_unit.bbl_apps_devstg\" Then from the root context -> cd ./root/organizaton make import Recommended following configurations Identities Switch to the project-security account for consolidated and centralized User Mgmt and access to the AWS Org.","title":"Example"},{"location":"user-guide/reliability/backups/","text":"","title":"Backups"},{"location":"user-guide/reliability/dr/","text":"","title":"Disaster Recovery"},{"location":"user-guide/reliability/health-checks/","text":"","title":"Health Checks"},{"location":"user-guide/secrets/secrets/","text":"","title":"Secrets"},{"location":"user-guide/security/services/","text":"AWS Security & Compliance Services \u00b6 How it works \u00b6 TODO User guide \u00b6 TODO","title":"Services"},{"location":"user-guide/security/services/#aws-security-compliance-services","text":"","title":"AWS Security &amp; Compliance Services"},{"location":"user-guide/security/services/#how-it-works","text":"TODO","title":"How it works"},{"location":"user-guide/security/services/#user-guide","text":"TODO","title":"User guide"},{"location":"user-guide/security/vpn/","text":"VPN Server \u00b6 How it works \u00b6 TODO User guide \u00b6 TODO","title":"VPN"},{"location":"user-guide/security/vpn/#vpn-server","text":"","title":"VPN Server"},{"location":"user-guide/security/vpn/#how-it-works","text":"TODO","title":"How it works"},{"location":"user-guide/security/vpn/#user-guide","text":"TODO","title":"User guide"},{"location":"user-guide/storage/storage/","text":"","title":"Storage"},{"location":"user-guide/tools/tools/","text":"","title":"Tools"},{"location":"work-with-us/","text":"Work with us \u00b6 Customers collaboration methodology \u00b6 Customer Support workflow \u00b6","title":"Overview"},{"location":"work-with-us/#work-with-us","text":"","title":"Work with us"},{"location":"work-with-us/#customers-collaboration-methodology","text":"","title":"Customers collaboration methodology"},{"location":"work-with-us/#customer-support-workflow","text":"","title":"Customer Support workflow"},{"location":"work-with-us/careers/","text":"Careers \u00b6 How we work \u00b6 Binbash work culture Fully Remote Binbash was founded as a remote-first company. That means you can always work from home, a co-working place, a nice cafe, or wherever else you feel comfortable, and you'll have almost complete control over your working hours. Why \"almost\"? Because depending on the current projects we'll require few hours of overlap between all Leverage collaborators for some specific meetings or shared sessions (pair-programming). Distributed Team Despite the fact that our collaborators are currently located in \ud83c\udde6\ud83c\uddf7 Argentina, \ud83c\udde7\ud83c\uddf7 Brazil and \ud83c\uddfa\ud83c\uddfe Uruguay, consider we are currently hiring from most countries in the time zones between GMT-7 (e.g. California, USA) to GMT+2 (e.g., Berlin, Germany). We promote life-work balance Job burnout is an epidemic \ud83d\ude46, and we tech workers are especially at risk. So we'll do our best to de-stress our workforce at Binbash. In order to achieve this we offer: Remote work that lets you control your hours and physical location. Normal working hours (prime-time 9am-5pm GTM-3), in average no more than ~30-40hs per week, and we don't work during weekends or your country of residence national holidays. Project management and planning that will take into consideration the time zone of all our team members. A flexible vacation policy where you could take 3 weeks per year away from the keyboard. If more time is needed we could always try to arrange it for you. No ON-CALL rotation. We only offer support contracts with SLAs of responses on prime time business days hours exlusively. You will take on big challenges, but the hours are reasonable. Everyone is treated fairly and with respect, but where disagreement and feedback is always welcome. That is welcoming, safe, and inclusive for people of all cultures, genders, and races. Leverage Software / DevOps Engineer Profile \u00b6 What You'll Work On (our tech stack) DevOps Automation Code Library Create a collection of reusable, tested, production-ready E2E AWS oriented infrastructure modules (e.g., VPC, IAM, Kubernetes, Prometheus, Grafana, EFK, Consul, Vault, Jenkins, etc.) using several tool and languages: Terraform, Ansible, Helm, Dockerfiles, Python, Bash and Makefiles . Reference Architecture Improve, maintain, extend and update our reference architecture, which has been desingned under optimal configs for the most popular modern web and mobile applications needs. Its design is fully based on the AWS Well Architected Framework . Open Source & Leverage DevOps Tools Contribute to our open source projects to continue building a fundamentally better DevOps experience, including our open source modules , leverage python CLI , Makefiles Lib among others. Document team knowledge Get siloed and not yet documented knowledge and extend the Leverage documentation , such as creating knowledgebase articles, runbooks, and other documentation for the internal team as well as Binbash Leverage customers. Customer engineering support While participating in business-hours only support rotations, collaborate with customer requests, teach Binbash Leverage and DevOps best-practices, help resolve problems, escalate to internal SMEs, and automate and document the solutions so that problems are mitigated for future scenarios and users. Role scope and extra points! Responsible for the development, maintenance, support and delivery of Binbash Leverage Products. Client side Leverage Reference Architecture solutions implementation, maintenance and support. Client side cloud solutions & tech management (service delivery and project task management). Bring Leverage recs for re-engineering, bug fixes (issues) report and improvements based on real scenario implementations. Mentoring, KT, PRs and team tech follow up both internally and customer facing. Binbash is a small, distributed startup, so things are changing all the time, and from time to time we all wear many hats. You should expect to write lot of code, but, depending on your interests, there will also be lot of opportunities to write blog posts, give talks, contribute to open source, go to conferences, talk with customers, do sales calls, think through financial questions, interview candidates, mentor new hires, design products, come up with marketing ideas, discuss strategy, consider legal questions, and all the other tasks that are part of working at a small company. Nice to have background You hate repeating and doing the same thing twice and would rather spend the time to automate a problem away than do the same task again. You have strong English communication skills and are comfortable engaging with external customers. You know how to write code across the stack ( \u201cDev\u201d ) and feel very comfortable with Infra as Code (\"IaC\"). You have experience running production software environments ( \"Ops\" ). You have a strong background in software engineering and understanding of CI/CD (or you are working hard on it!). You have a passion for learning new technologies, tools and programming languages. Bonus points for a sense of humor, empathy, autonomy and curiosity. Note that even if we're concerned with prior experience like AWS, Linux and Terraform, we're more concerned with curiosity about all areas of the Leverage stack and demonstrated ability to learn quickly and go deep when necessary.","title":"Careers"},{"location":"work-with-us/careers/#careers","text":"","title":"Careers"},{"location":"work-with-us/careers/#how-we-work","text":"Binbash work culture Fully Remote Binbash was founded as a remote-first company. That means you can always work from home, a co-working place, a nice cafe, or wherever else you feel comfortable, and you'll have almost complete control over your working hours. Why \"almost\"? Because depending on the current projects we'll require few hours of overlap between all Leverage collaborators for some specific meetings or shared sessions (pair-programming). Distributed Team Despite the fact that our collaborators are currently located in \ud83c\udde6\ud83c\uddf7 Argentina, \ud83c\udde7\ud83c\uddf7 Brazil and \ud83c\uddfa\ud83c\uddfe Uruguay, consider we are currently hiring from most countries in the time zones between GMT-7 (e.g. California, USA) to GMT+2 (e.g., Berlin, Germany). We promote life-work balance Job burnout is an epidemic \ud83d\ude46, and we tech workers are especially at risk. So we'll do our best to de-stress our workforce at Binbash. In order to achieve this we offer: Remote work that lets you control your hours and physical location. Normal working hours (prime-time 9am-5pm GTM-3), in average no more than ~30-40hs per week, and we don't work during weekends or your country of residence national holidays. Project management and planning that will take into consideration the time zone of all our team members. A flexible vacation policy where you could take 3 weeks per year away from the keyboard. If more time is needed we could always try to arrange it for you. No ON-CALL rotation. We only offer support contracts with SLAs of responses on prime time business days hours exlusively. You will take on big challenges, but the hours are reasonable. Everyone is treated fairly and with respect, but where disagreement and feedback is always welcome. That is welcoming, safe, and inclusive for people of all cultures, genders, and races.","title":"How we work"},{"location":"work-with-us/careers/#leverage-software-devops-engineer-profile","text":"What You'll Work On (our tech stack) DevOps Automation Code Library Create a collection of reusable, tested, production-ready E2E AWS oriented infrastructure modules (e.g., VPC, IAM, Kubernetes, Prometheus, Grafana, EFK, Consul, Vault, Jenkins, etc.) using several tool and languages: Terraform, Ansible, Helm, Dockerfiles, Python, Bash and Makefiles . Reference Architecture Improve, maintain, extend and update our reference architecture, which has been desingned under optimal configs for the most popular modern web and mobile applications needs. Its design is fully based on the AWS Well Architected Framework . Open Source & Leverage DevOps Tools Contribute to our open source projects to continue building a fundamentally better DevOps experience, including our open source modules , leverage python CLI , Makefiles Lib among others. Document team knowledge Get siloed and not yet documented knowledge and extend the Leverage documentation , such as creating knowledgebase articles, runbooks, and other documentation for the internal team as well as Binbash Leverage customers. Customer engineering support While participating in business-hours only support rotations, collaborate with customer requests, teach Binbash Leverage and DevOps best-practices, help resolve problems, escalate to internal SMEs, and automate and document the solutions so that problems are mitigated for future scenarios and users. Role scope and extra points! Responsible for the development, maintenance, support and delivery of Binbash Leverage Products. Client side Leverage Reference Architecture solutions implementation, maintenance and support. Client side cloud solutions & tech management (service delivery and project task management). Bring Leverage recs for re-engineering, bug fixes (issues) report and improvements based on real scenario implementations. Mentoring, KT, PRs and team tech follow up both internally and customer facing. Binbash is a small, distributed startup, so things are changing all the time, and from time to time we all wear many hats. You should expect to write lot of code, but, depending on your interests, there will also be lot of opportunities to write blog posts, give talks, contribute to open source, go to conferences, talk with customers, do sales calls, think through financial questions, interview candidates, mentor new hires, design products, come up with marketing ideas, discuss strategy, consider legal questions, and all the other tasks that are part of working at a small company. Nice to have background You hate repeating and doing the same thing twice and would rather spend the time to automate a problem away than do the same task again. You have strong English communication skills and are comfortable engaging with external customers. You know how to write code across the stack ( \u201cDev\u201d ) and feel very comfortable with Infra as Code (\"IaC\"). You have experience running production software environments ( \"Ops\" ). You have a strong background in software engineering and understanding of CI/CD (or you are working hard on it!). You have a passion for learning new technologies, tools and programming languages. Bonus points for a sense of humor, empathy, autonomy and curiosity. Note that even if we're concerned with prior experience like AWS, Linux and Terraform, we're more concerned with curiosity about all areas of the Leverage stack and demonstrated ability to learn quickly and go deep when necessary.","title":"Leverage Software / DevOps Engineer Profile"},{"location":"work-with-us/contact/","text":"Contact Us \u00b6 Contact points \ud83d\udce7 Email | info@binbash.com.ar \ud83c\udf0e Web Site | https://www.binbash.com.ar \ud83c\udfe2 LinkedIn | https://www.linkedin.com/company/binbash \ud83d\udcde Phone | +1 786 2244551 \ud83d\udcf1 WhatsApp / Telegram | +54 9351 5510132 || +54 93543 516289 \ud83d\udcac Slack | Join Leverage channel","title":"Contact"},{"location":"work-with-us/contact/#contact-us","text":"Contact points \ud83d\udce7 Email | info@binbash.com.ar \ud83c\udf0e Web Site | https://www.binbash.com.ar \ud83c\udfe2 LinkedIn | https://www.linkedin.com/company/binbash \ud83d\udcde Phone | +1 786 2244551 \ud83d\udcf1 WhatsApp / Telegram | +54 9351 5510132 || +54 93543 516289 \ud83d\udcac Slack | Join Leverage channel","title":"Contact Us"},{"location":"work-with-us/contribute/","text":"Contribute and Developing Binbash Leverage \u00b6 This document explains how to get started with developing for Leverage Reference Architecture . It includes how to build, test, and release new versions. Quick Start \u00b6 Getting the code \u00b6 The code must be checked out from this same github.com repo inside the Binbash Leverage Github Organization . git clone git@github.com:binbashar/le-tf-infra-aws.git cd le-tf-infra-aws cd .. git clone git@github.com:binbashar/le-ansible-infra.git cd le-ansible-infra cd .. Initial developer environment build \u00b6 TODO Dependencies \u00b6 This guide requires you to install X v0.1 or newer. Deploying \u00b6 To deploy the Leverage Reference Architecture onto AWS. Please check the deployment guide Testing \u00b6 To run tests, just run... Releasing \u00b6 CircleCi PR auto-release job \u00b6 https://circleci.com/gh/binbashar/bb-devops-tf-infra-aws NOTE: Will only run after merged PR.","title":"Contribute"},{"location":"work-with-us/contribute/#contribute-and-developing-binbash-leverage","text":"This document explains how to get started with developing for Leverage Reference Architecture . It includes how to build, test, and release new versions.","title":"Contribute and Developing Binbash Leverage"},{"location":"work-with-us/contribute/#quick-start","text":"","title":"Quick Start"},{"location":"work-with-us/contribute/#getting-the-code","text":"The code must be checked out from this same github.com repo inside the Binbash Leverage Github Organization . git clone git@github.com:binbashar/le-tf-infra-aws.git cd le-tf-infra-aws cd .. git clone git@github.com:binbashar/le-ansible-infra.git cd le-ansible-infra cd ..","title":"Getting the code"},{"location":"work-with-us/contribute/#initial-developer-environment-build","text":"TODO","title":"Initial developer environment build"},{"location":"work-with-us/contribute/#dependencies","text":"This guide requires you to install X v0.1 or newer.","title":"Dependencies"},{"location":"work-with-us/contribute/#deploying","text":"To deploy the Leverage Reference Architecture onto AWS. Please check the deployment guide","title":"Deploying"},{"location":"work-with-us/contribute/#testing","text":"To run tests, just run...","title":"Testing"},{"location":"work-with-us/contribute/#releasing","text":"","title":"Releasing"},{"location":"work-with-us/contribute/#circleci-pr-auto-release-job","text":"https://circleci.com/gh/binbashar/bb-devops-tf-infra-aws NOTE: Will only run after merged PR.","title":"CircleCi PR auto-release job"},{"location":"work-with-us/faqs/","text":"Frequently Asked Questions (FAQs) \u00b6 Target audience \u00b6 Who is Leverage's target audience? Leverage Reference Architecture is mainly oriented to Latam, North America and European startup's CTOs, VPEs and/or engineering team leads (Software Architects / DevOps Engineers / Cloud Engineers) looking to rapidly set and host their modern web and mobile applications and systems in Amazon Web Services (\u2705 typically in just few weeks!). Oriented to Development leads or teams looking to solve their current AWS infrastructure and software delivery business needs in a securely and reliably maner, under the most modern best practices. Your Entire AWS Cloud solutions based on DevOps practices journey will be achieved: Containerization Infrastructure as Code Container Orchestration (K8s) & Application Services CI / CD Security, Compliance & Reliability Cost Optimization & Performance Effiency Observability & Monitoring Moreover, if you are looking to have the complete control of the source code, and of course be able to run it without us, such as building new Development environments and supporting your Production Cloud environments, you're a great fit for the Leverage AWS Cloud Solutions Reference Architecture model. And remember you could implement yourself or we could implement it for you! \ud83d\udcaa Why Leverage? \u00b6 Why Leverage for CIOs, CTOs and VPs of Engineering? Accelerate development and optimize costs: Annual cost savings are a new standard and best practice. Profits are being targeted to business development, regulatory and compliance needs. Resulting in a reduction of pressure on IT and development budgets, granting the opportunity to focus in new features and boost innovation. Modernize applications architecture (loosely coupled and modular): Strategically decompose the monolith into a fine-grained, loosely coupled modular architecture to increase both development and business agility. When the system architecture is designed to allow teams to test, deploy and change systems without relying on other teams, they require little communication to get the job done. In other words, both the architecture and the teams are loosely coupled. Innovation - Rapidly adopt new technologies and reduce development time: Use Leverage Reference Architecture and for AWS + our libraries to provide a collection of cloud application architecture components to build and deploy faster in the cloud. Building a cloud Landing Zone is complex, especially since most companies have little or no expertise in this area. And it can take a significant amount of time to get it right. Leverage a reference architecture to give you an AWS Landing Zone that provides a consistent and solid \"foundations\" to bootstrap your project in the cloud. The code solution implements the best AWS Well-Architected Framework practices as well as the battle-tested tech experience and years of knowledge of our contributors. Hours or days, not weeks or months: Leverage implements infrastructure as code at all times. We have rolled this out using Terraform, and has been fully proven in AWS and other Terraform providers that are part of our reference architecture like Kubernetes, Helm and Hashicorp Vault. By using the leverage cli , our binary will help you to quickly bootstrap your AWS Landing Zone in a matter of hours (or at most a few days). It's not just a pile of scripts: It's not just another layer of untested, one time and stand-alone developed scripts. The code is modularized and well designed under best practices, our leverage cli has both unite and integration tests. While our Terraform code has been extensively E2E tested. Moreover, 100% of the code is yours (to modify, extend, reuse, etc), with no vendor locking and vendor licensing fees. We use the MIT license, so you can take the code, modify it and use it as your private code. All we ask in return is a friendly greeting and that (if possible) consider contributing to Binbash Leverage project. Implement Leverage yourself or we can deploy it for you! DevOps culture and methodologies: Team agility and continuous improvements based on feedback loops are some of the main drivers of cloud adoption, and IAC's goal of reducing the frequency of deployment of both infrastructure and applications are some of the most important aspects of DevOps practices. We continue to apply these methodologies to achieve a DevOps first culture. We have experienced and demonstrated their potential and have practiced them in dozens of projects over the past 5 years. The Leverage reference architecture for AWS combines a set of application best practices, technology patterns and a common CI/CD deployment approach through leverage cli for all your application environments. As a result, we are pursuing a world-class software delivery performance through optimized collaboration, communication, reliability, stability, scalability and security at ever-decreasing cost and effort. Repeatable, composable and extensible immutable infrastructure: The best high-performance development teams create and recreate their development and production environments using infrastructure as code (IaC) as part of their daily development processes. The leverage cli allows to build repeatable and immutable infrastructure. So your cloud development, staging and production environments will consistently be the same. Why Leverage for DevOps Engineers, Cloud Architects and Software Engineers? Provisioning infrastructure as code (Iac): Instead of manually provisioning infrastructure, the real benefits of cloud adoption come from orchestrating infrastructure through code. However, this is really challenging to achieve, there are literally thousands of tiny things and configs to consider and they all seem to take forever. Our experience is that it can take teams up to 24 months to achieve a desired infra state in AWS. By using Leverage you could get your AWS Landing-Zone in few weeks, or your entire AWS Well-Architected based cloud solution within 1 to 3 months (depending on your project complexity needs). We've done it before (don't reinvent the wheel): Often, development teams have similar and recurring requests such as: iam, networking, security, storage, databases, compute and secret management, etc. Binbash Leverage has been proven in dozen of project to create software-defined (IaC) AWS environments. Best practices baked in the code: Leverage provides IaC reference architecture for AWS hosted applications infrastructure. This is baked into the code as a combination of the best AWS Well-Architected framework practices and the experience of having successfully orchestrated many customers to AWS cloud. On-demand infra deployment: Leverage provides your DevOps, Cloud, SRE and Development teams with the ability to provision on-demand granting that it will meet the rigorous security requirements of the modern cloud native best practices. It fully implements AWS Well-Architected Framework (WAF) and best DevOps practices, including release management, testing and validation and configuration. Easier to support and maintain: Leverage IaC approach significantly reduce your AWS infra deployment, config and support burden and reduce risk. Our code backed provisioning has been rigorously tested many times, eliminating the possibility of manual errors. Because the entire infrastructure is deployed from the same proven code, the consistency your cloud environments will simplify your setup and maintenance. Use the versioned code to iterate and improve, extend or compose your internal processes as your cloud operating model evolves. There is no vendor lock-in. You own the solution: With Leverage you own 100% of the code with no lock-in clauses. If you choose to leave Leverage , you will still have your entire AWS cloud infrastructure that you can access and manage. If you drop Leverage , you will still have your entire cloud native infrastructure code (Terraform, Helm, Ansible, Python). It\u2019s 100% Open Source on GitHub and is free to use with no strings attached under MIT license (no licensing fees), and you are free to commercially and privately use, distribute and modify. Consistent environments (Dev/prod parity): Keep development, staging, and production cloud envs parity. Infrastructure as code allow us to define and provisioning all infrastructure components (think networks, load balancers, databases, security, compute and storage, etc.) using code. Leverage uses Terraform as the IaC language, to deploy and setup all the AWS, Kubernetes and Hashicorp Vault resources (it has support for multiple cloud and technology providers). Backed by code, your cloud environments are built exactly the identical way all the time. Finally, this will result in no differences between development, staging and production. Development in production like envs: IaC allows your development team to deploy and test the AWS infrastructure as if it were application code. Your development is always done in production-like environments. Provision your cloud test and sandbox environments on demand and tear them down when all your testing is complete. Leverage takes all the pain out of maintaining production-like environments, with stable infra releases. It eliminates the unpredictability of wondering if what actually worked in your development envs will work in production.","title":"FAQs"},{"location":"work-with-us/faqs/#frequently-asked-questions-faqs","text":"","title":"Frequently Asked Questions (FAQs)"},{"location":"work-with-us/faqs/#target-audience","text":"Who is Leverage's target audience? Leverage Reference Architecture is mainly oriented to Latam, North America and European startup's CTOs, VPEs and/or engineering team leads (Software Architects / DevOps Engineers / Cloud Engineers) looking to rapidly set and host their modern web and mobile applications and systems in Amazon Web Services (\u2705 typically in just few weeks!). Oriented to Development leads or teams looking to solve their current AWS infrastructure and software delivery business needs in a securely and reliably maner, under the most modern best practices. Your Entire AWS Cloud solutions based on DevOps practices journey will be achieved: Containerization Infrastructure as Code Container Orchestration (K8s) & Application Services CI / CD Security, Compliance & Reliability Cost Optimization & Performance Effiency Observability & Monitoring Moreover, if you are looking to have the complete control of the source code, and of course be able to run it without us, such as building new Development environments and supporting your Production Cloud environments, you're a great fit for the Leverage AWS Cloud Solutions Reference Architecture model. And remember you could implement yourself or we could implement it for you! \ud83d\udcaa","title":"Target audience"},{"location":"work-with-us/faqs/#why-leverage","text":"Why Leverage for CIOs, CTOs and VPs of Engineering? Accelerate development and optimize costs: Annual cost savings are a new standard and best practice. Profits are being targeted to business development, regulatory and compliance needs. Resulting in a reduction of pressure on IT and development budgets, granting the opportunity to focus in new features and boost innovation. Modernize applications architecture (loosely coupled and modular): Strategically decompose the monolith into a fine-grained, loosely coupled modular architecture to increase both development and business agility. When the system architecture is designed to allow teams to test, deploy and change systems without relying on other teams, they require little communication to get the job done. In other words, both the architecture and the teams are loosely coupled. Innovation - Rapidly adopt new technologies and reduce development time: Use Leverage Reference Architecture and for AWS + our libraries to provide a collection of cloud application architecture components to build and deploy faster in the cloud. Building a cloud Landing Zone is complex, especially since most companies have little or no expertise in this area. And it can take a significant amount of time to get it right. Leverage a reference architecture to give you an AWS Landing Zone that provides a consistent and solid \"foundations\" to bootstrap your project in the cloud. The code solution implements the best AWS Well-Architected Framework practices as well as the battle-tested tech experience and years of knowledge of our contributors. Hours or days, not weeks or months: Leverage implements infrastructure as code at all times. We have rolled this out using Terraform, and has been fully proven in AWS and other Terraform providers that are part of our reference architecture like Kubernetes, Helm and Hashicorp Vault. By using the leverage cli , our binary will help you to quickly bootstrap your AWS Landing Zone in a matter of hours (or at most a few days). It's not just a pile of scripts: It's not just another layer of untested, one time and stand-alone developed scripts. The code is modularized and well designed under best practices, our leverage cli has both unite and integration tests. While our Terraform code has been extensively E2E tested. Moreover, 100% of the code is yours (to modify, extend, reuse, etc), with no vendor locking and vendor licensing fees. We use the MIT license, so you can take the code, modify it and use it as your private code. All we ask in return is a friendly greeting and that (if possible) consider contributing to Binbash Leverage project. Implement Leverage yourself or we can deploy it for you! DevOps culture and methodologies: Team agility and continuous improvements based on feedback loops are some of the main drivers of cloud adoption, and IAC's goal of reducing the frequency of deployment of both infrastructure and applications are some of the most important aspects of DevOps practices. We continue to apply these methodologies to achieve a DevOps first culture. We have experienced and demonstrated their potential and have practiced them in dozens of projects over the past 5 years. The Leverage reference architecture for AWS combines a set of application best practices, technology patterns and a common CI/CD deployment approach through leverage cli for all your application environments. As a result, we are pursuing a world-class software delivery performance through optimized collaboration, communication, reliability, stability, scalability and security at ever-decreasing cost and effort. Repeatable, composable and extensible immutable infrastructure: The best high-performance development teams create and recreate their development and production environments using infrastructure as code (IaC) as part of their daily development processes. The leverage cli allows to build repeatable and immutable infrastructure. So your cloud development, staging and production environments will consistently be the same. Why Leverage for DevOps Engineers, Cloud Architects and Software Engineers? Provisioning infrastructure as code (Iac): Instead of manually provisioning infrastructure, the real benefits of cloud adoption come from orchestrating infrastructure through code. However, this is really challenging to achieve, there are literally thousands of tiny things and configs to consider and they all seem to take forever. Our experience is that it can take teams up to 24 months to achieve a desired infra state in AWS. By using Leverage you could get your AWS Landing-Zone in few weeks, or your entire AWS Well-Architected based cloud solution within 1 to 3 months (depending on your project complexity needs). We've done it before (don't reinvent the wheel): Often, development teams have similar and recurring requests such as: iam, networking, security, storage, databases, compute and secret management, etc. Binbash Leverage has been proven in dozen of project to create software-defined (IaC) AWS environments. Best practices baked in the code: Leverage provides IaC reference architecture for AWS hosted applications infrastructure. This is baked into the code as a combination of the best AWS Well-Architected framework practices and the experience of having successfully orchestrated many customers to AWS cloud. On-demand infra deployment: Leverage provides your DevOps, Cloud, SRE and Development teams with the ability to provision on-demand granting that it will meet the rigorous security requirements of the modern cloud native best practices. It fully implements AWS Well-Architected Framework (WAF) and best DevOps practices, including release management, testing and validation and configuration. Easier to support and maintain: Leverage IaC approach significantly reduce your AWS infra deployment, config and support burden and reduce risk. Our code backed provisioning has been rigorously tested many times, eliminating the possibility of manual errors. Because the entire infrastructure is deployed from the same proven code, the consistency your cloud environments will simplify your setup and maintenance. Use the versioned code to iterate and improve, extend or compose your internal processes as your cloud operating model evolves. There is no vendor lock-in. You own the solution: With Leverage you own 100% of the code with no lock-in clauses. If you choose to leave Leverage , you will still have your entire AWS cloud infrastructure that you can access and manage. If you drop Leverage , you will still have your entire cloud native infrastructure code (Terraform, Helm, Ansible, Python). It\u2019s 100% Open Source on GitHub and is free to use with no strings attached under MIT license (no licensing fees), and you are free to commercially and privately use, distribute and modify. Consistent environments (Dev/prod parity): Keep development, staging, and production cloud envs parity. Infrastructure as code allow us to define and provisioning all infrastructure components (think networks, load balancers, databases, security, compute and storage, etc.) using code. Leverage uses Terraform as the IaC language, to deploy and setup all the AWS, Kubernetes and Hashicorp Vault resources (it has support for multiple cloud and technology providers). Backed by code, your cloud environments are built exactly the identical way all the time. Finally, this will result in no differences between development, staging and production. Development in production like envs: IaC allows your development team to deploy and test the AWS infrastructure as if it were application code. Your development is always done in production-like environments. Provision your cloud test and sandbox environments on demand and tear them down when all your testing is complete. Leverage takes all the pain out of maintaining production-like environments, with stable infra releases. It eliminates the unpredictability of wondering if what actually worked in your development envs will work in production.","title":"Why Leverage?"},{"location":"work-with-us/subscription-plans/","text":"","title":"Subscription Plans"},{"location":"work-with-us/support/","text":"Support \u00b6 Leverage Reference Architecture \u00b6 Please create a Github Issue to get immediate support from the Binbash Leverage Team Our Engineering & Support Team \u00b6 AWS Well Architected Review \u00b6 Feel free to contact us for an AWS Well Architected Framework Review Well Architected Framework Review Reference Study Case Operational Excellence Security Cost Optimization Reliability Performance Efficiency WAF Exta Material DevSecOps Security Audit - v0.1 WAF Cost Optimization Checklist - v0.1 Read More \u00b6 How AWS Well-Architected Reviews Can Drive a Customer-First Culture","title":"Support"},{"location":"work-with-us/support/#support","text":"","title":"Support"},{"location":"work-with-us/support/#leverage-reference-architecture","text":"Please create a Github Issue to get immediate support from the Binbash Leverage Team","title":"Leverage Reference Architecture"},{"location":"work-with-us/support/#our-engineering-support-team","text":"","title":"Our Engineering &amp; Support Team"},{"location":"work-with-us/support/#aws-well-architected-review","text":"Feel free to contact us for an AWS Well Architected Framework Review Well Architected Framework Review Reference Study Case Operational Excellence Security Cost Optimization Reliability Performance Efficiency WAF Exta Material DevSecOps Security Audit - v0.1 WAF Cost Optimization Checklist - v0.1","title":"AWS Well Architected Review"},{"location":"work-with-us/support/#read-more","text":"How AWS Well-Architected Reviews Can Drive a Customer-First Culture","title":"Read More"},{"location":"work-with-us/team/","text":"","title":"Team"},{"location":"work-with-us/testimonials/","text":"Testimonials (by industry) \u00b6 Helthcare (Clinics) & Real State & Travel \u00b6 Yury Yakubchyk | Founder, Investor, Board Member & Advisor @ Multiple US Industries \ud83c\udf0e Company website: lifehousehotels.com \ud83c\udf0e Company website: joinsprouttherapy.com \"We utilized BinBash to help get our DevOps infrastructure launched and manage all aspects of our AWS-based setup. Their security and compliance focus turned out to be a great fit for our HIPAA regulation needs. We\u2019ve been very pleased with their responsiveness and thoroughness with regards to our technology needs and would be happy to work with them again in the future.\" EdTech (Education) \u00b6 Alejandro Parise | Founder & CEO @ Latam & North America EdTech Industry \ud83c\udf0e Company website: e-valuados.com \"Binbash provided us with cloud architecture consulting at a critical time for our company. Prior to our production go-live they reviewed in-depth our application architecture. Resulting in several optimization points, with focus on response time, security, costs, monitoring, DB data sets, backups & restore. They truly exceeded our expectations and provided support when e-valuados greatly needed it.\" Banking, Fintech and Insurtech \u00b6 Martin Vago | IT & CloudOps Manager @ Latam Fintech Industry \ud83c\udf0e Company website: tunubi.com \"Binbash has a focused and highly productive professional team. They have exceptional tech skills and effectively transmit and implement their solutions, such as Leverage. Thanks to their collaboration we managed to have a superlative product hosted in AWS. It is undoubtedly a company with which I would like to work in any challenge that lies ahead\" Felipe Lerena | Software Architect & Dev Lead @ Latam Fintech Industry \ud83c\udf0e Company website: tunubi.com \"Binbash helped us build a resilient, future-proof infrastructure in record time. They will not only do things for you, they will transfer all the knowledge and make you part of the decision. They are always thinking about re-usability, security and scale. The Binbash team is not only technically proficient but they are really nice human beings.\" Juan Manuel Rodrigo | CTO @ Latam Fintech / Banking / Insurtech Industries \ud83c\udf0e Company website: flexibility.com.ar \"We found the right AWS and DevOps technology partner at Binbash, who quickly interpreted our core business, and while working together we identified quick gains that helped us to significantly differentiate our cloud native solution. Moreover, their product, Leverage resulted in an AWS automation framework that accelerated our business roadmap, highlighting the technical talent of the Binbash team for its rapid and solid implementation.\" Alejandro Creta | Infrastructure Architecture Lead @ Latam Fintech / Banking / Insurtech Industries \ud83c\udf0e Company website: flexibility.com.ar \"Our experience with Binbash Leverage has been overwhelmingly positive. It allowed us to adopt AWS Well-Architected Framework and Infrastructure as Code in an accelerated and efficient way without losing compatibility with the tool's stardards we're using such as Terraform, Ansible and Helm. The project planning and the work methodology helped us to strengthen our DevOps culture in the areas involved such as loosely couple cloud architecture, development & CI/CD, monitoring, shift left on security and audit, adding important cultural values such as collaboration, continuous improvement and knowledge transfer.\" Media & Entertainment (Streaming) \u00b6 Max Ivanov | Software Architect @ US Media Entertainment Industry \"Binbash has a focused and highly productive professional team. They have exceptional tech skills and effectively transmit and implement their solutions, such as Leverage. Thanks to their collaboration we managed to have a superlative product hosted in AWS. It is undoubtedly a company with which I would like to work in any challenge that lies ahead\" Cyber Security & Risk Management \u00b6 Franco Gauchat | DevSecOps Engineer @ Cyber Security Industry \ud83c\udf0e Company website: btrconsulting.com \"Binbash team is highly experienced and skilled in DevOps practices and AWS cloud solutions. I have worked very closely with them on a large scale Fintech project and have demonstrated a deep understanding of our deployed AWS infrastructure. They collaborate with us to design and implement the best possible course of action for each of our applications based on short and long term business and security goals. Including cloud architecture, security, audit, monitoring, centralized logs, deployments, infra as code, scalability and performance. Hope our paths cross again very soon.\" Horacio G. de Oro | Head of DevOps @ Risk Management US Industry \ud83c\udf0e Company website: thirdpartytrust.com \"Binbash redesigned and helped us to develop our entire AWS Cloud Solutions Architecture under a IaC (InfraAsCode) best practice approach, and always taking care of the environment security. Including our new AWS Organizations Multi-Account and Kubernetes deployment infrastructure. The Binbash team managed the entire engagement effectively and within budget. The team members are knowledgeable, proactive, work collaboratively, offer solid and creative solutions, communicate well, and deliver timely and high quality work products, such as Leverage. We highly recommend Binbash for your AWS cloud infrastructure implementation.\" Sports and Events \u00b6 Leandro Basso | Co-Founder & BizDev Manager @ Latam Sports and Events Industry \ud83c\udf0e Company website: hayturno.com \"When our company The Ideas Factory was in search of a new DevOps team, we looked for a partnership that would not only help direct us into the modern world, but also one that understood our business. Binbash DevOps Cloud Services performed over and above our expectations, and the initial response from visitors of our remodeled WebApp has been overwhelmingly positive.\" Digital Advertising / Marketing \u00b6 Alina Fermo | Project Manager @ US Digital Marketing Industry \ud83c\udf0e Company website: grey.com \"I\u00b4ve got the pleasure to work with Binbash leaders in the recent past. I was managing client side DevOps project for a US Digital Marketing customer and I have only good things to say about them. Good professionals and also kind persons. They are those people you hope to have in all your projects. They are always there to give an efective response to the client, to suggest good practices, improvements with excellent communication skills. When I needed them, they were always there to solve our issues and attend our requests. Moreover, one of the Binbash Tech architected several of our project applications and also work along with their team in achieving continuous integration implementation and kick-off.Their sense of urgency and his ability to discover potential risks are great, as well as finding the best way to solve problems when facing them. There is nothing bad I can say about these guys. I only wish someday to have the pleasure to work with them again.\"","title":"Testimonials"},{"location":"work-with-us/testimonials/#testimonials-by-industry","text":"","title":"Testimonials (by industry)"},{"location":"work-with-us/testimonials/#helthcare-clinics-real-state-travel","text":"Yury Yakubchyk | Founder, Investor, Board Member & Advisor @ Multiple US Industries \ud83c\udf0e Company website: lifehousehotels.com \ud83c\udf0e Company website: joinsprouttherapy.com \"We utilized BinBash to help get our DevOps infrastructure launched and manage all aspects of our AWS-based setup. Their security and compliance focus turned out to be a great fit for our HIPAA regulation needs. We\u2019ve been very pleased with their responsiveness and thoroughness with regards to our technology needs and would be happy to work with them again in the future.\"","title":"Helthcare (Clinics) &amp; Real State &amp; Travel"},{"location":"work-with-us/testimonials/#edtech-education","text":"Alejandro Parise | Founder & CEO @ Latam & North America EdTech Industry \ud83c\udf0e Company website: e-valuados.com \"Binbash provided us with cloud architecture consulting at a critical time for our company. Prior to our production go-live they reviewed in-depth our application architecture. Resulting in several optimization points, with focus on response time, security, costs, monitoring, DB data sets, backups & restore. They truly exceeded our expectations and provided support when e-valuados greatly needed it.\"","title":"EdTech (Education)"},{"location":"work-with-us/testimonials/#banking-fintech-and-insurtech","text":"Martin Vago | IT & CloudOps Manager @ Latam Fintech Industry \ud83c\udf0e Company website: tunubi.com \"Binbash has a focused and highly productive professional team. They have exceptional tech skills and effectively transmit and implement their solutions, such as Leverage. Thanks to their collaboration we managed to have a superlative product hosted in AWS. It is undoubtedly a company with which I would like to work in any challenge that lies ahead\" Felipe Lerena | Software Architect & Dev Lead @ Latam Fintech Industry \ud83c\udf0e Company website: tunubi.com \"Binbash helped us build a resilient, future-proof infrastructure in record time. They will not only do things for you, they will transfer all the knowledge and make you part of the decision. They are always thinking about re-usability, security and scale. The Binbash team is not only technically proficient but they are really nice human beings.\" Juan Manuel Rodrigo | CTO @ Latam Fintech / Banking / Insurtech Industries \ud83c\udf0e Company website: flexibility.com.ar \"We found the right AWS and DevOps technology partner at Binbash, who quickly interpreted our core business, and while working together we identified quick gains that helped us to significantly differentiate our cloud native solution. Moreover, their product, Leverage resulted in an AWS automation framework that accelerated our business roadmap, highlighting the technical talent of the Binbash team for its rapid and solid implementation.\" Alejandro Creta | Infrastructure Architecture Lead @ Latam Fintech / Banking / Insurtech Industries \ud83c\udf0e Company website: flexibility.com.ar \"Our experience with Binbash Leverage has been overwhelmingly positive. It allowed us to adopt AWS Well-Architected Framework and Infrastructure as Code in an accelerated and efficient way without losing compatibility with the tool's stardards we're using such as Terraform, Ansible and Helm. The project planning and the work methodology helped us to strengthen our DevOps culture in the areas involved such as loosely couple cloud architecture, development & CI/CD, monitoring, shift left on security and audit, adding important cultural values such as collaboration, continuous improvement and knowledge transfer.\"","title":"Banking, Fintech and Insurtech"},{"location":"work-with-us/testimonials/#media-entertainment-streaming","text":"Max Ivanov | Software Architect @ US Media Entertainment Industry \"Binbash has a focused and highly productive professional team. They have exceptional tech skills and effectively transmit and implement their solutions, such as Leverage. Thanks to their collaboration we managed to have a superlative product hosted in AWS. It is undoubtedly a company with which I would like to work in any challenge that lies ahead\"","title":"Media &amp; Entertainment (Streaming)"},{"location":"work-with-us/testimonials/#cyber-security-risk-management","text":"Franco Gauchat | DevSecOps Engineer @ Cyber Security Industry \ud83c\udf0e Company website: btrconsulting.com \"Binbash team is highly experienced and skilled in DevOps practices and AWS cloud solutions. I have worked very closely with them on a large scale Fintech project and have demonstrated a deep understanding of our deployed AWS infrastructure. They collaborate with us to design and implement the best possible course of action for each of our applications based on short and long term business and security goals. Including cloud architecture, security, audit, monitoring, centralized logs, deployments, infra as code, scalability and performance. Hope our paths cross again very soon.\" Horacio G. de Oro | Head of DevOps @ Risk Management US Industry \ud83c\udf0e Company website: thirdpartytrust.com \"Binbash redesigned and helped us to develop our entire AWS Cloud Solutions Architecture under a IaC (InfraAsCode) best practice approach, and always taking care of the environment security. Including our new AWS Organizations Multi-Account and Kubernetes deployment infrastructure. The Binbash team managed the entire engagement effectively and within budget. The team members are knowledgeable, proactive, work collaboratively, offer solid and creative solutions, communicate well, and deliver timely and high quality work products, such as Leverage. We highly recommend Binbash for your AWS cloud infrastructure implementation.\"","title":"Cyber Security &amp; Risk Management"},{"location":"work-with-us/testimonials/#sports-and-events","text":"Leandro Basso | Co-Founder & BizDev Manager @ Latam Sports and Events Industry \ud83c\udf0e Company website: hayturno.com \"When our company The Ideas Factory was in search of a new DevOps team, we looked for a partnership that would not only help direct us into the modern world, but also one that understood our business. Binbash DevOps Cloud Services performed over and above our expectations, and the initial response from visitors of our remodeled WebApp has been overwhelmingly positive.\"","title":"Sports and Events"},{"location":"work-with-us/testimonials/#digital-advertising-marketing","text":"Alina Fermo | Project Manager @ US Digital Marketing Industry \ud83c\udf0e Company website: grey.com \"I\u00b4ve got the pleasure to work with Binbash leaders in the recent past. I was managing client side DevOps project for a US Digital Marketing customer and I have only good things to say about them. Good professionals and also kind persons. They are those people you hope to have in all your projects. They are always there to give an efective response to the client, to suggest good practices, improvements with excellent communication skills. When I needed them, they were always there to solve our issues and attend our requests. Moreover, one of the Binbash Tech architected several of our project applications and also work along with their team in achieving continuous integration implementation and kick-off.Their sense of urgency and his ability to discover potential risks are great, as well as finding the best way to solve problems when facing them. There is nothing bad I can say about these guys. I only wish someday to have the pleasure to work with them again.\"","title":"Digital Advertising / Marketing"},{"location":"work-with-us/roadmap/cost-optimization/","text":"Cost Optimization Roadmap \u00b6 Features / Functionalities \ud83d\udcb0\ud83d\udcca\ud83d\udcc9 \u00b6 Category Tags / Labels Feature / Functionality Status Doc Cost Optimization (FinOps) leverage cloud-solutions-architecture documentation Calculate Cloud provider costs (Cost optimization focus!) \u2705 \u274c Cost Optimization (FinOps) leverage cost-optimization billing AWS billing alarms + AWS Budget (forecasted account cost / RI Coverage) Notifications to Slack \u2705 \u274c Cost Optimization (FinOps) leverage cost-optimization cost Activate AWS Trusted Advisor cost related results \u2705 \u274c Cost Optimization (FinOps) leverage cost-optimization lambda-nuke Setup Lambda nuke to automatically clean up AWS account resources \u2705 \u274c Cost Optimization (FinOps) leverage cost-optimization lambda-scheduler Setup lambda scheduler for stop and start resources on AWS (EC2, ASG & RDS) \u2705 \u274c","title":"Cost Optimization"},{"location":"work-with-us/roadmap/cost-optimization/#cost-optimization-roadmap","text":"","title":"Cost Optimization Roadmap"},{"location":"work-with-us/roadmap/cost-optimization/#features-functionalities","text":"Category Tags / Labels Feature / Functionality Status Doc Cost Optimization (FinOps) leverage cloud-solutions-architecture documentation Calculate Cloud provider costs (Cost optimization focus!) \u2705 \u274c Cost Optimization (FinOps) leverage cost-optimization billing AWS billing alarms + AWS Budget (forecasted account cost / RI Coverage) Notifications to Slack \u2705 \u274c Cost Optimization (FinOps) leverage cost-optimization cost Activate AWS Trusted Advisor cost related results \u2705 \u274c Cost Optimization (FinOps) leverage cost-optimization lambda-nuke Setup Lambda nuke to automatically clean up AWS account resources \u2705 \u274c Cost Optimization (FinOps) leverage cost-optimization lambda-scheduler Setup lambda scheduler for stop and start resources on AWS (EC2, ASG & RDS) \u2705 \u274c","title":"Features / Functionalities \ud83d\udcb0\ud83d\udcca\ud83d\udcc9"},{"location":"work-with-us/roadmap/demo-apps/","text":"Demo Applications Roadmap \u00b6 Features / Functionalities \ud83d\udc68\u200d\ud83d\udcbb\ud83c\udccf\ud83d\udd79\ud83c\udfaf \u00b6 Category Tags / Labels Feature / Functionality Status Doc CI/CD Pipeline automation & imple leverage ci-cd-pipeline docker build FrontEnd Build (Demo App): set up ECR, create IAM permissions, create pipelines (Jenkins / DroneCI), set up GitHub triggers 2021 Q2 \u274c CI/CD Pipeline automation & imple leverage ci-cd-pipeline deploy FrontEnd Deploy (Demo App): create pipelines (Jenkins / Spinnaker), set up ECR/Github triggers 2021 Q2 \u274c CI/CD Pipeline automation & imple leverage ci-cd-pipeline docker build BackEnd Build (Demo App): set up ECR, create IAM permissions, create pipelines (Jenkins / DroneCI), set up GitHub triggers 2021 Q2 \u274c CI/CD Pipeline automation & imple leverage ci-cd-pipeline deploy BackEnd Deploy (Demo App): create pipelines (Jenkins / Spinnaker), set up ECR triggers 2021 Q2 \u274c Testing (QA) leverage testing ci-cd-pipeline Unit Testing (Demo App): Dev team needs this to run on a Jenkins/CircleCI/DroneCI/Spinnaker pipeline 2021 Q2 \u274c Testing (QA) leverage testing ci-cd-pipeline Integration Testing (Demo App): QA team needs automation to run on a Jenkins/Spinnaker pipeline for AWS Cloud QA / Stage envs. 2021 Q2 \u274c Testing (QA) leverage testing ci-cd-pipeline E2E Functional / Aceptannce (Demo App): QA team needs Smoke tests automation to run on a Jenkins/Spinnaker pipeline for AWS Cloud Stage / Prod envs. 2021 Q2 \u274c Testing (QA) leverage testing ci-cd-pipeline Static Analysis (Demo App): code complexity, dependency graph, code frequency, contributors, code activity, and so on. 2021 Q2 \u274c CI/CD Pipeline automation & imple leverage ci-cd-pipeline kubernetes pbe Push Button Environments (Demo App): implement ephemeral environments. 2021 Q2 \u274c","title":"Demo Applications"},{"location":"work-with-us/roadmap/demo-apps/#demo-applications-roadmap","text":"","title":"Demo Applications Roadmap"},{"location":"work-with-us/roadmap/demo-apps/#features-functionalities","text":"Category Tags / Labels Feature / Functionality Status Doc CI/CD Pipeline automation & imple leverage ci-cd-pipeline docker build FrontEnd Build (Demo App): set up ECR, create IAM permissions, create pipelines (Jenkins / DroneCI), set up GitHub triggers 2021 Q2 \u274c CI/CD Pipeline automation & imple leverage ci-cd-pipeline deploy FrontEnd Deploy (Demo App): create pipelines (Jenkins / Spinnaker), set up ECR/Github triggers 2021 Q2 \u274c CI/CD Pipeline automation & imple leverage ci-cd-pipeline docker build BackEnd Build (Demo App): set up ECR, create IAM permissions, create pipelines (Jenkins / DroneCI), set up GitHub triggers 2021 Q2 \u274c CI/CD Pipeline automation & imple leverage ci-cd-pipeline deploy BackEnd Deploy (Demo App): create pipelines (Jenkins / Spinnaker), set up ECR triggers 2021 Q2 \u274c Testing (QA) leverage testing ci-cd-pipeline Unit Testing (Demo App): Dev team needs this to run on a Jenkins/CircleCI/DroneCI/Spinnaker pipeline 2021 Q2 \u274c Testing (QA) leverage testing ci-cd-pipeline Integration Testing (Demo App): QA team needs automation to run on a Jenkins/Spinnaker pipeline for AWS Cloud QA / Stage envs. 2021 Q2 \u274c Testing (QA) leverage testing ci-cd-pipeline E2E Functional / Aceptannce (Demo App): QA team needs Smoke tests automation to run on a Jenkins/Spinnaker pipeline for AWS Cloud Stage / Prod envs. 2021 Q2 \u274c Testing (QA) leverage testing ci-cd-pipeline Static Analysis (Demo App): code complexity, dependency graph, code frequency, contributors, code activity, and so on. 2021 Q2 \u274c CI/CD Pipeline automation & imple leverage ci-cd-pipeline kubernetes pbe Push Button Environments (Demo App): implement ephemeral environments. 2021 Q2 \u274c","title":"Features / Functionalities \ud83d\udc68\u200d\ud83d\udcbb\ud83c\udccf\ud83d\udd79\ud83c\udfaf"},{"location":"work-with-us/roadmap/operational-excellence/","text":"Operational Excellence Roadmap \u00b6 Features / Functionalities \ud83d\udc68\u200d\ud83d\udcbb \ud83d\udcaf\ud83e\udd47 \u00b6 Category Tags / Labels Feature / Functionality Status Doc Cloud Solutions Architecture leverage cloud-solutions-architecture documentation DevSecOps & AWS Cloud Solutions Architecture Doc \u2705 \u2705 Cloud Solutions Architecture leverage cloud-solutions-architecture documentation Demo Applications architecture / Services Specifications Doc 2021 Q1 \u274c Base Infrastructure leverage base-infrastructure github Open Source Ref Architecture (le-tf-aws / le-ansible / le-tf-vault / le-tf-github) 2021 Q2 \u274c Base Infrastructure leverage base-infrastructure cli Leverage cli (https://github.com/binbashar/leverage) for every Reference Architecture Repo (le-tf-aws / le-ansible / le-tf-vault / le-tf-github) 2021 Q2 \u274c Base Infrastructure leverage base-infrastructure organizations Account Settings: Account Aliases and Password Policies, MFA, and enable IAM Access Analyzer across accounts. \u2705 \u274c Base Infrastructure leverage base-infrastructure storage Storage: Account Enable encrypted EBS by default on all accounts; disable S3 public ACLs and policies \u2705 \u274c Base Infrastructure leverage base-infrastructure region Define AWS Region / Multi-Region: keep in mind customers proximity, number of subnets, and other region limitations (https://infrastructure.aws) \u2705 \u274c Base Infrastructure leverage base-infrastructure vcs Terraform Github Ref Architecture / Pre-requisites: permissions to set up webhooks, create/configure repositories, create groups (Preferred SSO tool) 2021 Q2 \u274c Base Infrastructure leverage base-infrastructure organizations AWS Organizations: development/stage, production, shared, security, legacy \u2705 \u2705 Base Infrastructure leverage base-infrastructure iam IAM: initial accounts (security users, groups, policies, roles; shared/appdevtsg/appprd DevOps role) \u2705 \u2705 Base Infrastructure leverage base-infrastructure vpc Networking 1: DNS, VPC, Subnets, Route Tables, NACLs, NATGW, VPC Peering or TGW \u2705 \u274c Base Infrastructure leverage base-infrastructure vpn Networking 2: VPN (install Pritunl, create organization, servers and users) \u2705 \u274c Kubernetes leverage kubernetes eks Production Grade Cluster: deploy EKS cluster as code \u2705 \u274c Kubernetes leverage kubernetes k8s K8s Helm + Terraform Binbash Leverage repository backing all the K8s components deployment and configuration \u2705 \u274c Kubernetes leverage kubernetes metrics Monitoring: metrics-server (metrcis for K8s HPA + Cluster AutoScaler + Prom node Exporter) + kube-state-metrics (for Grafana Dasboards) 2021 Q2 \u274c Kubernetes leverage kubernetes iam security Security: Iam-authenticator, K8s RBAC (user, group and roles) \u2705 \u274c Kubernetes leverage kubernetes iam Implement AWS service accounts (IRSA for EKS) to provide IAM credentials to containers running inside a kubernetes cluster based on annotations. \u2705 \u274c Kubernetes leverage kubernetes dashboard Monitoring: K8s dashboard & Weave Scope \u2705 \u274c Kubernetes leverage kubernetes ingress Ingress: review, analyze and implement (alb skipper, k8s nginx, alb sigs, etc) \u2705 \u274c Kubernetes leverage kubernetes ingress Load Balancing: review, analyze and implement Ingress w/ LB (AWS ALB or NLB + access logs) \u2705 \u274c Kubernetes leverage kubernetes dns Implement external-dns w/ annotations for K8s deployed Apps (https://github.com/kubernetes-sigs/external-dns) \u2705 \u274c Kubernetes leverage kubernetes services-discovery Service Discovery: review, analyze and implement k8s native [env vars & core-dns] or Consul 2021 Q3 \u274c Kubernetes leverage kubernetes service-mesh linkerd Service Mesh: review, analyze and implement consul or linkerd2. 2021 Q3 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure jenkins Jenkins: installation, configuration, GitHub/GSuite/Bitbucket SSO-Auth integration \u2705 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure spinnaker Deployments / Jenkins or Tekton Pipelines + Argo-CD: installation, configuration, Github integration 2021 Q3 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure droneci DroneCI: installation, configuration, Github integration 2021 Q4 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure webhook Proxy Instance (webhooks) : installation, configuration, GitHub integration 2021 Q4 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure qa SonarQube: installation, configuration, GitHub/GSuite/Bitbucket SSO-Auth integration 2021 Q4 \u274c Applications Infrastructure leverage apps-infrastructure docker containers Automate and containerized app environments by using docker images, enabling consistent experience in local environment and dev/stage/prod Cloud environments. \u2705 \u274c Applications Infrastructure leverage apps-infrastructure docker containers Automate and containerized app environments by using docker images, enabling consistent experience in local environment and dev/stage/prod Cloud environments. \u2705 \u274c Applications Infrastructure leverage apps-infrastructure database rds Databases: RDS (most likely AWS Aurora MySql, single db for all microservices at first - Prod dedicated instance considering new auto-scaling feature and read-replicas) + RDS Proxy (if needed for high Cx N\u00b0) - Compliance: Consider using SSL/TLS to Encrypt a Connection to a DB Instance \u2705 \u274c Applications Infrastructure leverage apps-infrastructure queue sqs Queues: SQS (recommended for background workers and some microservices). Redis (AWS ElasticCache) / RabbitMQ (K8s Containerzied). \u2705 \u274c Applications Infrastructure leverage apps-infrastructure storage s3 Storage: S3 (for the FrontEnd statics) \u2705 \u274c Applications Infrastructure leverage apps-infrastructure cloudfront cdn Caching: CloudFront (for the FrontEnd) w/ access logs \u2705 \u274c Applications Infrastructure leverage apps-infrastructure cache redis CacheLayer: AWS Elasticache (Memcache or Redis) \u2705 \u274c","title":"Operational Excellence"},{"location":"work-with-us/roadmap/operational-excellence/#operational-excellence-roadmap","text":"","title":"Operational Excellence Roadmap"},{"location":"work-with-us/roadmap/operational-excellence/#features-functionalities","text":"Category Tags / Labels Feature / Functionality Status Doc Cloud Solutions Architecture leverage cloud-solutions-architecture documentation DevSecOps & AWS Cloud Solutions Architecture Doc \u2705 \u2705 Cloud Solutions Architecture leverage cloud-solutions-architecture documentation Demo Applications architecture / Services Specifications Doc 2021 Q1 \u274c Base Infrastructure leverage base-infrastructure github Open Source Ref Architecture (le-tf-aws / le-ansible / le-tf-vault / le-tf-github) 2021 Q2 \u274c Base Infrastructure leverage base-infrastructure cli Leverage cli (https://github.com/binbashar/leverage) for every Reference Architecture Repo (le-tf-aws / le-ansible / le-tf-vault / le-tf-github) 2021 Q2 \u274c Base Infrastructure leverage base-infrastructure organizations Account Settings: Account Aliases and Password Policies, MFA, and enable IAM Access Analyzer across accounts. \u2705 \u274c Base Infrastructure leverage base-infrastructure storage Storage: Account Enable encrypted EBS by default on all accounts; disable S3 public ACLs and policies \u2705 \u274c Base Infrastructure leverage base-infrastructure region Define AWS Region / Multi-Region: keep in mind customers proximity, number of subnets, and other region limitations (https://infrastructure.aws) \u2705 \u274c Base Infrastructure leverage base-infrastructure vcs Terraform Github Ref Architecture / Pre-requisites: permissions to set up webhooks, create/configure repositories, create groups (Preferred SSO tool) 2021 Q2 \u274c Base Infrastructure leverage base-infrastructure organizations AWS Organizations: development/stage, production, shared, security, legacy \u2705 \u2705 Base Infrastructure leverage base-infrastructure iam IAM: initial accounts (security users, groups, policies, roles; shared/appdevtsg/appprd DevOps role) \u2705 \u2705 Base Infrastructure leverage base-infrastructure vpc Networking 1: DNS, VPC, Subnets, Route Tables, NACLs, NATGW, VPC Peering or TGW \u2705 \u274c Base Infrastructure leverage base-infrastructure vpn Networking 2: VPN (install Pritunl, create organization, servers and users) \u2705 \u274c Kubernetes leverage kubernetes eks Production Grade Cluster: deploy EKS cluster as code \u2705 \u274c Kubernetes leverage kubernetes k8s K8s Helm + Terraform Binbash Leverage repository backing all the K8s components deployment and configuration \u2705 \u274c Kubernetes leverage kubernetes metrics Monitoring: metrics-server (metrcis for K8s HPA + Cluster AutoScaler + Prom node Exporter) + kube-state-metrics (for Grafana Dasboards) 2021 Q2 \u274c Kubernetes leverage kubernetes iam security Security: Iam-authenticator, K8s RBAC (user, group and roles) \u2705 \u274c Kubernetes leverage kubernetes iam Implement AWS service accounts (IRSA for EKS) to provide IAM credentials to containers running inside a kubernetes cluster based on annotations. \u2705 \u274c Kubernetes leverage kubernetes dashboard Monitoring: K8s dashboard & Weave Scope \u2705 \u274c Kubernetes leverage kubernetes ingress Ingress: review, analyze and implement (alb skipper, k8s nginx, alb sigs, etc) \u2705 \u274c Kubernetes leverage kubernetes ingress Load Balancing: review, analyze and implement Ingress w/ LB (AWS ALB or NLB + access logs) \u2705 \u274c Kubernetes leverage kubernetes dns Implement external-dns w/ annotations for K8s deployed Apps (https://github.com/kubernetes-sigs/external-dns) \u2705 \u274c Kubernetes leverage kubernetes services-discovery Service Discovery: review, analyze and implement k8s native [env vars & core-dns] or Consul 2021 Q3 \u274c Kubernetes leverage kubernetes service-mesh linkerd Service Mesh: review, analyze and implement consul or linkerd2. 2021 Q3 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure jenkins Jenkins: installation, configuration, GitHub/GSuite/Bitbucket SSO-Auth integration \u2705 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure spinnaker Deployments / Jenkins or Tekton Pipelines + Argo-CD: installation, configuration, Github integration 2021 Q3 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure droneci DroneCI: installation, configuration, Github integration 2021 Q4 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure webhook Proxy Instance (webhooks) : installation, configuration, GitHub integration 2021 Q4 \u274c CI/CD Infrastructure leverage ci-cd-infrastructure qa SonarQube: installation, configuration, GitHub/GSuite/Bitbucket SSO-Auth integration 2021 Q4 \u274c Applications Infrastructure leverage apps-infrastructure docker containers Automate and containerized app environments by using docker images, enabling consistent experience in local environment and dev/stage/prod Cloud environments. \u2705 \u274c Applications Infrastructure leverage apps-infrastructure docker containers Automate and containerized app environments by using docker images, enabling consistent experience in local environment and dev/stage/prod Cloud environments. \u2705 \u274c Applications Infrastructure leverage apps-infrastructure database rds Databases: RDS (most likely AWS Aurora MySql, single db for all microservices at first - Prod dedicated instance considering new auto-scaling feature and read-replicas) + RDS Proxy (if needed for high Cx N\u00b0) - Compliance: Consider using SSL/TLS to Encrypt a Connection to a DB Instance \u2705 \u274c Applications Infrastructure leverage apps-infrastructure queue sqs Queues: SQS (recommended for background workers and some microservices). Redis (AWS ElasticCache) / RabbitMQ (K8s Containerzied). \u2705 \u274c Applications Infrastructure leverage apps-infrastructure storage s3 Storage: S3 (for the FrontEnd statics) \u2705 \u274c Applications Infrastructure leverage apps-infrastructure cloudfront cdn Caching: CloudFront (for the FrontEnd) w/ access logs \u2705 \u274c Applications Infrastructure leverage apps-infrastructure cache redis CacheLayer: AWS Elasticache (Memcache or Redis) \u2705 \u274c","title":"Features / Functionalities \ud83d\udc68\u200d\ud83d\udcbb \ud83d\udcaf\ud83e\udd47"},{"location":"work-with-us/roadmap/overview/","text":"Roadmap \u00b6 Leverage AWS Cloud Solutions Reference Architecture Features / Functionalities per category Operational Excellence Reliability & Performance Security Cost Optimization Demo Applications","title":"Overview"},{"location":"work-with-us/roadmap/overview/#roadmap","text":"Leverage AWS Cloud Solutions Reference Architecture Features / Functionalities per category Operational Excellence Reliability & Performance Security Cost Optimization Demo Applications","title":"Roadmap"},{"location":"work-with-us/roadmap/reliability-performance/","text":"Reliability Performance Roadmap \u00b6 Features / Functionalities \ud83d\ude80\u23f2\ud83d\udcca \u00b6 Category Tags / Labels Feature / Functionality Status Doc Monitoring Metrics & Alerting leverage monitoring-metrics-alerting prometheus grafana Metrics: install and configure Prometheus (NodeExporter for EC2 / BlackBox exporter / Alert Monitroing), install and configure Grafana (K8s Plugin + Prometheus int + CloudWatch int) \u2705 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting grafana cloudwatch Metrics: Grafana + AWS Cloudwatch integrations config (https://github.com/monitoringartist/grafana-aws-cloudwatch-dashboards) 2021 Q2 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting apm APM: review, analyze and implement (New Relic, DataDog, ElasticAPM Agent/Server) 2021 Q2 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting documentation Define and document reference notification/escalation procedure \u2705 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting Alerting: configure AlertsManager, Elastalert (optimized logs rotation when using it from docker image), PagerDuty, Slack according to the procedure above 2021 Q2 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting prometheus Monitor Infra Tool Instances (WebHook Proxy, Jenkins, Vault, Pritunl, Prometheus, Grafana, etc) / implement monitoring via Prometheus + Grafana or Another Solution \u2705 \u274c Monitoring Distributed Tracing leverage monitoring-tracing jaeger Distributed Tracing Instrumentation: review, analyze and implement to detect and improve transactions performance and svs dep analysis (jaeger, instana, lightstep, AWS X-Ray, etc) 2021 Q3 \u274c Monitoring Logging leverage monitoring-logs efk Logging / EFK - use separate indexes per K8s components & apps/svc for each custer/env (segregating dev/stg from prd) + enable ES monitoring w/ X-Pack + configure curator to rotate indices + tool to improve index mgmt 2021 Q2 \u274c Performance & Optimization leverage performance-optimization ci-cd-pipeline Load Testing: set up and run continuous load tests pipelines (Jenkins) to determine and improve apps/services capacity through time (apapche ab, gatling, iperf, locust, taurus, BlazeMeter and https://github.com/loadimpact/k6) 2021 Q3 \u274c Performance & Optimization leverage performance-optimization ci-cd-pipeline Performance Testing (stress, soak, spike, etc): set up and run continuous performance tests pipelines (Jenkins) to measure performance through time (apapche ab, gatling, iperf, locust, taurus and BlazeMeter) 2021 Q3 \u274c Performance & Optimization leverage performance-optimization kubernetes Tune K8S nodes (EC2 family type, size and AWS ASG -> K8s HPA + Cluster AutoScaler ) 2021 Q3 \u274c Performance & Optimization leverage performance-optimization kubernetes Tune K8S requests and limits per namespace (CPU and RAM) / https://github.com/FairwindsOps/goldilocks 2021 Q2 \u274c Performance & Optimization leverage performance-optimization s3 S3: ensure each bucket is using the proper storage types and persistence (automate mv these objs into lower $ storage tier w/ Life Cycle Policies or w/ S3 Intelligent-Tiering) \u2705 \u274c Disaster Recovery leverage disaster-recovery backup AWS Backup Service: RDS, EC2 (AMI), EBS, Dynamo, EFS, SFx, Storage Gw \u2705 \u274c Disaster Recovery leverage disaster-recovery backup Replication: S3 (CRR cross-region replication or SRR same-region replication) \u2705 \u274c Disaster Recovery leverage disaster-recovery backup Replication: VPC / Compute / Database (CRR cross-region replication) \u2705 \u274c Disaster Recovery leverage disaster-recovery backup kubernetes Backup and migrate Kubernetes applications and their persistent volumes w/ https://velero.io/ 2021 Q3 \u274c Disaster Recovery leverage documentation disaster-recovery Review: Disaster recovery plan, missing resources, RTO / RPO, level of automation 2021 Q4 \u274c Disaster Recovery leverage documentation disaster-recovery Improve Plan: create a plan to improve the existing recovery plan and determine implementation phases 2021 Q4 \u274c Disaster Recovery leverage documentation disaster-recovery Execute Plan: implement according to the plan, review/measure and iterate 2021 Q4 \u274c","title":"Reliability & Performance"},{"location":"work-with-us/roadmap/reliability-performance/#reliability-performance-roadmap","text":"","title":"Reliability Performance Roadmap"},{"location":"work-with-us/roadmap/reliability-performance/#features-functionalities","text":"Category Tags / Labels Feature / Functionality Status Doc Monitoring Metrics & Alerting leverage monitoring-metrics-alerting prometheus grafana Metrics: install and configure Prometheus (NodeExporter for EC2 / BlackBox exporter / Alert Monitroing), install and configure Grafana (K8s Plugin + Prometheus int + CloudWatch int) \u2705 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting grafana cloudwatch Metrics: Grafana + AWS Cloudwatch integrations config (https://github.com/monitoringartist/grafana-aws-cloudwatch-dashboards) 2021 Q2 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting apm APM: review, analyze and implement (New Relic, DataDog, ElasticAPM Agent/Server) 2021 Q2 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting documentation Define and document reference notification/escalation procedure \u2705 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting Alerting: configure AlertsManager, Elastalert (optimized logs rotation when using it from docker image), PagerDuty, Slack according to the procedure above 2021 Q2 \u274c Monitoring Metrics & Alerting leverage monitoring-metrics-alerting prometheus Monitor Infra Tool Instances (WebHook Proxy, Jenkins, Vault, Pritunl, Prometheus, Grafana, etc) / implement monitoring via Prometheus + Grafana or Another Solution \u2705 \u274c Monitoring Distributed Tracing leverage monitoring-tracing jaeger Distributed Tracing Instrumentation: review, analyze and implement to detect and improve transactions performance and svs dep analysis (jaeger, instana, lightstep, AWS X-Ray, etc) 2021 Q3 \u274c Monitoring Logging leverage monitoring-logs efk Logging / EFK - use separate indexes per K8s components & apps/svc for each custer/env (segregating dev/stg from prd) + enable ES monitoring w/ X-Pack + configure curator to rotate indices + tool to improve index mgmt 2021 Q2 \u274c Performance & Optimization leverage performance-optimization ci-cd-pipeline Load Testing: set up and run continuous load tests pipelines (Jenkins) to determine and improve apps/services capacity through time (apapche ab, gatling, iperf, locust, taurus, BlazeMeter and https://github.com/loadimpact/k6) 2021 Q3 \u274c Performance & Optimization leverage performance-optimization ci-cd-pipeline Performance Testing (stress, soak, spike, etc): set up and run continuous performance tests pipelines (Jenkins) to measure performance through time (apapche ab, gatling, iperf, locust, taurus and BlazeMeter) 2021 Q3 \u274c Performance & Optimization leverage performance-optimization kubernetes Tune K8S nodes (EC2 family type, size and AWS ASG -> K8s HPA + Cluster AutoScaler ) 2021 Q3 \u274c Performance & Optimization leverage performance-optimization kubernetes Tune K8S requests and limits per namespace (CPU and RAM) / https://github.com/FairwindsOps/goldilocks 2021 Q2 \u274c Performance & Optimization leverage performance-optimization s3 S3: ensure each bucket is using the proper storage types and persistence (automate mv these objs into lower $ storage tier w/ Life Cycle Policies or w/ S3 Intelligent-Tiering) \u2705 \u274c Disaster Recovery leverage disaster-recovery backup AWS Backup Service: RDS, EC2 (AMI), EBS, Dynamo, EFS, SFx, Storage Gw \u2705 \u274c Disaster Recovery leverage disaster-recovery backup Replication: S3 (CRR cross-region replication or SRR same-region replication) \u2705 \u274c Disaster Recovery leverage disaster-recovery backup Replication: VPC / Compute / Database (CRR cross-region replication) \u2705 \u274c Disaster Recovery leverage disaster-recovery backup kubernetes Backup and migrate Kubernetes applications and their persistent volumes w/ https://velero.io/ 2021 Q3 \u274c Disaster Recovery leverage documentation disaster-recovery Review: Disaster recovery plan, missing resources, RTO / RPO, level of automation 2021 Q4 \u274c Disaster Recovery leverage documentation disaster-recovery Improve Plan: create a plan to improve the existing recovery plan and determine implementation phases 2021 Q4 \u274c Disaster Recovery leverage documentation disaster-recovery Execute Plan: implement according to the plan, review/measure and iterate 2021 Q4 \u274c","title":"Features / Functionalities \ud83d\ude80\u23f2\ud83d\udcca"},{"location":"work-with-us/roadmap/security/","text":"Security Roadmap \u00b6 Features / Functionalities \ud83d\udd10\u2705 \u00b6 Category Tags / Labels Feature / Functionality Status Doc Security & Audit (SecOps) leverage security-audit passwords Team Password Management: review, analyze and implement (passbolt, bitwarden, 1password, etc) \u2705 \u274c Security & Audit (SecOps) leverage ci-cd-infrastructure secrets Secrets Management: review, analyze and implement Hashicorp vault 2021 Q1 \u274c Compliance (SecOps) leverage secrets aws-vault implementation 2021 Q1 \u274c Security & Audit (SecOps) leverage security-audit guardduty AWS Guarduty (Cross-Org with Master and member accounts setup + Trusted IP Lists and Threat IP Lists / Creation + Deletion of Filters for your GuardDuty findings to avoid false possitives + CloudWatch Rule to Lambda/ Cw-Metrics w/ CloudWatch Dashboard) \u2705 \u274c Security & Audit (SecOps) leverage security-audit inspector AWS Inspector (w/ Ansible aws-inpector galaxy role per EC2) 2021 Q3 \u274c Security & Audit (SecOps) leverage security-audit cloudtrail AWS CloudTrail w/ CloudWatch Dashboard + Alarms (include RootLogin) to Slack \u2705 \u274c Security & Audit (SecOps) leverage security-audit firewall AWS Firewall Manager (cross-org WAF + Shield integrated with ALBs, CloudFront and/or API-GW + Cross-org Sec group audit) \u2705 \u274c Security & Audit (SecOps) leverage security-audit vpc AWS VPC Flow Logs \u2705 \u274c Security & Audit (SecOps) leverage security-audit ScoutSuite / Prowler: set up continuous, automated reports for each account (Evaluate the use of CloudMapper) 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit users Infra DevOps Tools OS Layer ( OS security updates and patches, root user config, ssh port, fail2ban ) \u2705 \u274c Compliance (SecOps) leverage security-audit compliance AWS Config: implement audit controls (evaluate automatic remediation if applicable) \u2705 \u274c Compliance (SecOps) leverage security-audit compliance AWS Security Hub: implement audit controls 2021 Q3 \u274c Compliance (SecOps) leverage security-audit compliance AWS Trusted Advisor: Review automated Costs Optimization, Performance, Security, Fault Tolerance and Service Limits audit results. \u2705 \u274c Compliance (SecOps) leverage security-audit compliance kubernetes Kubernetes Audit: implement on the clusters: KubeAudit, Kube-Bench, Kube-Hunter and Starboard. 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit ci-cd-pipeline Security and Vulnerability static code analysis (code dependencies): implement tools to continuously analyze and report vulnerabilities, automated reports (OWASP, bandit, snyk, HawkEye scanner, yarn audit, etc) 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit docker Containers: implement tools to continuously analyze and report on vulnerabilities (docker-bench-security, snyk, aquasecurity microscanner, docker-bench, aws ecr scan) \u2705 \u274c Security & Audit (SecOps) leverage security-audit Review and Fix all snyk high sev findings 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit Security and cost analysis in the CI PR automated process (le-tf-aws / le-ansible / le-tf-vault / le-tf-github) 2021 Q1 \u274c Security & Audit (SecOps) leverage security-audit Comply with AWS Security Maturity Roadmap 2021 2021 Q2 \u274c Compliance (SecOps) leverage security-audit compliance Certified compliant by the Center for Internet Security (CIS) end-to-end CIS-compliant Reference Architecture (get compliance out of the box) 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit dashboard Centralized DevSecOps Tools and Audit Report Dashboard 2021 Q3 \u274c","title":"Security"},{"location":"work-with-us/roadmap/security/#security-roadmap","text":"","title":"Security Roadmap"},{"location":"work-with-us/roadmap/security/#features-functionalities","text":"Category Tags / Labels Feature / Functionality Status Doc Security & Audit (SecOps) leverage security-audit passwords Team Password Management: review, analyze and implement (passbolt, bitwarden, 1password, etc) \u2705 \u274c Security & Audit (SecOps) leverage ci-cd-infrastructure secrets Secrets Management: review, analyze and implement Hashicorp vault 2021 Q1 \u274c Compliance (SecOps) leverage secrets aws-vault implementation 2021 Q1 \u274c Security & Audit (SecOps) leverage security-audit guardduty AWS Guarduty (Cross-Org with Master and member accounts setup + Trusted IP Lists and Threat IP Lists / Creation + Deletion of Filters for your GuardDuty findings to avoid false possitives + CloudWatch Rule to Lambda/ Cw-Metrics w/ CloudWatch Dashboard) \u2705 \u274c Security & Audit (SecOps) leverage security-audit inspector AWS Inspector (w/ Ansible aws-inpector galaxy role per EC2) 2021 Q3 \u274c Security & Audit (SecOps) leverage security-audit cloudtrail AWS CloudTrail w/ CloudWatch Dashboard + Alarms (include RootLogin) to Slack \u2705 \u274c Security & Audit (SecOps) leverage security-audit firewall AWS Firewall Manager (cross-org WAF + Shield integrated with ALBs, CloudFront and/or API-GW + Cross-org Sec group audit) \u2705 \u274c Security & Audit (SecOps) leverage security-audit vpc AWS VPC Flow Logs \u2705 \u274c Security & Audit (SecOps) leverage security-audit ScoutSuite / Prowler: set up continuous, automated reports for each account (Evaluate the use of CloudMapper) 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit users Infra DevOps Tools OS Layer ( OS security updates and patches, root user config, ssh port, fail2ban ) \u2705 \u274c Compliance (SecOps) leverage security-audit compliance AWS Config: implement audit controls (evaluate automatic remediation if applicable) \u2705 \u274c Compliance (SecOps) leverage security-audit compliance AWS Security Hub: implement audit controls 2021 Q3 \u274c Compliance (SecOps) leverage security-audit compliance AWS Trusted Advisor: Review automated Costs Optimization, Performance, Security, Fault Tolerance and Service Limits audit results. \u2705 \u274c Compliance (SecOps) leverage security-audit compliance kubernetes Kubernetes Audit: implement on the clusters: KubeAudit, Kube-Bench, Kube-Hunter and Starboard. 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit ci-cd-pipeline Security and Vulnerability static code analysis (code dependencies): implement tools to continuously analyze and report vulnerabilities, automated reports (OWASP, bandit, snyk, HawkEye scanner, yarn audit, etc) 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit docker Containers: implement tools to continuously analyze and report on vulnerabilities (docker-bench-security, snyk, aquasecurity microscanner, docker-bench, aws ecr scan) \u2705 \u274c Security & Audit (SecOps) leverage security-audit Review and Fix all snyk high sev findings 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit Security and cost analysis in the CI PR automated process (le-tf-aws / le-ansible / le-tf-vault / le-tf-github) 2021 Q1 \u274c Security & Audit (SecOps) leverage security-audit Comply with AWS Security Maturity Roadmap 2021 2021 Q2 \u274c Compliance (SecOps) leverage security-audit compliance Certified compliant by the Center for Internet Security (CIS) end-to-end CIS-compliant Reference Architecture (get compliance out of the box) 2021 Q2 \u274c Security & Audit (SecOps) leverage security-audit dashboard Centralized DevSecOps Tools and Audit Report Dashboard 2021 Q3 \u274c","title":"Features / Functionalities \ud83d\udd10\u2705"},{"location":"work-with-us/updates/product-updates-01-2021/","text":"Leverage Product Updates | January 2021 \u00b6 Dear Leveragers, We kicked off the new year with a lot of improvements and some exciting new features Updates \ud83d\udce2 TODO New Features \ud83c\udf89\ud83d\ude4c TODO Improvements \ud83d\ude80 TODO Bug Fixes \ud83d\udc1b TODO","title":"January 2021"},{"location":"work-with-us/updates/product-updates-01-2021/#leverage-product-updates-january-2021","text":"Dear Leveragers, We kicked off the new year with a lot of improvements and some exciting new features Updates \ud83d\udce2 TODO New Features \ud83c\udf89\ud83d\ude4c TODO Improvements \ud83d\ude80 TODO Bug Fixes \ud83d\udc1b TODO","title":"Leverage Product Updates | January 2021"}]}